"""
G≈Ç√≥wny modu≈Ç przetwarzania danych
Orchestrator dla ca≈Çego pipeline'u validation_and_labeling
‚úÖ TRAINING COMPATIBILITY: Obs≈Çuguje generowanie training-ready output
"""
import logging
import json
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

import pandas as pd
import numpy as np

# Obs≈Çuga import√≥w - zar√≥wno jako modu≈Ç jak i standalone script
try:
    from . import config
    from .utils import (
        setup_logging, find_input_files, load_data_file, save_data_file,
        cleanup_partial_file, get_memory_usage_mb, PerformanceTimer
    )
    from .data_validator import DataValidator
    from .feature_calculator import FeatureCalculator, FeatureQualityValidator, FeatureDistributionAnalyzer
    from .competitive_labeler import CompetitiveLabeler, LabelingStatistics
except ImportError:
    # Standalone script - dodaj bie≈ºƒÖcy katalog do sys.path
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    import config
    from utils import (
        setup_logging, find_input_files, load_data_file, save_data_file,
        cleanup_partial_file, get_memory_usage_mb, PerformanceTimer
    )
    from data_validator import DataValidator
    from feature_calculator import FeatureCalculator, FeatureQualityValidator, FeatureDistributionAnalyzer
    from competitive_labeler import CompetitiveLabeler, LabelingStatistics

class NumpyEncoder(json.JSONEncoder):
    """JSON Encoder kt√≥ry obs≈Çuguje numpy types"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (bool, np.bool)):
            return bool(obj)
        return super().default(obj)

class ValidationAndLabelingPipeline:
    """
    G≈Ç√≥wna klasa Pipeline dla przetwarzania danych
    ‚úÖ TRAINING COMPATIBILITY: Generuje training-ready output
    """
    
    def __init__(self):
        self.logger = setup_logging(f"{__name__}.ValidationAndLabelingPipeline")
        
        # Inicjalizuj komponenty
        self.data_validator = DataValidator()
        self.feature_calculator = FeatureCalculator()
        self.feature_quality_validator = FeatureQualityValidator()
        self.feature_distribution_analyzer = FeatureDistributionAnalyzer()
        self.competitive_labeler = CompetitiveLabeler()
        self.labeling_statistics = LabelingStatistics()
        
        self.logger.info("Pipeline validation_and_labeling initialized")
        
        # ‚úÖ TRAINING COMPATIBILITY: Zaloguj konfiguracjƒô
        if config.TRAINING_COMPATIBILITY_MODE:
            self.logger.info(f"üéØ TRAINING COMPATIBILITY MODE: ENABLED")
            self.logger.info(f"Label format: {config.LABEL_OUTPUT_FORMAT}, dtype: {config.LABEL_DTYPE}")
            self.logger.info(f"Output files: *_training_ready.feather")
        else:
            self.logger.info("Training compatibility mode: DISABLED")
        
        self.logger.info(f"Konfiguracja: TP={config.LONG_TP_PCT}%, SL={config.LONG_SL_PCT}%, FW={config.FUTURE_WINDOW}min")
    
    def process_single_pair(self, pair_name: str, input_file_path: Path) -> Dict[str, Any]:
        """
        Przetwarza pojedynczƒÖ parƒô walutowƒÖ
        ‚úÖ TRAINING COMPATIBILITY: Generuje training-ready output z metadata
        
        Args:
            pair_name: Nazwa pary (np. BTCUSDT)
            input_file_path: ≈öcie≈ºka do pliku wej≈õciowego
            
        Returns:
            dict: Raport przetwarzania z training compatibility metadata
        """
        self.logger.info(f"Rozpoczynam przetwarzanie pary {pair_name}")
        
        timer = PerformanceTimer()
        timer.start()
        
        # Przygotuj raport
        processing_report = {
            "pair": pair_name,
            "input_file": str(input_file_path),
            "config_snapshot": self._get_config_snapshot(),
            "success": False
        }
        
        # ‚úÖ TRAINING COMPATIBILITY: Dodaj metadata do raportu
        if config.TRAINING_COMPATIBILITY_MODE:
            processing_report["training_compatibility"] = {
                "enabled": True,
                "metadata": config.get_training_metadata(),
                "output_filename_suffix": "training_ready"
            }
        
        output_file_path = None
        
        try:
            # KROK 1: Za≈Çaduj surowe dane
            self.logger.info(f"≈Åadujƒô dane z {input_file_path}")
            raw_data = load_data_file(input_file_path)
            self.logger.info(f"Za≈Çadowano {len(raw_data):,} wierszy surowych danych")
            processing_report["input_rows"] = len(raw_data)
            
            # KROK 2: Walidacja i czyszczenie danych
            self.logger.info("Rozpoczynam walidacjƒô i czyszczenie danych")
            validated_data, validation_report = self.data_validator.validate_and_clean(raw_data, pair_name)
            processing_report["validation_report"] = validation_report
            
            # üÜï KROK 2.5: Zapisz raw validated data dla strategii (je≈õli w≈ÇƒÖczone)
            if config.SAVE_RAW_VALIDATED_DATA:
                self.logger.info("Zapisujƒô raw validated data w formacie FreqTrade")
                
                # Przygotuj nazwƒô pliku w formacie FreqTrade
                # BTCUSDT -> BTC_USDT_USDT-1m-futures.feather
                freqtrade_pair_name = pair_name.replace('/', '_').replace(':', '_')
                raw_validated_filename = f"{freqtrade_pair_name}-1m-futures.feather"
                raw_validated_path = config.RAW_VALIDATED_OUTPUT_PATH / raw_validated_filename
                
                # Przygotuj dane w formacie FreqTrade
                freqtrade_data = validated_data.reset_index()
                
                # Zmie≈Ñ nazwƒô kolumny datetime -> date (wymagane przez FreqTrade)
                freqtrade_data.rename(columns={'datetime': 'date'}, inplace=True)
                
                # üéØ KLUCZOWA POPRAWKA: Poprawnie przypisz strefƒô czasowƒÖ UTC, nie konwertuj!
                # Dane sƒÖ ju≈º w UTC, wiƒôc tylko to deklarujemy.
                freqtrade_data['date'] = pd.to_datetime(freqtrade_data['date']).dt.tz_localize('UTC')
                
                # Zapisz w formacie kompatybilnym z FreqTrade
                freqtrade_data.to_feather(raw_validated_path)
                
                self.logger.info(f"üíæ Zapisano raw validated data (FreqTrade format): {raw_validated_path}")
                self.logger.info(f"üìä Format: date={freqtrade_data['date'].dtype}, shape={freqtrade_data.shape}")
                processing_report["raw_validated_file"] = str(raw_validated_path)

            # == EARLY EXIT: Je≈õli LABELING jest wy≈ÇƒÖczony w konfiguracji ==
            if not getattr(config, "ENABLE_LABELING", True):
                self.logger.info("‚ö†Ô∏è  Labeling disabled via config.ENABLE_LABELING = False. Zako≈Ñczono po walidacji danych.")

                # Finalize timings
                timer.stop()
                processing_report["processing_time_seconds"] = timer.elapsed_seconds()
                processing_report["memory_usage_mb"] = get_memory_usage_mb()
                processing_report["success"] = True

                # Zapisz raport i zako≈Ñcz
                report_path = config.REPORTS_PATH / f"{pair_name}_validation_only_report.json"
                with open(report_path, "w", encoding="utf-8") as f:
                    json.dump(processing_report, f, indent=4, cls=NumpyEncoder)
                self.logger.info(f"‚úÖ Validation-only processing finished. Report saved to {report_path}")
                return processing_report

            # === CONTINUE NORMAL PIPELINE WHEN LABELING ENABLED ===
            # KROK 3: Obliczanie features technicznych
            self.logger.info("Rozpoczynam obliczanie features technicznych")
            features_data, features_report = self.feature_calculator.calculate_features(validated_data, pair_name)
            processing_report["features_report"] = features_report
            
            # KROK 4: Walidacja jako≈õci features
            self.logger.info("Rozpoczynam walidacjƒô jako≈õci features")
            features_quality_report = self.feature_quality_validator.validate_features_quality(
                features_data, pair_name
            )
            processing_report["features_report"]["quality_validation"] = features_quality_report
            
            # KROK 4.5: Analiza rozk≈Çadu warto≈õci features
            self.logger.info("Rozpoczynam analizƒô rozk≈Çadu warto≈õci features")
            distribution_report = self.feature_distribution_analyzer.analyze_feature_distributions(
                features_data, pair_name
            )
            processing_report["features_report"]["distribution_analysis"] = distribution_report
            
            # KROK 5: Competitive labeling (z dostƒôpem do OHLC)
            self.logger.info("Rozpoczynam competitive labeling")
            labeled_data, labeling_report = self.competitive_labeler.generate_labels_with_ohlc(
                features_data, validated_data, pair_name
            )
            processing_report["labeling_report"] = labeling_report
            
            # KROK 6: Statystyki ko≈Ñcowe
            # ‚úÖ TRAINING COMPATIBILITY: Handle different label formats for statistics
            if config.LABEL_OUTPUT_FORMAT == "onehot":
                # For onehot format, pass the DataFrame to handle properly
                label_stats = self.labeling_statistics.analyze_label_distribution(
                    labeled_data[['label_0', 'label_1', 'label_2']]
                )
            else:
                # For int8/sparse_categorical, pass the label column
                label_stats = self.labeling_statistics.analyze_label_distribution(labeled_data['label'])
            
            processing_report["labeling_report"]["detailed_statistics"] = label_stats
            
            # KROK 7: Zapisz finalne dane
            output_filename = config.get_output_filename(pair_name, config.TIMEFRAME)
            output_file_path = config.OUTPUT_DATA_PATH / output_filename
            
            self.logger.info(f"Zapisujƒô finalne dane do {output_file_path}")
            
            # ‚úÖ TRAINING COMPATIBILITY: Log format info
            if config.TRAINING_COMPATIBILITY_MODE:
                if config.LABEL_OUTPUT_FORMAT == "onehot":
                    label_cols = ['label_0', 'label_1', 'label_2']
                    self.logger.info(f"üéØ Training-ready format: Features + One-hot labels ({label_cols})")
                else:
                    self.logger.info(f"üéØ Training-ready format: Features + {config.LABEL_OUTPUT_FORMAT} labels")
            
            # Sprawd≈∫ czy plik istnieje i loguj nadpisanie
            if output_file_path.exists():
                self.logger.warning(f"Nadpisujƒô istniejƒÖcy plik: {output_file_path}")
            
            # ‚úÖ ZACHOWAJ TIMESTAMP - konwertuj datetime index na kolumnƒô przed zapisem
            labeled_data_with_timestamp = labeled_data.reset_index()
            if 'datetime' not in labeled_data_with_timestamp.columns and labeled_data_with_timestamp.columns[0] != 'datetime':
                # Je≈õli pierwsza kolumna to datetime index, zmie≈Ñ nazwƒô
                labeled_data_with_timestamp.rename(columns={labeled_data_with_timestamp.columns[0]: 'datetime'}, inplace=True)
            
            # üéØ KLUCZOWA POPRAWKA: Upewnij siƒô, ≈ºe dane treningowe majƒÖ strefƒô czasowƒÖ UTC
            if 'datetime' in labeled_data_with_timestamp.columns:
                self.logger.info("Konwertujƒô kolumnƒô 'datetime' na ≈õwiadomƒÖ strefy czasowej UTC.")
                labeled_data_with_timestamp['datetime'] = pd.to_datetime(labeled_data_with_timestamp['datetime'], utc=True)

            # ‚úÖ KOMPATYBILNO≈öƒÜ Z MODU≈ÅEM TRENUJƒÑCYM - zmie≈Ñ 'datetime' na 'timestamp'
            if 'datetime' in labeled_data_with_timestamp.columns:
                labeled_data_with_timestamp.rename(columns={'datetime': 'timestamp'}, inplace=True)
                self.logger.info("üîÑ Renamed 'datetime' column to 'timestamp' for training module compatibility")
            
            self.logger.info(f"üíæ Zapisujƒô dane z timestamp: {labeled_data_with_timestamp.shape} (kolumny: {list(labeled_data_with_timestamp.columns)})")
            
            save_data_file(labeled_data_with_timestamp, output_file_path, overwrite=config.OVERWRITE_FILES)
            processing_report["output_file"] = str(output_file_path)
            
            # KROK 8: Finalne statystyki
            timer.stop()
            processing_report["processing_time_seconds"] = timer.elapsed_seconds()
            processing_report["memory_usage_mb"] = get_memory_usage_mb()
            processing_report["success"] = True
            
            # KROK 9: Zapisz pr√≥bkƒô etykiet do CSV dla weryfikacji wizualnej
            self._save_label_sample_to_csv(labeled_data, pair_name)
            
            # Przygotuj statystyki do raportu
            processing_report["statistics"] = {
                "total_rows": len(labeled_data),
                "input_rows": len(raw_data),
                "gaps_filled": validation_report.get("gaps_filled", 0),
                "duplicates_removed": validation_report.get("duplicates_removed", 0),
                "features_anomalies": features_quality_report.get("extreme_values", {}),
                "label_distribution": labeling_report["label_distribution"],
                "processing_time_seconds": processing_report["processing_time_seconds"],
                "rows_per_second": timer.rows_per_second(len(labeled_data)),
                "memory_usage_mb": processing_report["memory_usage_mb"]
            }
            
            # ‚úÖ TRAINING COMPATIBILITY: Add training-specific statistics
            if config.TRAINING_COMPATIBILITY_MODE:
                processing_report["statistics"]["training_compatibility"] = {
                    "label_format": config.LABEL_OUTPUT_FORMAT,
                    "features_count": len(config.get_training_metadata()["features_list"]),
                    "data_shape": list(labeled_data.shape),
                    "memory_efficient": config.LABEL_OUTPUT_FORMAT != "onehot",
                    "ready_for_keras": True
                }
            
            success_msg = f"Przetwarzanie {pair_name} zako≈Ñczone pomy≈õlnie: {len(labeled_data):,} wierszy w {timer.elapsed_seconds():.2f}s (Speed: {timer.rows_per_second(len(labeled_data)):.0f} rows/s)"
            
            if config.TRAINING_COMPATIBILITY_MODE:
                success_msg += f" ‚Üí Training-ready output: {output_filename}"
            
            self.logger.info(success_msg)
            
            return processing_report
            
        except Exception as e:
            timer.stop()
            error_msg = f"B≈ÇƒÖd podczas przetwarzania {pair_name}: {str(e)}"
            self.logger.error(error_msg)
            
            # Cleanup w przypadku b≈Çƒôdu (strategia all-or-nothing)
            if output_file_path and output_file_path.exists():
                cleanup_partial_file(output_file_path, self.logger)
            
            processing_report["success"] = False
            processing_report["error_message"] = str(e)
            processing_report["processing_time_seconds"] = timer.elapsed_seconds()
            processing_report["memory_usage_mb"] = get_memory_usage_mb()
            
            return processing_report
    
    def _save_label_sample_to_csv(self, final_df: pd.DataFrame, pair_name: str):
        """Zapisuje pr√≥bkƒô danych (OHLC + cechy + etykieta) do pliku CSV dla weryfikacji wizualnej."""
        
        # Konfiguracja "na twardo", aby uniknƒÖƒá problem√≥w z plikiem config.py
        SAVE_SAMPLE = True
        SAMPLE_HOURS = 24
        DIAGNOSTICS_DIR = "diagnostics"

        if not SAVE_SAMPLE:
            return

        try:
            self.logger.info(f"Zapisywanie {SAMPLE_HOURS}h pr√≥bki etykiet do pliku CSV dla {pair_name}...")
            
            # Stw√≥rz kopiƒô, aby uniknƒÖƒá modyfikacji oryginalnego DataFrame (SettingWithCopyWarning)
            df_copy = final_df.copy()

            # Dynamicznie znajd≈∫ kolumnƒô z etykietƒÖ i przygotuj jƒÖ do mapowania
            label_col_name = None
            if 'label' in df_copy.columns:
                label_col_name = 'label'
            elif 'label_1' in df_copy.columns: # Obs≈Çuga formatu one-hot
                # SHORT=0, LONG=1, HOLD=2 (zgodnie z competitive_labeler)
                df_copy['temp_label'] = 2  # Domy≈õlnie HOLD
                df_copy.loc[df_copy['label_0'] == 1, 'temp_label'] = 0  # SHORT
                df_copy.loc[df_copy['label_1'] == 1, 'temp_label'] = 1  # LONG
                label_col_name = 'temp_label'

            # Je≈õli znaleziono kolumnƒô z etykietami, zmapuj jƒÖ na warto≈õci s≈Çowne
            if label_col_name:
                signal_map = {0: 'SHORT', 1: 'LONG', 2: 'HOLD'}
                df_copy['signal'] = df_copy[label_col_name].map(signal_map)
                # Opcjonalnie usu≈Ñ tymczasowƒÖ kolumnƒô, je≈õli by≈Ça tworzona
                if label_col_name == 'temp_label':
                    df_copy.drop(columns=['temp_label'], inplace=True)
            else:
                self.logger.warning("Nie znaleziono kolumny z etykietƒÖ ('label' lub 'label_x'). Pr√≥bka CSV nie bƒôdzie zawieraƒá sygna≈Çu.")

            # Upewnijmy siƒô, ≈ºe DataFrame ma indeks typu Datetime
            if not isinstance(df_copy.index, pd.DatetimeIndex):
                 self.logger.warning("DataFrame nie ma indeksu typu Datetime. Pr√≥bujƒô przekonwertowaƒá...")
                 # Spr√≥buj przekonwertowaƒá, je≈õli to mo≈ºliwe, lub zrezygnuj
                 if 'timestamp' in df_copy.columns:
                     df_copy = df_copy.set_index('timestamp')
                 elif 'datetime' in df_copy.columns:
                     df_copy = df_copy.set_index('datetime')
                 else:
                     self.logger.error("Brak kolumny czasowej do ustawienia jako indeks. Nie mo≈ºna zapisaƒá pr√≥bki.")
                     return

            last_timestamp = df_copy.index.max()
            start_timestamp = last_timestamp - pd.Timedelta(hours=SAMPLE_HOURS)
            sample_df = df_copy[df_copy.index >= start_timestamp].copy()

            columns_to_save = [
                'open', 'high', 'low', 'close', 'volume',
                'high_change', 'low_change', 'close_change', 'volume_change',
                'price_to_ma1440', 'price_to_ma43200',
                'volume_to_ma1440', 'volume_to_ma43200',
                'signal'
            ]
            
            # Sprawd≈∫, kt√≥re z ≈ºƒÖdanych kolumn faktycznie istniejƒÖ
            existing_columns = [col for col in columns_to_save if col in sample_df.columns]
            if len(existing_columns) != len(columns_to_save):
                missing = set(columns_to_save) - set(existing_columns)
                self.logger.warning(f"BrakujƒÖce kolumny w ramce danych: {missing}. Zapisujƒô tylko dostƒôpne.")

            output_path = config.OUTPUT_DATA_PATH / DIAGNOSTICS_DIR
            os.makedirs(output_path, exist_ok=True)
            filename = f"label_sample_{pair_name}_{last_timestamp.strftime('%Y%m%d')}.csv"
            full_path = os.path.join(output_path, filename)

            # Definicja precyzji zaokrƒÖglenia dla poszczeg√≥lnych kolumn
            rounding_rules = {
                'open': 4,
                'high': 4,
                'low': 4,
                'close': 4,
                'volume': 2,
                'high_change': 6,
                'low_change': 6,
                'close_change': 6,
                'volume_change': 2,
                'price_to_ma1440': 8,
                'price_to_ma43200': 8,
                'volume_to_ma1440': 8,
                'volume_to_ma43200': 8
            }
            
            # ZaokrƒÖglij warto≈õci w kolumnach, kt√≥re istniejƒÖ w DataFrame
            for col, precision in rounding_rules.items():
                if col in sample_df.columns:
                    sample_df[col] = sample_df[col].round(precision)

            sample_df[existing_columns].to_csv(full_path, index=True)

            self.logger.info(f"‚úÖ Pr√≥bka etykiet zapisana pomy≈õlnie do: {full_path}")

        except Exception as e:
            self.logger.error(f"‚ùå WystƒÖpi≈Ç b≈ÇƒÖd podczas zapisywania pr√≥bki etykiet: {e}")
            import traceback
            traceback.print_exc()

    def process_all_pairs(self, input_dir: Optional[Path] = None, 
                         output_dir: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        Przetwarza wszystkie pary z katalogu input
        
        Args:
            input_dir: Katalog z surowymi danymi (domy≈õlnie z config)
            output_dir: Katalog docelowy (domy≈õlnie z config)
            
        Returns:
            list: Lista raport√≥w dla ka≈ºdej pary
        """
        if input_dir is None:
            input_dir = config.INPUT_DATA_PATH
        if output_dir is None:
            output_dir = config.OUTPUT_DATA_PATH
        
        self.logger.info(f"Rozpoczynam przetwarzanie wszystkich par z {input_dir}")
        
        # Znajd≈∫ wszystkie pliki wej≈õciowe
        input_files = find_input_files(input_dir)
        
        if not input_files:
            self.logger.warning(f"Nie znaleziono plik√≥w wej≈õciowych w {input_dir}")
            return []
        
        self.logger.info(f"Znaleziono {len(input_files)} plik√≥w do przetworzenia")
        
        all_reports = []
        successful_pairs = 0
        failed_pairs = 0
        
        for pair_name, file_path in input_files:
            try:
                self.logger.info(f"Przetwarzam parƒô {pair_name} ({successful_pairs + failed_pairs + 1}/{len(input_files)})")
                
                report = self.process_single_pair(pair_name, file_path)
                all_reports.append(report)
                
                if report["success"]:
                    successful_pairs += 1
                    # Zapisz raport dla tej pary
                    self._save_pair_report(report)
                else:
                    failed_pairs += 1
                    self.logger.error(f"Przetwarzanie {pair_name} nieudane: {report.get('error_message', 'Unknown error')}")
                
            except Exception as e:
                failed_pairs += 1
                error_report = {
                    "pair": pair_name,
                    "success": False,
                    "error_message": str(e),
                    "input_file": str(file_path)
                }
                all_reports.append(error_report)
                self.logger.error(f"Krytyczny b≈ÇƒÖd dla {pair_name}: {str(e)}")
        
        # Statystyki ko≈Ñcowe
        final_msg = f"Przetwarzanie zako≈Ñczone: {successful_pairs} sukces, {failed_pairs} b≈Çƒôd√≥w, ≈ÇƒÖcznie {len(input_files)} par"
        
        if config.TRAINING_COMPATIBILITY_MODE and successful_pairs > 0:
            final_msg += f"\nüéØ Training-ready files generated in: {config.OUTPUT_DATA_PATH}"
        
        self.logger.info(final_msg)
        
        return all_reports
    
    def _get_config_snapshot(self) -> Dict[str, Any]:
        """
        Zwraca snapshot aktualnej konfiguracji
        ‚úÖ TRAINING COMPATIBILITY: Dodaje training-specific config
        """
        base_config = {
            "LONG_TP_PCT": config.LONG_TP_PCT,
            "LONG_SL_PCT": config.LONG_SL_PCT,
            "SHORT_TP_PCT": config.SHORT_TP_PCT,
            "SHORT_SL_PCT": config.SHORT_SL_PCT,
            "FUTURE_WINDOW": config.FUTURE_WINDOW,
            "MA_SHORT_WINDOW": config.MA_SHORT_WINDOW,
            "MA_LONG_WINDOW": config.MA_LONG_WINDOW,
            "MAX_CHANGE_THRESHOLD": config.MAX_CHANGE_THRESHOLD,
            "MAX_VOLUME_CHANGE": config.MAX_VOLUME_CHANGE,
            "TIMEFRAME": config.TIMEFRAME
        }
        
        # ‚úÖ TRAINING COMPATIBILITY: Add training-specific config
        if config.TRAINING_COMPATIBILITY_MODE:
            base_config.update({
                "TRAINING_COMPATIBILITY_MODE": config.TRAINING_COMPATIBILITY_MODE,
                "LABEL_OUTPUT_FORMAT": config.LABEL_OUTPUT_FORMAT,
                "LABEL_DTYPE": config.LABEL_DTYPE,
                "STANDARDIZE_TIMESTAMP_FORMAT": config.STANDARDIZE_TIMESTAMP_FORMAT,
                "INCLUDE_TRAINING_METADATA": config.INCLUDE_TRAINING_METADATA
            })
        
        return base_config
    
    def _save_pair_report(self, report: Dict[str, Any]) -> None:
        """
        Zapisuje raport JSON dla pary
        ‚úÖ TRAINING COMPATIBILITY: Enhanced report with training metadata
        """
        try:
            pair_name = report["pair"]
            report_filename = config.get_report_filename(pair_name, config.TIMEFRAME)
            report_path = config.REPORTS_PATH / report_filename
            
            # Upewnij siƒô ≈ºe katalog reports istnieje
            config.REPORTS_PATH.mkdir(parents=True, exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
            self.logger.debug(f"Raport zapisany: {report_path}")
            
        except Exception as e:
            self.logger.error(f"B≈ÇƒÖd podczas zapisywania raportu dla {report.get('pair', 'unknown')}: {e}")

def main():
    """
    G≈Ç√≥wna funkcja uruchamiajƒÖca przetwarzanie
    Uruchomienie: python main.py
    """
    # Ustawienie logowania na poziomie g≈Ç√≥wnym
    logger = setup_logging("main")
    
    logger.info("=" * 80)
    logger.info("URUCHOMIENIE MODU≈ÅU VALIDATION_AND_LABELING")
    # ‚úÖ TRAINING COMPATIBILITY: Enhanced startup message
    if config.TRAINING_COMPATIBILITY_MODE:
        logger.info(f"üéØ TRAINING COMPATIBILITY MODE: {config.LABEL_OUTPUT_FORMAT} labels")
    logger.info("=" * 80)
    
    try:
        # Sprawd≈∫ czy katalogi istniejƒÖ
        if not config.INPUT_DATA_PATH.exists():
            logger.error(f"Katalog input nie istnieje: {config.INPUT_DATA_PATH}")
            logger.info("Utw√≥rz katalog i umie≈õƒá w nim pliki .feather lub .csv z danymi OHLCV")
            return
        
        # Inicjalizuj pipeline
        pipeline = ValidationAndLabelingPipeline()
        
        # Przetw√≥rz wszystkie pary
        reports = pipeline.process_all_pairs()
        
        # Podsumowanie
        if reports:
            successful = sum(1 for r in reports if r.get("success", False))
            total = len(reports)
            
            logger.info("=" * 80)
            logger.info(f"PODSUMOWANIE: {successful}/{total} par przetworzonych pomy≈õlnie")
            
            # ‚úÖ TRAINING COMPATIBILITY: Enhanced summary
            if config.TRAINING_COMPATIBILITY_MODE and successful > 0:
                logger.info(f"üéØ TRAINING-READY OUTPUT:")
                logger.info(f"   Format: {config.LABEL_OUTPUT_FORMAT} labels, {config.LABEL_DTYPE} dtype")
                logger.info(f"   Location: {config.OUTPUT_DATA_PATH}")
                logger.info(f"   Files: *_training_ready.feather")
                logger.info(f"   Ready for: Keras/TensorFlow training pipeline")
            
            logger.info("=" * 80)
            
            if successful < total:
                failed_pairs = [r["pair"] for r in reports if not r.get("success", False)]
                logger.warning(f"Nieudane pary: {failed_pairs}")
        else:
            logger.warning("Brak raport√≥w - prawdopodobnie nie znaleziono plik√≥w do przetworzenia")
    
    except Exception as e:
        logger.error(f"Krytyczny b≈ÇƒÖd w main(): {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 