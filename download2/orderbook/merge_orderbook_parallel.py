#!/usr/bin/env python3
"""
Skrypt r√≥wnoleg≈Çego ≈ÇƒÖczenia danych orderbook w format feather
Przetwarza wiele par jednocze≈õnie w osobnych procesach
"""

import os
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import argparse
from pathlib import Path
import logging
import sys
import subprocess
import time
from typing import List, Dict, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing

# Import konfiguracji
from config import PAIRS, FILE_CONFIG, LOGGING_CONFIG

def setup_logging():
    """Konfiguruje system logowania"""
    log_file = Path("merge_orderbook_parallel.log")
    
    logging.basicConfig(
        level=getattr(logging, LOGGING_CONFIG['level']),
        format=LOGGING_CONFIG['format'],
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    
    return logging.getLogger(__name__)

def process_single_pair(symbol: str) -> Dict:
    """Przetwarza jednƒÖ parƒô w osobnym procesie"""
    start_time = datetime.now()
    
    try:
        # Uruchom skrypt dla jednej pary
        cmd = [sys.executable, "merge_orderbook_to_feather.py", "--symbol", symbol]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=7200)  # 2h timeout
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        if result.returncode == 0:
            return {
                'symbol': symbol,
                'status': 'success',
                'output': result.stdout,
                'error': None,
                'duration': duration,
                'start_time': start_time,
                'end_time': end_time
            }
        else:
            return {
                'symbol': symbol,
                'status': 'error',
                'output': result.stdout,
                'error': result.stderr,
                'duration': duration,
                'start_time': start_time,
                'end_time': end_time
            }
    except subprocess.TimeoutExpired:
        end_time = datetime.now()
        duration = end_time - start_time
        return {
            'symbol': symbol,
            'status': 'timeout',
            'output': None,
            'error': f"Przekroczono limit czasu (2h) dla {symbol}",
            'duration': duration,
            'start_time': start_time,
            'end_time': end_time
        }
    except Exception as e:
        end_time = datetime.now()
        duration = end_time - start_time
        return {
            'symbol': symbol,
            'status': 'exception',
            'output': None,
            'error': str(e),
            'duration': duration,
            'start_time': start_time,
            'end_time': end_time
        }

def merge_orderbook_parallel(max_workers: int = None):
    """G≈Ç√≥wna funkcja r√≥wnoleg≈Çego ≈ÇƒÖczenia order book"""
    logger = setup_logging()
    
    if max_workers is None:
        max_workers = min(4, len(PAIRS), multiprocessing.cpu_count())
    
    logger.info("Rozpoczynam r√≥wnoleg≈Çe ≈ÇƒÖczenie order book")
    logger.info(f"Pary: {', '.join(PAIRS)}")
    logger.info(f"Liczba proces√≥w r√≥wnoleg≈Çych: {max_workers}")
    
    start_time = datetime.now()
    successful_pairs = []
    failed_pairs = []
    
    # KROK 1: Uruchom przetwarzanie r√≥wnoleg≈Çe
    logger.info(f"Uruchamiam {max_workers} proces√≥w r√≥wnoleg≈Çych...")
    logger.info(f"Rozpoczynam przetwarzanie {len(PAIRS)} par...")
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # Submituj wszystkie zadania
        future_to_symbol = {executor.submit(process_single_pair, symbol): symbol for symbol in PAIRS}
        logger.info(f"Wszystkie zadania zosta≈Çy uruchomione. Oczekujƒô na wyniki...")
        
        # Zbierz wyniki z lepszym logowaniem
        completed = 0
        pair_durations = []
        start_waiting = datetime.now()
        last_progress_time = start_waiting
        
        # Dodaj timer dla sprawdzania postƒôpu
        logger.info(f"‚è∞ Pierwsze wyniki powinny pojawiƒá siƒô w ciƒÖgu 5-15 minut...")
        logger.info(f"üìä Logowanie postƒôpu co 30 sekund...")
        
        # Prostsze rozwiƒÖzanie - sprawdzaj co 30 sekund
        last_log_time = datetime.now()
        
        for future in as_completed(future_to_symbol):
            # Loguj postƒôp co 30 sekund
            current_time = datetime.now()
            if (current_time - last_log_time).total_seconds() >= 30:
                elapsed = current_time - start_waiting
                logger.info(f"‚è≥ Czekam na wyniki... (up≈Çynƒô≈Ço: {elapsed:.0f}, uko≈Ñczono: {completed}/{len(PAIRS)})")
                last_log_time = current_time
            
            symbol = future_to_symbol[future]
            completed += 1
            
            # Dodaj informacjƒô o rozpoczƒôciu przetwarzania wyniku
            waiting_time = datetime.now() - start_waiting
            logger.info(f"üéØ Otrzymano wynik dla {symbol} (po {waiting_time:.0f} oczekiwania)")
            
            try:
                result = future.result()
                
                if result['status'] == 'success':
                    successful_pairs.append(symbol)
                    duration = result['duration']
                    pair_durations.append(duration.total_seconds())
                    
                    # Oblicz ≈õredni czas i szacowany czas pozosta≈Çy
                    avg_duration = sum(pair_durations) / len(pair_durations)
                    remaining_pairs = len(PAIRS) - completed
                    estimated_remaining = timedelta(seconds=avg_duration * remaining_pairs / max_workers)
                    
                    logger.info(f"[OK] {symbol} - pomy≈õlnie przetworzono ({completed}/{len(PAIRS)})")
                    logger.info(f"  Czas: {duration}")
                    logger.info(f"  ≈öredni czas na parƒô: {timedelta(seconds=avg_duration):.0f}")
                    logger.info(f"  Szacowany czas pozosta≈Çy: {estimated_remaining:.0f}")
                    
                else:
                    failed_pairs.append(symbol)
                    duration = result['duration']
                    logger.error(f"[ERROR] {symbol} - {result['status']}: {result['error']}")
                    logger.error(f"  Czas: {duration}")
                
                # Progress update z szacowaniem
                progress = (completed / len(PAIRS)) * 100
                elapsed_time = datetime.now() - start_time
                
                if completed > 0:
                    # Szacuj ca≈Çkowity czas na podstawie dotychczasowego postƒôpu
                    estimated_total = elapsed_time * len(PAIRS) / completed
                    estimated_remaining = estimated_total - elapsed_time
                    
                    logger.info(f"üìä POSTƒòP: {completed}/{len(PAIRS)} par ({progress:.1f}%)")
                    logger.info(f"  Czas up≈Çyniony: {elapsed_time:.0f}")
                    logger.info(f"  Szacowany czas ca≈Çkowity: {estimated_total:.0f}")
                    logger.info(f"  Szacowany czas pozosta≈Çy: {estimated_remaining:.0f}")
                    logger.info(f"  Prƒôdko≈õƒá: {completed/elapsed_time.total_seconds()*3600:.1f} par/godzinƒô")
                    logger.info("-" * 50)
                
            except Exception as e:
                failed_pairs.append(symbol)
                logger.error(f"[ERROR] {symbol} - b≈ÇƒÖd krytyczny: {e}")
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    # KROK 2: Podsumowanie
    logger.info(f"\n{'='*60}")
    logger.info(f"R√ìWNOLEG≈ÅE ≈ÅƒÑCZENIE ORDERBOOK ZAKO≈ÉCZONE!")
    logger.info(f"{'='*60}")
    logger.info(f"Uda≈Ço siƒô: {len(successful_pairs)}/{len(PAIRS)} par")
    logger.info(f"Czas ca≈Çkowity: {duration}")
    
    if pair_durations:
        avg_duration = sum(pair_durations) / len(pair_durations)
        min_duration = min(pair_durations)
        max_duration = max(pair_durations)
        
        logger.info(f"Statystyki czas√≥w:")
        logger.info(f"  ≈öredni czas na parƒô: {timedelta(seconds=avg_duration):.0f}")
        logger.info(f"  Najszybsza para: {timedelta(seconds=min_duration):.0f}")
        logger.info(f"  Najwolniejsza para: {timedelta(seconds=max_duration):.0f}")
        logger.info(f"  Prƒôdko≈õƒá: {len(successful_pairs)/duration.total_seconds()*3600:.1f} par/godzinƒô")
    
    logger.info(f"Liczba proces√≥w r√≥wnoleg≈Çych: {max_workers}")
    
    if successful_pairs:
        logger.info(f"Pomy≈õlnie przetworzone pary: {', '.join(successful_pairs)}")
    
    if failed_pairs:
        logger.warning(f"Nieudane pary: {', '.join(failed_pairs)}")
    
    # KROK 3: Zapisz metadane
    metadata = {
        'merge_date': datetime.now().isoformat(),
        'pairs': PAIRS,
        'successful_pairs': successful_pairs,
        'failed_pairs': failed_pairs,
        'duration_seconds': duration.total_seconds(),
        'success_rate': f"{len(successful_pairs)/len(PAIRS)*100:.1f}%",
        'max_workers': max_workers,
        'parallel_processing': True
    }
    
    metadata_file = Path("merge_orderbook_parallel_metadata.json")
    try:
        import json
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
        logger.info(f"Metadane zapisane: {metadata_file}")
    except Exception as e:
        logger.error(f"B≈ÇƒÖd zapisywania metadanych: {e}")
    
    return successful_pairs, failed_pairs

def main():
    """G≈Ç√≥wna funkcja"""
    parser = argparse.ArgumentParser(description='R√≥wnoleg≈Çe ≈ÇƒÖczenie danych order book')
    parser.add_argument('--max-workers', type=int, help='Maksymalna liczba proces√≥w r√≥wnoleg≈Çych')
    
    args = parser.parse_args()
    
    merge_orderbook_parallel(max_workers=args.max_workers)

if __name__ == "__main__":
    main() 