"""
ModuÅ‚ do obliczania cech (features) na podstawie danych OHLC + Order Book.
ObsÅ‚uguje format JSON z danymi order book i Å›wieczkami OHLC.
"""
import logging
import os
import sys
import argparse
import json
from typing import List, Optional, Dict, Any

import pandas as pd
import numpy as np
import bamboo_ta as bta
from scipy import stats

# Dodajemy Å›cieÅ¼kÄ™ do gÅ‚Ã³wnego katalogu, aby importy dziaÅ‚aÅ‚y poprawnie
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

try:
    import feature_calculator_snapshot.config as config
except ImportError:
    import config

def setup_logging():
    """Konfiguruje system logowania."""
    log_dir = config.LOG_DIR
    os.makedirs(log_dir, exist_ok=True)
    
    logging.basicConfig(
        level=config.LOG_LEVEL,
        format=config.LOG_FORMAT,
        handlers=[
            logging.FileHandler(os.path.join(log_dir, config.LOG_FILENAME)),
            logging.StreamHandler(sys.stdout)
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

class OrderBookFeatureCalculator:
    """
    GÅ‚Ã³wna klasa odpowiedzialna za transformacjÄ™ danych OHLCV + Order Book
    do finalnego zbioru cech gotowego do treningu modelu.
    """
    def __init__(self):
        """Inicjalizuje klasÄ™ z parametrami."""
        self.ma_periods = config.MA_WINDOWS
        self.history_window = config.ORDERBOOK_HISTORY_WINDOW
        self.short_window = config.ORDERBOOK_SHORT_WINDOW
        self.momentum_window = config.ORDERBOOK_MOMENTUM_WINDOW
        
        logger.info(f"OrderBookFeatureCalculator zainicjalizowany")
        logger.info(f"MA okresy: {self.ma_periods}")
        logger.info(f"Order book history window: {self.history_window}")

    def load_data(self, file_path: str) -> Optional[pd.DataFrame]:
        """Wczytuje i przygotowuje dane wejÅ›ciowe z pliku JSON."""
        logger.info(f"Wczytywanie danych z: {file_path}")
        if not os.path.exists(file_path):
            logger.error(f"Plik wejÅ›ciowy nie istnieje: {file_path}")
            return None
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            
            logger.info(f"Wczytano {len(json_data)} wierszy danych z JSON.")
            
            # Konwertuj JSON na DataFrame
            df = self._json_to_dataframe(json_data)
            
            logger.info("Dane wczytane i przygotowane pomyÅ›lnie.")
            return df
        except Exception as e:
            logger.error(f"WystÄ…piÅ‚ bÅ‚Ä…d podczas wczytywania danych: {e}", exc_info=True)
            return None

    def _json_to_dataframe(self, json_data: List[Dict[str, Any]]) -> pd.DataFrame:
        """Konwertuje dane JSON na DataFrame."""
        rows = []
        
        for item in json_data:
            # Podstawowe dane OHLCV
            row = {
                'timestamp': pd.to_datetime(item['timestamp']),
                'open': float(item['ohlc']['open']),
                'high': float(item['ohlc']['high']),
                'low': float(item['ohlc']['low']),
                'close': float(item['ohlc']['close']),
                'volume': float(item['ohlc']['volume']),
                'data_quality': item['data_quality']
            }
            
            # Dane order book (jeÅ›li dostÄ™pne)
            if item['data_quality'] == 'complete' and 'orderbook' in item:
                ob = item['orderbook']
                
                # Snapshot 1
                row['snapshot1_timestamp'] = pd.to_datetime(ob['snapshot1']['timestamp'])
                for level, data in ob['snapshot1']['levels'].items():
                    row[f'snapshot1_depth_{level}'] = float(data['depth'])
                    row[f'snapshot1_notional_{level}'] = float(data['notional'])
                
                # Snapshot 2
                row['snapshot2_timestamp'] = pd.to_datetime(ob['snapshot2']['timestamp'])
                for level, data in ob['snapshot2']['levels'].items():
                    row[f'snapshot2_depth_{level}'] = float(data['depth'])
                    row[f'snapshot2_notional_{level}'] = float(data['notional'])
            else:
                # Brak danych order book - wypeÅ‚nij NaN
                for snapshot in ['snapshot1', 'snapshot2']:
                    row[f'{snapshot}_timestamp'] = pd.NaT
                    for level in config.ORDERBOOK_LEVELS:
                        row[f'{snapshot}_depth_{level}'] = np.nan
                        row[f'{snapshot}_notional_{level}'] = np.nan
            
            rows.append(row)
        
        df = pd.DataFrame(rows)
        df.set_index('timestamp', inplace=True)
        df.sort_index(inplace=True)
        
        logger.info(f"Konwersja JSON -> DataFrame: {len(df)} wierszy")
        logger.info(f"Wiersze z kompletnymi danymi order book: {(df['data_quality'] == 'complete').sum()}")
        
        return df

    def calculate_traditional_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Oblicza tradycyjne wskaÅºniki techniczne (bez order book)."""
        logger.info("Obliczanie tradycyjnych wskaÅºnikÃ³w technicznych...")
        
        # Grupa B: ZmiennoÅ›Ä‡ (Volatility)
        logger.info("  -> Obliczanie WstÄ™g Bollingera...")
        bbands = bta.bollinger_bands(df, 'close', period=20, std_dev=2.0)
        df['bb_width'] = (bbands['bb_upper'] - bbands['bb_lower']) / bbands['bb_middle']
        df['bb_position'] = (df['close'] - bbands['bb_lower']) / (bbands['bb_upper'] - bbands['bb_lower'])
        df['bb_position'] = (df['bb_position'] - 0.5) * 2

        # Grupa C: PÄ™d/SiÅ‚a Ruchu (Momentum)
        logger.info("  -> Obliczanie RSI...")
        df['rsi_14'] = bta.relative_strength_index(df, column='close', period=14)['rsi']
        
        logger.info("  -> Obliczanie MACD...")
        macd = bta.macd(df, 'close', short_window=12, long_window=26, signal_window=9)
        df['macd_hist'] = macd['macd_histogram']

        # Grupa D: SiÅ‚a i Kierunku Trendu (Trend)
        logger.info("  -> Obliczanie ADX...")
        df['adx_14'] = self._calculate_manual_adx(df, period=14)

        logger.info("  -> Obliczanie Choppiness Index...")
        df['choppiness_index'] = self._calculate_manual_chop(df, period=14)

        # Åšrednie kroczÄ…ce
        logger.info("Obliczanie Å›rednich kroczÄ…cych...")
        for period in self.ma_periods:
            logger.info(f"  -> MA {period}...")
            df[f'ma_{period}'] = df['close'].rolling(window=period).mean()
        
        # Cechy stosunkowe
        logger.info("Obliczanie cech stosunkowych...")
        df['price_to_ma_60'] = df['close'] / df['ma_60']
        df['price_to_ma_240'] = df['close'] / df['ma_240']
        df['ma_60_to_ma_240'] = df['ma_60'] / df['ma_240']
        df['price_to_ma_1440'] = df['close'] / df['ma_1440']
        
        # Cechy wolumenu
        df['volume_change_norm'] = df['volume'].pct_change().fillna(0)
        
        # Cechy dedykowane
        logger.info("Obliczanie cech dedykowanych...")
        highest_high_15m = df['high'].rolling(window=15).max()
        lowest_low_15m = df['low'].rolling(window=15).min()
        df['whipsaw_range_15m'] = (highest_high_15m - lowest_low_15m) / df['close']

        candle_range = df['high'] - df['low']
        upper_wick = df['high'] - df[['open', 'close']].max(axis=1)
        lower_wick = df[['open', 'close']].min(axis=1) - df['low']

        df['upper_wick_ratio'] = np.where(candle_range > 0, upper_wick / candle_range, 0)
        df['lower_wick_ratio'] = np.where(candle_range > 0, lower_wick / candle_range, 0)
        df['upper_wick_ratio_5m'] = df['upper_wick_ratio'].rolling(window=5).mean()
        df['lower_wick_ratio_5m'] = df['lower_wick_ratio'].rolling(window=5).mean()

        df.drop(columns=['upper_wick_ratio', 'lower_wick_ratio'], inplace=True)
        
        logger.info("Tradycyjne wskaÅºniki obliczone.")
        return df

    def calculate_orderbook_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Oblicza wszystkie 30 cech order book."""
        logger.info("Rozpoczynanie obliczania cech order book...")
        
        # SprawdÅº czy mamy dane order book
        complete_data = df['data_quality'] == 'complete'
        logger.info(f"Wiersze z kompletnymi danymi order book: {complete_data.sum()}/{len(df)}")
        
        if complete_data.sum() == 0:
            logger.warning("Brak kompletnych danych order book - pomijam cechy order book")
            return df
        
        # Grupa A: Cechy Podstawowe (z 2 snapshotÃ³w)
        logger.info("  -> Grupa A: Cechy podstawowe...")
        df = self._calculate_basic_features(df)
        
        # Grupa B: Cechy GÅ‚Ä™bokoÅ›ci PoziomÃ³w
        logger.info("  -> Grupa B: Cechy gÅ‚Ä™bokoÅ›ci poziomÃ³w...")
        df = self._calculate_level_features(df)
        
        # Grupa C: Cechy Dynamiczne
        logger.info("  -> Grupa C: Cechy dynamiczne...")
        df = self._calculate_dynamic_features(df)
        
        # Grupa D: Cechy Historyczne
        logger.info("  -> Grupa D: Cechy historyczne...")
        df = self._calculate_historical_features(df)
        
        # Grupa E: Cechy Korelacyjne
        logger.info("  -> Grupa E: Cechy korelacyjne...")
        df = self._calculate_correlation_features(df)
        
        # Grupa F: Cechy Momentum
        logger.info("  -> Grupa F: Cechy momentum...")
        df = self._calculate_momentum_features(df)
        
        # Grupa G: Cechy Koncentracji
        logger.info("  -> Grupa G: Cechy koncentracji...")
        df = self._calculate_concentration_features(df)
        
        logger.info("Wszystkie cechy order book obliczone.")
        return df

    def _calculate_basic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa A: Cechy Podstawowe (z 2 snapshotÃ³w)."""
        
        # Pomocnicze funkcje
        def calc_bid_sum(prefix):
            return sum(df[f'{prefix}depth_{level}'] for level in config.BID_LEVELS)
        
        def calc_ask_sum(prefix):
            return sum(df[f'{prefix}depth_{level}'] for level in config.ASK_LEVELS)
        
        # 1-2. Stosunek presji kupna do sprzedaÅ¼y
        bid_sum_s1 = calc_bid_sum('snapshot1_')
        ask_sum_s1 = calc_ask_sum('snapshot1_')
        bid_sum_s2 = calc_bid_sum('snapshot2_')
        ask_sum_s2 = calc_ask_sum('snapshot2_')
        
        df['buy_sell_ratio_s1'] = bid_sum_s1 / ask_sum_s1
        df['buy_sell_ratio_s2'] = bid_sum_s2 / ask_sum_s2
        
        # 3-4. NierÃ³wnowaga order book
        df['imbalance_s1'] = (bid_sum_s1 - ask_sum_s1) / (bid_sum_s1 + ask_sum_s1)
        df['imbalance_s2'] = (bid_sum_s2 - ask_sum_s2) / (bid_sum_s2 + ask_sum_s2)
        
        # 5. Zmiana presji miÄ™dzy snapshotami
        df['pressure_change'] = (df['buy_sell_ratio_s2'] - df['buy_sell_ratio_s1']) / df['buy_sell_ratio_s1']
        
        return df

    def _calculate_level_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa B: Cechy GÅ‚Ä™bokoÅ›ci PoziomÃ³w."""
        
        # 6-8. GÅ‚Ä™bokoÅ›Ä‡ na poziomach TP/SL
        df['tp_1pct_depth_s1'] = df['snapshot1_depth_1']
        df['tp_2pct_depth_s1'] = df['snapshot1_depth_2']
        df['sl_1pct_depth_s1'] = df['snapshot1_depth_-1']
        
        # 9-10. Stosunki TP do SL
        df['tp_sl_ratio_1pct'] = df['snapshot1_depth_1'] / df['snapshot1_depth_-1']
        df['tp_sl_ratio_2pct'] = df['snapshot1_depth_2'] / df['snapshot1_depth_-2']
        
        return df

    def _calculate_dynamic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa C: Cechy Dynamiczne (zmiany miÄ™dzy snapshotami)."""
        
        # Pomocnicze funkcje
        def calc_total_depth(prefix):
            return sum(df[f'{prefix}depth_{level}'] for level in config.ORDERBOOK_LEVELS)
        
        def calc_total_notional(prefix):
            return sum(df[f'{prefix}notional_{level}'] for level in config.ORDERBOOK_LEVELS)
        
        total_depth_s1 = calc_total_depth('snapshot1_')
        total_depth_s2 = calc_total_depth('snapshot2_')
        total_notional_s1 = calc_total_notional('snapshot1_')
        total_notional_s2 = calc_total_notional('snapshot2_')
        
        # 11. Zmiana gÅ‚Ä™bokoÅ›ci totalnej
        df['total_depth_change'] = (total_depth_s2 - total_depth_s1) / total_depth_s1
        
        # 12. Zmiana wartoÅ›ci nominalnej
        df['notional_change'] = (total_notional_s2 - total_notional_s1) / total_notional_s1
        
        # 13-14. Zmiany gÅ‚Ä™bokoÅ›ci na poziomach +1/-1
        df['depth_1_change'] = (df['snapshot2_depth_1'] - df['snapshot1_depth_1']) / df['snapshot1_depth_1']
        df['depth_neg1_change'] = (df['snapshot2_depth_-1'] - df['snapshot1_depth_-1']) / df['snapshot1_depth_-1']
        
        return df

    def _calculate_historical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa D: Cechy Historyczne (ostatnie 30 snapshotÃ³w)."""
        
        # Pomocnicze funkcje dla total depth
        def calc_total_depth_series():
            return sum(df[f'snapshot1_depth_{level}'] for level in config.ORDERBOOK_LEVELS)
        
        total_depth = calc_total_depth_series()
        
        # 15. Trend gÅ‚Ä™bokoÅ›ci totalnej
        df['depth_trend'] = total_depth.rolling(window=self.history_window).apply(
            lambda x: stats.linregress(range(len(x)), x)[0] if len(x) == self.history_window else np.nan
        )
        
        # 16. VolatilnoÅ›Ä‡ order book
        df['ob_volatility'] = total_depth.pct_change().rolling(window=self.history_window).std()
        
        # 17. Åšrednia gÅ‚Ä™bokoÅ›Ä‡ historyczna
        df['avg_depth_30'] = total_depth.rolling(window=self.history_window).mean()
        
        # 18. Anomalia gÅ‚Ä™bokoÅ›ci
        rolling_std = total_depth.rolling(window=self.history_window).std()
        df['depth_anomaly'] = (total_depth - df['avg_depth_30']) / rolling_std
        
        # 19-20. Trendy presji kupna/sprzedaÅ¼y
        buy_pressure = sum(df[f'snapshot1_depth_{level}'] for level in config.BID_LEVELS)
        sell_pressure = sum(df[f'snapshot1_depth_{level}'] for level in config.ASK_LEVELS)
        
        df['buy_pressure_trend'] = buy_pressure.rolling(window=self.history_window).apply(
            lambda x: stats.linregress(range(len(x)), x)[0] if len(x) == self.history_window else np.nan
        )
        df['sell_pressure_trend'] = sell_pressure.rolling(window=self.history_window).apply(
            lambda x: stats.linregress(range(len(x)), x)[0] if len(x) == self.history_window else np.nan
        )
        
        return df

    def _calculate_correlation_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa E: Cechy Korelacyjne."""
        
        total_depth = sum(df[f'snapshot1_depth_{level}'] for level in config.ORDERBOOK_LEVELS)
        
        # 21. Korelacja gÅ‚Ä™bokoÅ›ci z cenÄ…
        df['depth_price_corr'] = total_depth.rolling(window=self.history_window).corr(df['close'])
        
        # 22. Korelacja presji z wolumenem
        df['pressure_volume_corr'] = df['buy_sell_ratio_s1'].rolling(window=self.history_window).corr(df['volume'])
        
        return df

    def _calculate_momentum_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa F: Cechy Momentum."""
        
        total_depth = sum(df[f'snapshot1_depth_{level}'] for level in config.ORDERBOOK_LEVELS)
        depth_change = total_depth.pct_change()
        
        # 23. Przyspieszenie zmian gÅ‚Ä™bokoÅ›ci
        df['depth_acceleration'] = depth_change.diff()
        
        # 24. Momentum order book
        df['ob_momentum'] = depth_change.rolling(window=self.momentum_window).mean()
        
        # 25. Breakout signal
        rolling_max = total_depth.rolling(window=self.history_window).max()
        rolling_std = total_depth.rolling(window=self.history_window).std()
        df['breakout_signal'] = (total_depth - rolling_max) / rolling_std
        
        return df

    def _calculate_concentration_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Grupa G: Cechy Koncentracji."""
        
        # Pomocnicze funkcje
        def calc_all_depths():
            return [df[f'snapshot1_depth_{level}'] for level in config.ORDERBOOK_LEVELS]
        
        all_depths = calc_all_depths()
        total_depth = sum(all_depths)
        max_depth = pd.concat(all_depths, axis=1).max(axis=1)
        
        # 26. Koncentracja gÅ‚Ä™bokoÅ›ci
        df['depth_concentration'] = max_depth / total_depth
        
        # 27. Asymetria poziomÃ³w
        near_bid = sum(df[f'snapshot1_depth_{level}'] for level in [-1, -2, -3])
        near_ask = sum(df[f'snapshot1_depth_{level}'] for level in [1, 2, 3])
        df['level_asymmetry'] = (near_ask - near_bid) / total_depth
        
        # 28. Dominacja poziomÃ³w bliskich
        df['near_level_dominance'] = (df['snapshot1_depth_1'] + df['snapshot1_depth_-1']) / total_depth
        
        # 29. Dominacja poziomÃ³w dalekich
        df['far_level_dominance'] = (df['snapshot1_depth_5'] + df['snapshot1_depth_-5']) / total_depth
        
        # 30. StabilnoÅ›Ä‡ order book
        depth_changes = total_depth.pct_change().rolling(window=self.short_window).std()
        df['ob_stability'] = 1 / depth_changes
        
        return df

    def _calculate_manual_adx(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Oblicza wskaÅºnik ADX rÄ™cznie."""
        df_adx = df.copy()
        
        df_adx['tr'] = bta.true_range(df_adx)
        dm_result = bta.directional_movement(df_adx, length=period)
        df_adx['plus_dm'] = dm_result['dmp']
        df_adx['minus_dm'] = dm_result['dmn']

        alpha = 1 / period
        df_adx['plus_di'] = 100 * (df_adx['plus_dm'].ewm(alpha=alpha, adjust=False).mean() / df_adx['tr'].ewm(alpha=alpha, adjust=False).mean())
        df_adx['minus_di'] = 100 * (df_adx['minus_dm'].ewm(alpha=alpha, adjust=False).mean() / df_adx['tr'].ewm(alpha=alpha, adjust=False).mean())

        df_adx['dx'] = 100 * (abs(df_adx['plus_di'] - df_adx['minus_di']) / (df_adx['plus_di'] + df_adx['minus_di']))
        adx = df_adx['dx'].ewm(alpha=alpha, adjust=False).mean()
        
        return adx

    def _calculate_manual_chop(self, df: pd.DataFrame, period: int = 14) -> pd.Series:
        """Oblicza wskaÅºnik Choppiness Index (CHOP) rÄ™cznie."""
        df_chop = df.copy()
        
        df_chop['tr'] = bta.true_range(df_chop)
        sum_tr = df_chop['tr'].rolling(window=period).sum()
        highest_high = df_chop['high'].rolling(window=period).max()
        lowest_low = df_chop['low'].rolling(window=period).min()
        
        chop = 100 * np.log10(sum_tr / (highest_high - lowest_low)) / np.log10(period)
        
        return chop

    def calculate_features(self, df: pd.DataFrame, user_start_dt=None, user_end_dt=None) -> pd.DataFrame:
        """
        Oblicza wszystkie cechy i filtruje do zakresu uÅ¼ytkownika na koÅ„cu.
        
        Args:
            df: DataFrame z danymi OHLCV + Order Book
            user_start_dt: Data poczÄ…tkowa zakresu uÅ¼ytkownika (opcjonalna)
            user_end_dt: Data koÅ„cowa zakresu uÅ¼ytkownika (opcjonalna)
        """
        logger.info("Rozpoczynanie obliczania wszystkich cech...")
        
        # Oblicz tradycyjne cechy na peÅ‚nym zbiorze danych
        df_with_traditional = self.calculate_traditional_features(df.copy())
        
        # Oblicz cechy order book na peÅ‚nym zbiorze danych
        df_with_orderbook = self.calculate_orderbook_features(df_with_traditional)
        
        # UsuÅ„ wiersze z NaN (po obliczeniu wszystkich cech)
        initial_rows = len(df_with_orderbook)
        df_clean = df_with_orderbook.dropna()
        removed_rows = initial_rows - len(df_clean)
        logger.info(f"UsuniÄ™to {removed_rows:,} wierszy z brakujÄ…cymi danymi (NaN).")
        logger.info(f"PozostaÅ‚o {len(df_clean):,} kompletnych wierszy.")
        
        # NOWE: Filtruj do zakresu uÅ¼ytkownika (jeÅ›li podano)
        if user_start_dt is not None and user_end_dt is not None:
            logger.info(f"ðŸ”„ FiltrujÄ™ dane do zakresu uÅ¼ytkownika: {user_start_dt} - {user_end_dt}")
            
            # Dodaj 10 minut historii na poczÄ…tku
            filter_start_dt = user_start_dt - pd.Timedelta(minutes=10)
            filter_end_dt = user_end_dt + pd.Timedelta(days=1)  # CaÅ‚y dzieÅ„ koÅ„cowy
            
            # Filtruj dane
            df_filtered = df_clean[
                (df_clean.index >= filter_start_dt) & 
                (df_clean.index < filter_end_dt)
            ].copy()
            
            logger.info(f"ðŸ“Š Przed filtrowaniem: {len(df_clean):,} wierszy")
            logger.info(f"ðŸ“Š Po filtrowaniu: {len(df_filtered):,} wierszy")
            logger.info(f"â° Zakres wynikowy: {df_filtered.index.min()} do {df_filtered.index.max()}")
            
            df_final = df_filtered
        else:
            logger.info("âš ï¸ Brak zakresu uÅ¼ytkownika - zwracam wszystkie dane")
            df_final = df_clean
        
        # Wybierz finalne kolumny
        raw_columns = ['open', 'high', 'low', 'close', 'volume', 'data_quality']
        feature_columns = [col for col in df_final.columns if col not in raw_columns]
        
        final_columns = raw_columns + feature_columns
        df_result = df_final[final_columns]
        
        logger.info(f"Wybrano finalne kolumny: {len(raw_columns)} surowych i {len(feature_columns)} cech.")
        logger.info(f"Finalne kolumny: {final_columns}")
        
        return df_result

    def save_data(self, df: pd.DataFrame, file_path: str, to_csv: bool = False):
        """Zapisuje DataFrame do formatu feather i opcjonalnie CSV."""
        output_dir = os.path.dirname(file_path)
        os.makedirs(output_dir, exist_ok=True)
        
        feather_path = f"{os.path.splitext(file_path)[0]}.feather"
        df.reset_index().to_feather(feather_path)
        logger.info(f"Dane zapisane pomyÅ›lnie do: {feather_path}")

        if to_csv:
            csv_path = f"{os.path.splitext(file_path)[0]}.csv"
            df.to_csv(csv_path)
            logger.info(f"Dane zapisane pomyÅ›lnie do: {csv_path}")

def main():
    """GÅ‚Ã³wna pÄ™tla programu."""
    setup_logging()
    logger.info("--- Rozpoczynanie procesu obliczania cech Order Book ---")
    
    parser = argparse.ArgumentParser(description="Kalkulator Cech Order Book dla Danych Finansowych")
    parser.add_argument(
        '--input', 
        type=str, 
        default=str(config.INPUT_FILE_PATH),
        help=f"ÅšcieÅ¼ka do pliku wejÅ›ciowego JSON. DomyÅ›lnie: {config.INPUT_FILE_PATH}"
    )
    parser.add_argument(
        '--output', 
        type=str, 
        default=os.path.join(config.OUTPUT_DIR, config.DEFAULT_OUTPUT_FILENAME),
        help="ÅšcieÅ¼ka do pliku wyjÅ›ciowego. DomyÅ›lnie: orderbook_ohlc_features.feather"
    )
    parser.add_argument(
        '--user-start',
        type=str,
        help="Data poczÄ…tkowa zakresu uÅ¼ytkownika (YYYY-MM-DD). JeÅ›li nie podano, nie filtruje."
    )
    parser.add_argument(
        '--user-end',
        type=str,
        help="Data koÅ„cowa zakresu uÅ¼ytkownika (YYYY-MM-DD). JeÅ›li nie podano, nie filtruje."
    )
    parser.add_argument(
        '--to-csv',
        action='store_true',
        help="JeÅ›li podano, zapisuje rÃ³wnieÅ¼ kopiÄ™ wynikowÄ… w formacie .csv"
    )
    args = parser.parse_args()

    # Parsuj daty uÅ¼ytkownika jeÅ›li podano
    user_start_dt = None
    user_end_dt = None
    if args.user_start and args.user_end:
        try:
            user_start_dt = pd.to_datetime(args.user_start)
            user_end_dt = pd.to_datetime(args.user_end)
            logger.info(f"ðŸ“… Zakres uÅ¼ytkownika: {user_start_dt} do {user_end_dt}")
        except Exception as e:
            logger.error(f"BÅ‚Ä…d parsowania dat uÅ¼ytkownika: {e}")
            sys.exit(1)

    calculator = OrderBookFeatureCalculator()
    
    df = calculator.load_data(args.input)
    if df is not None:
        final_df = calculator.calculate_features(df, user_start_dt, user_end_dt)
        calculator.save_data(final_df, args.output, args.to_csv)
        logger.info("--- Proces obliczania cech Order Book zakoÅ„czony pomyÅ›lnie. ---")
        
        feather_path = f"{os.path.splitext(args.output)[0]}.feather"
        logger.info(f"Wynikowy plik (feather): {feather_path}")
        if args.to_csv:
            csv_path = f"{os.path.splitext(args.output)[0]}.csv"
            logger.info(f"Wynikowy plik (csv):    {csv_path}")

if __name__ == "__main__":
    main() 