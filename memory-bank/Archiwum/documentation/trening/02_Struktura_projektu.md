# ðŸ“– STRUKTURA PROJEKTU MODUÅU TRENUJÄ„CEGO V3

## ðŸ“ RZECZYWISTA STRUKTURA PLIKÃ“W

### **GÅ‚Ã³wny Katalog Trenowania**
```
ft_bot_docker_compose/user_data/trening2/
â”œâ”€â”€ ðŸ“„ config.py                    (276 linii) - Standalone configuration
â”œâ”€â”€ ðŸ“„ data_loader.py               (641 linii) - Explicit path data loading  
â”œâ”€â”€ ðŸ“„ sequence_generator.py        (570 linii) - Memory-efficient generators
â”œâ”€â”€ ðŸ“„ model_builder.py             (503 linie) - LSTM architecture builder
â”œâ”€â”€ ðŸ“„ trainer.py                   (724 linie) - Main training pipeline
â”œâ”€â”€ ðŸ“„ utils.py                     (422 linie) - Helper functions & monitoring
â”œâ”€â”€ ðŸ“ __pycache__/                 - Python cache files
â”œâ”€â”€ ðŸ“ inputs/                      - Training-ready data files
â”œâ”€â”€ ðŸ“ outputs/                     - Models, scalers, metadata
â””â”€â”€ ðŸ“ temp/                        - Temporary processing files
```

### **Katalogi Data Flow**
```
ðŸ“ inputs/                          # Training-ready files from validation module
â”œâ”€â”€ BTCUSDT_TF-1m__FW-120__SL-050__TP-100__training_ready.feather
â”œâ”€â”€ ETHUSDT_TF-1m__FW-120__SL-050__TP-100__training_ready.feather
â””â”€â”€ ... (other pairs)

ðŸ“ outputs/                         # Generated artifacts
â”œâ”€â”€ ðŸ“ models/
â”‚   â”œâ”€â”€ ðŸ“ BTCUSDT/
â”‚   â”‚   â”œâ”€â”€ model_BTCUSDT_FW120_SL050_TP100.keras      # TensorFlow model
â”‚   â”‚   â”œâ”€â”€ scaler_BTCUSDT_FW120_SL050_TP100.pkl       # Feature scaler
â”‚   â”‚   â”œâ”€â”€ metadata_BTCUSDT_FW120_SL050_TP100.json    # Training metadata
â”‚   â”‚   â””â”€â”€ ðŸ“ logs/                                    # TensorBoard logs
â”‚   â””â”€â”€ ðŸ“ ETHUSDT/
â”‚       â””â”€â”€ ... (same structure for other pairs)
â””â”€â”€ ðŸ“ reports/                     # Training reports & confusion matrices

ðŸ“ temp/                            # Temporary files (auto-cleanup)
â”œâ”€â”€ temp_sequences_*.npy
â”œâ”€â”€ temp_labels_*.npy  
â””â”€â”€ memory_monitor.log
```

## ðŸ”— **DEPENDENCIES DIAGRAM**

### **File Dependency Graph**
```
config.py                           # â† CORE CONFIG (no dependencies)
    â†‘
    â”œâ”€â”€ data_loader.py              # â† Uses config for paths, parameters
    â”‚   â†‘
    â”‚   â”œâ”€â”€ sequence_generator.py   # â† Uses data_loader output
    â”‚   â”‚   â†‘
    â”‚   â”‚   â””â”€â”€ trainer.py          # â† Orchestrates all components
    â”‚       â†‘
    â”‚       â”œâ”€â”€ model_builder.py    # â† Uses config for architecture
    â”‚       â””â”€â”€ utils.py            # â† Helper functions (optional)
```

### **Import Dependencies**
```python
# config.py - ZERO IMPORTS (core configuration)
# No external dependencies

# data_loader.py
import config                       # â† Local config
from sklearn.preprocessing import   # â† Feature scaling

# sequence_generator.py  
import config                       # â† Local config
import numpy as np                  # â† Memory views

# model_builder.py
import config                       # â† Local config
import tensorflow as tf             # â† Model building

# trainer.py - MAIN ORCHESTRATOR
import config                       # â† Local config
from data_loader import TrainingDataLoader
from sequence_generator import MemoryEfficientDataLoader
from model_builder import DualWindowLSTMBuilder
from utils import monitor_memory_usage

# utils.py
import psutil                       # â† Optional memory monitoring
```

## ðŸŽ¯ **CORE MODULES OVERVIEW**

### 1. **ðŸ“„ config.py - Single Source of Truth**

#### **Responsibility:**
- âœ… **Centralized configuration** - wszystkie parametry w jednym miejscu
- âœ… **Parameter validation** - comprehensive validation functions
- âœ… **Filename generation** - consistent naming based on parameters
- âœ… **Helper functions** - configuration-related utilities

#### **Key Functions:**
```python
# Configuration
PAIR = "BTCUSDT"                    # Crypto pair selection
EPOCHS = 100                        # Training parameters
LSTM_UNITS = [128, 64, 32]         # Model architecture

# Helper functions
get_expected_filename()             # Generate expected input filename
get_model_output_dir()              # Generate output directory
validate_config()                   # Validate all parameters
print_config_summary()              # Display configuration overview
```

#### **Zero Dependencies:**
- âŒ **NO imports** from other modules
- âŒ **NO validation module** dependencies
- âœ… **Pure configuration** - standalone operation

### 2. **ðŸ“„ data_loader.py - Explicit Path Loading**

#### **Responsibility:**
- âœ… **Explicit path configuration** - no auto-detection chaos
- âœ… **Filename parameter parsing** - extract parameters from filename
- âœ… **Feature scaling system** - zero data leakage prevention
- âœ… **Class balancing** - systematic undersampling algorithms
- âœ… **Date filtering** - optional date range support

#### **Key Classes:**
```python
class TrainingDataLoader:
    def __init__(self, explicit_path: str, pair: str)
    def load_training_data() -> Dict[str, Any]      # Main loading function
    def apply_class_balancing(features, labels)     # Balance classes
    def validate_file_exists() -> bool              # Check file existence
    def _split_features_labels(df) -> Tuple        # Split with scaling
```

#### **Innovation - Feature Scaling:**
```python
# ZERO DATA LEAKAGE - proper train/val splitting
def _split_features_labels(self, df, fit_scaler: bool = True):
    if config.ENABLE_FEATURE_SCALING:
        if fit_scaler and not self.scaler_fitted:
            # FIT scaler only on training data
            self.scaler = self._create_scaler()
            features = self.scaler.fit_transform(features)
            self.scaler_fitted = True
        elif self.scaler_fitted:
            # TRANSFORM validation data with train scaler
            features = self.scaler.transform(features)
```

### 3. **ðŸ“„ sequence_generator.py - Memory Efficiency**

#### **Responsibility:**
- âœ… **Memory-efficient generators** - 2-3GB instead of 81GB+
- âœ… **Numpy memory views** - zero-copy data access
- âœ… **Pre-computed labels** - eliminates competitive labeling
- âœ… **Chronological splitting** - proper train/validation split
- âœ… **Batch generation** - on-demand sequence loading

#### **Key Classes:**
```python
class MemoryEfficientDataLoader:
    def create_generators_from_arrays(features, labels)     # Main generator creation
    def generate_sequences_with_labels()                    # Core generator function
    def _chronological_split(features, labels)             # Time-aware splitting
    def test_generator(generator, num_batches)              # Generator validation
```

#### **Memory Innovation:**
```python
# NUMPY MEMORY VIEWS - zero copy
def generate_sequences_with_labels(self):
    for i in range(self.total_sequences):
        start_idx = self.indices[i]
        # Memory view - NO COPY!
        sequence = self.features[start_idx:start_idx + self.window_size]
        label = self.labels[start_idx + self.window_size - 1]
        yield sequence, label
```

### 4. **ðŸ“„ model_builder.py - LSTM Architecture**

#### **Responsibility:**
- âœ… **LSTM model building** - crypto-optimized architecture  
- âœ… **Production callbacks** - checkpointing, early stopping, LR reduction
- âœ… **GPU optimization** - memory growth, auto-fallback to CPU
- âœ… **Model persistence** - .keras format for TensorFlow 2.10+
- âœ… **Memory estimation** - predict training memory requirements

#### **Key Classes:**
```python
class DualWindowLSTMBuilder:
    def build_model(compile_model: bool = True) -> Model    # Build LSTM architecture
    def create_callbacks(model_output_dir: str) -> list     # Production callbacks
    def save_model(model, save_path, format='saved_model')  # Model persistence
    def setup_gpu_memory() -> bool                          # GPU optimization
```

#### **Architecture Design:**
```python
# CRYPTO-OPTIMIZED LSTM STACK
inputs = layers.Input(shape=(120, 8))                      # 120 timesteps, 8 features
x = layers.LSTM(128, return_sequences=True)(inputs)        # First LSTM layer
x = layers.LSTM(64, return_sequences=True)(x)              # Second LSTM layer  
x = layers.LSTM(32, return_sequences=False)(x)             # Final LSTM layer
x = layers.Dense(32, activation='relu')(x)                 # Dense layer 1
x = layers.Dropout(0.3)(x)                                 # Regularization
x = layers.Dense(16, activation='relu')(x)                 # Dense layer 2
x = layers.Dropout(0.3)(x)                                 # Regularization
outputs = layers.Dense(3, activation='softmax')(x)         # 3-class output
```

### 5. **ðŸ“„ trainer.py - Main Pipeline**

#### **Responsibility:**
- âœ… **Training orchestration** - complete training workflow
- âœ… **Component integration** - coordinates all modules
- âœ… **Error handling** - comprehensive error messages
- âœ… **Confusion matrix** - post-training analysis
- âœ… **CLI interface** - parameter override support

#### **Key Classes:**
```python
class StandaloneTrainer:
    def __init__()                                          # Initialize trainer
    def initialize_components()                             # Setup all components
    def load_and_validate_data() -> Dict[str, Any]         # Data loading pipeline
    def create_generators(features, labels) -> Tuple       # Generator creation
    def build_model()                                       # Model building
    def train_model(train_gen, val_gen, callbacks)         # Training execution
    def save_model_and_metadata(data_info)                 # Save results
    def generate_confusion_matrix_report()                 # Post-training analysis
    def run_training()                                      # Main training pipeline
```

#### **Complete Workflow:**
```python
def run_training(self):
    # 1. Initialize all components
    self.initialize_components()
    
    # 2. Load and validate data
    data_info = self.load_and_validate_data()
    
    # 3. Create memory-efficient generators
    train_gen, val_gen = self.create_generators(data_info['features'], data_info['labels'])
    
    # 4. Build model
    self.build_model()
    
    # 5. Setup callbacks
    callbacks_list = self.setup_callbacks()
    
    # 6. Train model
    self.train_model(train_gen, val_gen, callbacks_list)
    
    # 7. Save model and metadata
    self.save_model_and_metadata(data_info)
    
    # 8. Generate confusion matrix
    self.generate_confusion_matrix_report()
```

### 6. **ðŸ“„ utils.py - Helper Functions**

#### **Responsibility:**
- âœ… **Memory monitoring** - real-time memory usage tracking
- âœ… **Data validation** - OHLCV data quality checks
- âœ… **File operations** - directory management
- âœ… **Memory calculations** - estimate memory requirements
- âœ… **System utilities** - optional psutil integration

#### **Key Functions:**
```python
validate_data_quality(df, pair) -> bool                    # Data quality validation
calculate_memory_usage(data_shape, dtype) -> Dict          # Memory calculations
get_system_memory_info() -> Dict                           # System memory info
monitor_memory_usage(process_name) -> None                 # Real-time monitoring
chronological_split(df, train_ratio) -> Tuple             # Time-aware splitting
safe_memory_cleanup()                                      # Garbage collection
```

## ðŸ”„ **DATA FLOW WORKFLOW**

### **Training Pipeline Flow**
```
1. CONFIG LOADING
   config.py â†’ Load parameters, validate configuration

2. DATA LOADING  
   data_loader.py â†’ Load training-ready file, parse filename parameters
   â†“
   Validate file exists: BTCUSDT_TF-1m__FW-120__SL-050__TP-100__training_ready.feather
   â†“
   Apply feature scaling (fit on train, transform on train/val)
   â†“
   Apply class balancing (systematic undersampling or class weights)

3. SEQUENCE GENERATION
   sequence_generator.py â†’ Create memory-efficient generators
   â†“
   Chronological split (train/validation)
   â†“
   Generate sequences using numpy views (zero-copy)

4. MODEL BUILDING
   model_builder.py â†’ Build LSTM architecture from config
   â†“
   Setup production callbacks (checkpointing, early stopping, LR reduction)
   â†“
   Configure GPU optimization

5. TRAINING EXECUTION
   trainer.py â†’ Execute training pipeline
   â†“
   Train model with generators
   â†“
   Monitor progress with callbacks

6. RESULTS SAVING
   trainer.py â†’ Save model, scaler, metadata
   â†“
   Generate confusion matrix report
   â†“
   Save to outputs/models/{PAIR}/
```

### **File Format Specifications**

#### **Input File Format (Feather)**
```python
# Expected columns in training-ready file:
FEATURE_COLUMNS = [
    'high_change', 'low_change', 'close_change', 'volume_change',
    'price_to_ma1440', 'price_to_ma43200', 
    'volume_to_ma1440', 'volume_to_ma43200'
]

LABEL_COLUMNS = [
    'label_0',  # SHORT probability (one-hot encoded)
    'label_1',  # HOLD probability
    'label_2'   # LONG probability
]

# Optional datetime index for date filtering
```

#### **Output File Formats**
```python
# Model: .keras format (TensorFlow 2.10+)
model_BTCUSDT_FW120_SL050_TP100.keras

# Scaler: .pkl format (pickle)
scaler_BTCUSDT_FW120_SL050_TP100.pkl

# Metadata: .json format
metadata_BTCUSDT_FW120_SL050_TP100.json
```

## ðŸ³ **DOCKER ENVIRONMENT STRUCTURE**

### **Docker Paths Configuration**
```python
# EXPLICIT DOCKER PATHS (no relative paths)
TRAINING_DATA_PATH = "/freqtrade/user_data/trening2/inputs/"    # Input data
OUTPUT_BASE_PATH = "/freqtrade/user_data/trening2/outputs/"     # Output artifacts

# Path validation
if not TRAINING_DATA_PATH.startswith('/'):
    raise ValueError("TRAINING_DATA_PATH must be absolute Docker path")
```

### **Container Volume Mapping**
```yaml
# docker-compose.yml structure
volumes:
  - ./ft_bot_docker_compose/user_data:/freqtrade/user_data
  
# Results in:
# Host: ft_bot_docker_compose/user_data/trening2/
# Container: /freqtrade/user_data/trening2/
```

## ðŸ”§ **DEPENDENCY MANAGEMENT**

### **External Dependencies**
```python
# REQUIRED (core functionality)
tensorflow>=2.10.0          # Model building and training
pandas>=1.5.0               # Data manipulation
numpy>=1.21.0               # Numerical computing
pyarrow>=10.0.0             # Feather file support

# REQUIRED (feature scaling)  
scikit-learn>=1.1.0         # StandardScaler, RobustScaler, MinMaxScaler

# OPTIONAL (enhanced functionality)
psutil                      # Memory monitoring (graceful fallback if missing)
```

### **Internal Dependencies**
```python
# ZERO validation module dependencies
# All functionality is self-contained
# Clean separation of concerns
```

---

**ðŸŽ¯ KLUCZOWE ZALETY STRUKTURY V3:**
- âœ… **Modular Design** - kaÅ¼dy plik ma jasnÄ… odpowiedzialnoÅ›Ä‡
- âœ… **Zero Circular Dependencies** - clean import hierarchy  
- âœ… **Docker-Optimized** - explicit paths, production-ready
- âœ… **Memory-Efficient** - generators, views, lazy loading
- âœ… **Config-Driven** - single source of truth
- âœ… **Production-Ready** - error handling, monitoring, validation

**ðŸ“ˆ NEXT:** [03_Config.md](./03_Config.md) - SzczegÃ³Å‚owa dokumentacja konfiguracji 