# PARAMETRY MODELU XGBOOST - PRZEWODNIK

## üéØ **PODSTAWOWE PARAMETRY TRENINGU**

### **1. LICZBA DRZEW (N_ESTIMATORS)**
```python
XGB_N_ESTIMATORS = 300  # Maksymalna liczba drzew
```
- **Co to:** Maksymalna liczba drzew w ensemble
- **Zakres:** 100-1000 (300 to dobry start)
- **Wp≈Çyw:** Wiƒôcej drzew = d≈Çu≈ºszy trening, ryzyko overfitting
- **Optymalizacja:** Early stopping automatycznie wybiera najlepszƒÖ liczbƒô
- **Tw√≥j model:** Zatrzyma≈Ç siƒô na **147 drzewach** (early stopping)

### **2. LEARNING RATE (ETA)**
```python
XGB_LEARNING_RATE = 0.05  # Wsp√≥≈Çczynnik uczenia
```
- **Co to:** Jak szybko model siƒô uczy
- **Zakres:** 0.01 - 0.3 (0.05 to dobry start)
- **Wp≈Çyw:** 
  - Ni≈ºszy = wolniejszy ale stabilniejszy
  - Wy≈ºszy = szybszy ale ryzykowny
- **Rekomendacja:** 0.05-0.1 dla wiƒôkszo≈õci przypadk√≥w
- **Tw√≥j model:** 0.05 - konserwatywne podej≈õcie

### **3. MAX DEPTH**
```python
XGB_MAX_DEPTH = 5  # Maksymalna g≈Çƒôboko≈õƒá drzewa
```
- **Co to:** Jak g≈Çƒôbokie mogƒÖ byƒá pojedyncze drzewa
- **Zakres:** 3-10 (5 to dobry kompromis)
- **Wp≈Çyw:** 
  - G≈Çƒôbsze = bardziej skomplikowane wzorce, ryzyko overfitting
  - P≈Çytsze = prostsze wzorce, mniej overfitting
- **Rekomendacja:** 5-7 dla wiƒôkszo≈õci przypadk√≥w
- **Tw√≥j model:** 5 - umiarkowana z≈Ço≈ºono≈õƒá

### **4. SUBSAMPLE**
```python
XGB_SUBSAMPLE = 0.8  # Procent pr√≥bek do ka≈ºdego drzewa
```
- **Co to:** JakƒÖ czƒô≈õƒá danych u≈ºywa ka≈ºde drzewo
- **Zakres:** 0.5-1.0 (0.8 to standard)
- **Wp≈Çyw:** 
  - Ni≈ºszy = mniej overfitting
  - Wy≈ºszy = lepsze dopasowanie
- **Rekomendacja:** 0.8-0.9
- **Tw√≥j model:** 0.8 - standardowe podej≈õcie

### **5. COLSAMPLE_BYTREE**
```python
XGB_COLSAMPLE_BYTREE = 0.8  # Procent cech do ka≈ºdego drzewa
```
- **Co to:** JakƒÖ czƒô≈õƒá cech u≈ºywa ka≈ºde drzewo
- **Zakres:** 0.5-1.0 (0.8 to standard)
- **Wp≈Çyw:** 
  - Ni≈ºszy = mniej overfitting
  - Wy≈ºszy = lepsze dopasowanie
- **Rekomendacja:** 0.8-0.9
- **Tw√≥j model:** 0.8 - standardowe podej≈õcie

## üîÑ **PARAMETRY EARLY STOPPING**

### **6. EARLY STOPPING ROUNDS**
```python
XGB_EARLY_STOPPING_ROUNDS = 15  # Rundy bez poprawy
```
- **Co to:** Ile rund bez poprawy przed zatrzymaniem
- **Zakres:** 10-50 (15 to dobry start)
- **Wp≈Çyw:** 
  - Ni≈ºszy = szybsze zatrzymanie
  - Wy≈ºszy = wiƒôcej szans na poprawƒô
- **Tw√≥j model:** Zatrzyma≈Ç siƒô po **15 rundach** bez poprawy

### **7. EVAL METRIC**
```python
eval_metric = 'mlogloss'  # Metryka ewaluacji
```
- **Co to:** JakƒÖ metrykƒô u≈ºywa early stopping
- **Opcje:** 
  - 'mlogloss' - standardowa dla klasyfikacji wieloklasowej
  - 'merror' - b≈ÇƒÖd klasyfikacji
  - 'auc' - area under curve
- **Rekomendacja:** 'mlogloss' dla klasyfikacji wieloklasowej
- **Tw√≥j model:** 'mlogloss' - standardowa

## ‚öñÔ∏è **PARAMETRY BALANSOWANIA**

### **8. CLASS WEIGHTS**
```python
CLASS_WEIGHTS = {0: 1.0, 1: 1.0, 2: 1.0}  # WY≈ÅƒÑCZONE
```
- **Co to:** Wagi dla r√≥≈ºnych klas
- **Zakres:** Dowolne warto≈õci dodatnie
- **Wp≈Çyw:** Mo≈ºe pom√≥c z nier√≥wnomiernym rozk≈Çadem klas
- **Przyk≈Çad:** {0: 2.0, 1: 2.0, 2: 0.5} - zwiƒôksza wagƒô LONG/SHORT
- **Tw√≥j model:** Wszystkie klasy r√≥wne (wy≈ÇƒÖczone balansowanie)

### **9. SAMPLE WEIGHTS**
```python
ENABLE_WEIGHTED_LOSS = False  # WY≈ÅƒÑCZONE
```
- **Co to:** Wagi dla poszczeg√≥lnych pr√≥bek
- **Wp≈Çyw:** Mo≈ºe pom√≥c z problematycznymi pr√≥bkami
- **Rekomendacja:** W≈ÇƒÖcz dla nier√≥wnomiernych danych
- **Tw√≥j model:** Wy≈ÇƒÖczone

## üé≤ **PARAMETRY SPECYFICZNE DLA XGBOOST**

### **10. GAMMA**
```python
XGB_GAMMA = 0.1  # Minimalna redukcja straty
```
- **Co to:** Minimalna redukcja straty wymagana do podzia≈Çu
- **Zakres:** 0-10 (0.1 to standard)
- **Wp≈Çyw:** 
  - Wy≈ºszy = mniej podzia≈Ç√≥w, prostszy model
  - Ni≈ºszy = wiƒôcej podzia≈Ç√≥w, bardziej skomplikowany model
- **Rekomendacja:** 0.1-1.0
- **Tw√≥j model:** 0.1 - standardowe

### **11. RANDOM STATE**
```python
XGB_RANDOM_STATE = 42  # Ziarno losowo≈õci
```
- **Co to:** Ziarno dla powtarzalno≈õci wynik√≥w
- **Wp≈Çyw:** Ten sam seed = te same wyniki
- **Rekomendacja:** Ustaw dla powtarzalno≈õci eksperyment√≥w
- **Tw√≥j model:** 42 - standardowe

### **12. N_JOBS**
```python
XGB_N_JOBS = -1  # Liczba proces√≥w
```
- **Co to:** Ile proces√≥w u≈ºywa do treningu
- **Warto≈õci:** 
  - -1 = wszystkie dostƒôpne procesory
  - 1 = jeden proces
  - 4 = 4 procesy
- **Rekomendacja:** -1 dla maksymalnej wydajno≈õci
- **Tw√≥j model:** -1 - wszystkie procesory

## üìä **ANALIZA TWOJEGO MODELU**

### **DODATNIE ASPEKTY:**
- ‚úÖ **Early stopping dzia≈Ça** - model zatrzyma≈Ç siƒô na optymalnej iteracji
- ‚úÖ **Konserwatywne parametry** - niskie ryzyko overfitting
- ‚úÖ **Standardowe warto≈õci** - sprawdzone podej≈õcie
- ‚úÖ **Brak b≈Çƒôd√≥w infinity/NaN** - problem rozwiƒÖzany u ≈∫r√≥d≈Ça

### **MO≈ªLIWE ULEPSZENIA:**
```python
# Dla lepszych wynik√≥w LONG/SHORT:
XGB_LEARNING_RATE = 0.1      # Szybsze uczenie
XGB_MAX_DEPTH = 6            # Wiƒôcej z≈Ço≈ºono≈õci
XGB_SUBSAMPLE = 0.9          # Wiƒôcej danych
XGB_COLSAMPLE_BYTREE = 0.9   # Wiƒôcej cech
XGB_GAMMA = 0.05             # Wiƒôcej podzia≈Ç√≥w
```

## üîß **REKOMENDACJE EKSPERYMENT√ìW**

### **1. EKSPERYMENT 1 - SZYBSZE UCZENIE:**
```python
XGB_LEARNING_RATE = 0.1      # Zwiƒôksz z 0.05
XGB_EARLY_STOPPING_ROUNDS = 20  # Wiƒôcej szans
```

### **2. EKSPERYMENT 2 - WIƒòCEJ Z≈ÅO≈ªONO≈öCI:**
```python
XGB_MAX_DEPTH = 6            # Zwiƒôksz z 5
XGB_GAMMA = 0.05             # Zmniejsz z 0.1
```

### **3. EKSPERYMENT 3 - WIƒòCEJ DANYCH:**
```python
XGB_SUBSAMPLE = 0.9          # Zwiƒôksz z 0.8
XGB_COLSAMPLE_BYTREE = 0.9   # Zwiƒôksz z 0.8
```

### **4. EKSPERYMENT 4 - BALANSOWANIE KLAS:**
```python
CLASS_WEIGHTS = {0: 2.0, 1: 2.0, 2: 0.5}  # W≈ÇƒÖcz balansowanie
ENABLE_WEIGHTED_LOSS = True               # W≈ÇƒÖcz weighted loss
```

## üìà **MONITOROWANIE TRENINGU**

### **1. EARLY STOPPING:**
- **Obserwuj:** Czy model zatrzymuje siƒô za wcze≈õnie
- **Sprawdzaj:** Liczba iteracji vs maksymalna
- **Analizuj:** Czy metryka siƒô stabilizuje

### **2. OVERFITTING:**
- **Por√≥wnuj:** Train accuracy vs Validation accuracy
- **Sprawdzaj:** Czy r√≥≈ºnica ro≈õnie z czasem
- **Analizuj:** Czy validation accuracy spada

### **3. FEATURE IMPORTANCE:**
- **Analizuj:** Kt√≥re cechy sƒÖ najwa≈ºniejsze
- **Sprawdzaj:** Czy wa≈ºno≈õƒá ma sens
- **Optymalizuj:** Usu≈Ñ nieistotne cechy

## üéØ **PODSUMOWANIE**

### **OBECNE PARAMETRY (KONSERWATYWNE):**
- N_ESTIMATORS: 300 (u≈ºyto 147)
- LEARNING_RATE: 0.05
- MAX_DEPTH: 5
- SUBSAMPLE: 0.8
- COLSAMPLE_BYTREE: 0.8
- EARLY_STOPPING_ROUNDS: 15

### **REKOMENDOWANE EKSPERYMENTY:**
1. **Zwiƒôksz learning rate** do 0.1
2. **Zwiƒôksz max_depth** do 6
3. **W≈ÇƒÖcz class weights** dla lepszego balansu
4. **Zwiƒôksz subsample** do 0.9

### **KLUCZOWE METRYKI DO OBSERWOWANIA:**
- **Early stopping** - czy dzia≈Ça poprawnie
- **Overfitting** - r√≥≈ºnica train/val
- **Feature importance** - kt√≥re cechy sƒÖ wa≈ºne
- **Class balance** - czy LONG/SHORT sƒÖ rozpoznawane
