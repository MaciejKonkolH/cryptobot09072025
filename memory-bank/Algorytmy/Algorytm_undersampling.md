# ðŸŽ¯ SEQUENCE-AWARE UNDERSAMPLING ALGORITHM

## ðŸ“– **OPIS DLA CZÅOWIEKA - INTUICYJNY**

### **ðŸ” Problem do rozwiÄ…zania:**
W danych finansowych czasowych mamy problem podwÃ³jny:
1. **Class imbalance** - za duÅ¼o etykiet HOLD (70%), za maÅ‚o SHORT/LONG (15% kaÅ¼da)
2. **Temporal continuity** - LSTM potrzebuje ciÄ…gÅ‚ych sekwencji czasowych Å¼eby siÄ™ uczyÄ‡

### **âŒ Dlaczego tradycyjny undersampling nie dziaÅ‚a:**
Klasyczny undersampling usuwa co N-tÄ… prÃ³bkÄ™ (np. co 5-tÄ…). To niszczy ciÄ…gÅ‚oÅ›Ä‡ czasowÄ…:
- OryginaÅ‚: [t0, t1, t2, t3, t4, t5, t6, t7, t8, t9]
- Po undersampling: [t0, t5] 
- LSTM dostaje sekwencjÄ™ [t0, t5] - ale to nie sÄ… kolejne minuty!

### **âœ… Nasz algorytm:**
**Kluczowa idea:** Rozdzielamy problem balansowania od problemu sekwencji.

**Jak to dziaÅ‚a:**
1. **Oryginalne dane zostajÄ… nietkniÄ™te** - peÅ‚na historia czasowa [t0, t1, t2, ..., tN]
2. **Undersampling wybiera tylko KTÃ“RE momenty czasowe uÅ¼yÄ‡ do treningu** 
3. **Dla kaÅ¼dego wybranego momentu, sekwencja input jest zawsze z oryginalnych danych**

**PrzykÅ‚ad:**
- Undersampling wybraÅ‚ moment t500 do treningu
- Target (y) = etykieta z t500
- Sequence (X) = poprzednie 120 minut z ORYGINALNYCH danych [t380, t381, t382, ..., t499]
- Model dostaje naturalnÄ…, ciÄ…gÅ‚Ä… sekwencjÄ™ czasowÄ…!

### **ðŸŽ¯ Efekt:**
- âœ… Eliminujemy nadmiar nudnych momentÃ³w HOLD (balansowanie klas)
- âœ… Zachowujemy naturalne wzorce czasowe (LSTM moÅ¼e siÄ™ uczyÄ‡)
- âœ… Model widzi rzeczywiste przejÅ›cia miÄ™dzy stanami rynku

---

## ðŸ¤– **OPIS DLA AGENTÃ“W AI - TECHNICZNY**

### **ALGORITHM: Sequence-Aware Temporal Undersampling**

**INPUT:**
- `time_series_data`: Array[N, features] - complete temporal dataset
- `labels`: Array[N] - labels for each timestamp
- `window_size`: int - LSTM sequence length (e.g., 120)
- `target_balance`: Dict[class] -> float - desired class distribution

**CORE PRINCIPLE:**
Decouple sampling strategy from sequence generation. Sample target indices while preserving temporal continuity for feature sequences.

**ALGORITHM STEPS:**

1. **TARGET SELECTION PHASE:**
   ```
   eligible_indices = range(window_size, N)  # indices that can have full history
   class_counts = count_labels_per_class(labels[eligible_indices])
   minority_class_size = min(class_counts.values())
   
   selected_indices = []
   for class_id in classes:
       class_indices = find_indices_for_class(eligible_indices, class_id)
       target_count = calculate_target_count(minority_class_size, target_balance[class_id])
       sampled_indices = systematic_sample(class_indices, target_count)
       selected_indices.extend(sampled_indices)
   
   selected_indices.sort()  # maintain temporal order of training examples
   ```

2. **SEQUENCE GENERATION PHASE:**
   ```
   for target_idx in selected_indices:
       sequence_start = target_idx - window_size
       sequence_end = target_idx
       
       X_sequence = time_series_data[sequence_start:sequence_end]  # ORIGINAL data
       y_target = labels[target_idx]  # SAMPLED target
       
       training_batch.append((X_sequence, y_target))
   ```

**KEY PROPERTIES:**
- `X_sequences` are always temporally continuous from original dataset
- `y_targets` are class-balanced through selective sampling
- No data interpolation or synthetic generation
- Maintains causal relationships in temporal patterns
- Preserves LSTM's ability to learn temporal dependencies

**MATHEMATICAL GUARANTEE:**
```
âˆ€ sequence X_i: X_i = time_series_data[j:j+window_size] for some j
âˆ€ consecutive elements in X_i: temporal_gap = 1 time_unit
Class_distribution(sampled_targets) â‰ˆ target_balance
```

**COMPLEXITY:**
- Time: O(N) for sampling + O(M * window_size) for sequence generation, where M = selected samples
- Space: O(M * window_size) for training data
- Original dataset memory: O(N * features) - unchanged

**ADVANTAGES:**
1. Resolves class imbalance without destroying temporal patterns
2. LSTM receives natural progression of market states
3. Model can learn transition patterns between classes
4. No artificial data generation or time-series interpolation
5. Maintains causality: past â†’ present relationships intact

**USE CASE:**
Optimal for time-series classification where:
- Class imbalance exists in temporal data
- Sequential patterns are crucial for prediction
- Model architecture expects continuous temporal input (LSTM/GRU/Transformer)
- Temporal gaps would destroy learning signal
