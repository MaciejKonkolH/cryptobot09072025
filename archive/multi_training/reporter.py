"""
Modu≈Ç odpowiedzialny za generowanie szczeg√≥≈Çowego raportu ko≈Ñcowego.
"""
import time
from datetime import timedelta
import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

from training import config as cfg

class TrainingReporter:
    """
    Generuje i wy≈õwietla kompleksowy raport podsumowujƒÖcy proces treningu.
    """
    def __init__(self, trainer_instance):
        self.trainer = trainer_instance
        self.history = trainer_instance.history
        self.test_results = trainer_instance.evaluation_results
        
        # Przechowujemy surowe prawdopodobie≈Ñstwa do analizy
        self.raw_probabilities = trainer_instance.raw_probabilities
        
        # Stosujemy pr√≥g pewno≈õci (je≈õli w≈ÇƒÖczony)
        if cfg.ENABLE_CONFIDENCE_THRESHOLDING:
            self.filtered_predictions, self.threshold_stats = self._apply_confidence_thresholding()
            self.final_predictions = self.filtered_predictions[self.filtered_predictions != -1]
            self.final_true_labels = trainer_instance.true_labels[self.filtered_predictions != -1]
        else:
            self.filtered_predictions = trainer_instance.predictions
            self.threshold_stats = None
            self.final_predictions = trainer_instance.predictions
            self.final_true_labels = trainer_instance.true_labels

    def print_summary(self):
        """Drukuje pe≈Çny, sformatowany raport ko≈Ñcowy."""
        print(self._format_header("TRENING - RAPORT KO≈ÉCOWY"))
        print(self._format_section_data())
        print(self._format_section_model())
        print(self._format_section_training_results())
        if self.threshold_stats:
            print(self._format_section_thresholding_stats())
        print(self._format_section_test_evaluation())
        print(self._format_footer())

    def _format_header(self, title: str) -> str:
        """Formatuje g≈Ç√≥wny nag≈Ç√≥wek raportu."""
        border = "=" * 80
        return f"\n{border}\nüéØ {title:^76} üéØ\n{border}"

    def _format_footer(self) -> str:
        """Formatuje stopkƒô raportu."""
        return "=" * 80 + "\n"
    
    def _format_section_data(self) -> str:
        """Formatuje sekcjƒô z informacjami o danych."""
        start_time = self.trainer.start_time
        end_time = self.trainer.end_time
        total_time = timedelta(seconds=end_time - start_time)
        
        report = [
            "üìä DANE:",
            f"   Plik ≈∫r√≥d≈Çowy: {cfg.INPUT_FILENAME}",
            f"   Ca≈Çkowity czas: {str(total_time)}",
            f"   Rozmiary zbior√≥w:",
            f"     - Treningowy: {self.trainer.train_df.shape}",
            f"     - Walidacyjny: {self.trainer.val_df.shape}",
            f"     - Testowy: {self.trainer.test_df.shape}"
        ]
        return "\n".join(report)

    def _format_section_model(self) -> str:
        """Formatuje sekcjƒô z informacjami o architekturze modelu."""
        report = [
            "\nüß† MODEL:",
            f"   Architektura: LSTM {cfg.LSTM_UNITS} + Dense {cfg.DENSE_UNITS}",
            f"   D≈Çugo≈õƒá sekwencji: {cfg.SEQUENCE_LENGTH}",
            f"   Dropout: {cfg.DROPOUT_RATE}",
            f"   Learning Rate: {cfg.LEARNING_RATE}"
        ]
        return "\n".join(report)

    def _format_section_training_results(self) -> str:
        """Formatuje sekcjƒô z wynikami treningu."""
        best_epoch = np.argmin(self.history.history['val_loss']) + 1
        final_val_loss = self.history.history['val_loss'][-1]
        best_val_loss = self.history.history['val_loss'][best_epoch - 1]
        final_val_acc = self.history.history['val_accuracy'][-1]
        best_val_acc = self.history.history['val_accuracy'][best_epoch - 1]

        report = [
            "\nüèãÔ∏è WYNIKI TRENINGU:",
            f"   Liczba epok: {len(self.history.epoch)} / {cfg.EPOCHS}",
            f"   Najlepsza epoka (wg val_loss): {best_epoch}",
            f"   Ko≈Ñcowa strata walidacyjna: {final_val_loss:.4f}",
            f"   Najlepsza strata walidacyjna: {best_val_loss:.4f}",
            f"   Ko≈Ñcowa dok≈Çadno≈õƒá walidacyjna: {final_val_acc:.4f}",
            f"   Najlepsza dok≈Çadno≈õƒá walidacyjna: {best_val_acc:.4f}"
        ]
        return "\n".join(report)

    def _format_section_test_evaluation(self) -> str:
        """Formatuje sekcjƒô z ewaluacjƒÖ na zbiorze testowym."""
        report = [
            "\nüß™ EWALUACJA NA ZBIORZE TESTOWYM:",
            f"   Strata: {self.test_results['test_loss']:.4f}",
            f"   Dok≈Çadno≈õƒá: {self.test_results['test_accuracy']:.4f}",
        ]

        # Generowanie raportu klasyfikacji
        class_names = ['SHORT (0)', 'HOLD (1)', 'LONG (2)']
        class_report = classification_report(
            self.final_true_labels, 
            self.final_predictions, 
            target_names=class_names,
            digits=4
        )
        report.append("\n" + class_report)
        
        # Generowanie macierzy pomy≈Çek
        cm = confusion_matrix(self.final_true_labels, self.final_predictions)
        cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)
        
        report.append("\nMacierz pomy≈Çek (dla zaakceptowanych predykcji):")
        report.append(str(cm_df))

        return "\n".join(report)

    def _apply_confidence_thresholding(self) -> tuple[np.ndarray, dict]:
        """Stosuje pr√≥g pewno≈õci do surowych predykcji modelu."""
        thresholds = np.array([
            cfg.CONFIDENCE_THRESHOLDS.get(0, 0), # SHORT
            cfg.CONFIDENCE_THRESHOLDS.get(1, 0), # HOLD
            cfg.CONFIDENCE_THRESHOLDS.get(2, 0)  # LONG
        ])

        # Pobierz klasƒô z najwy≈ºszym prawdopodobie≈Ñstwem
        predicted_classes = np.argmax(self.raw_probabilities, axis=1)
        # Pobierz odpowiadajƒÖce jej prawdopodobie≈Ñstwo
        max_probabilities = np.max(self.raw_probabilities, axis=1)
        # Pobierz pr√≥g dla tej klasy
        class_thresholds = thresholds[predicted_classes]

        # Predykcja jest akceptowana, je≈õli jej prawdopodobie≈Ñstwo >= pr√≥g dla tej klasy
        accepted_mask = max_probabilities >= class_thresholds
        
        final_predictions = np.full_like(predicted_classes, -1) # -1 dla odrzuconych
        final_predictions[accepted_mask] = predicted_classes[accepted_mask]

        stats = {
            'total_predictions': len(self.raw_probabilities),
            'accepted': np.sum(accepted_mask),
            'rejected': np.sum(~accepted_mask)
        }
        return final_predictions, stats

    def _format_section_thresholding_stats(self) -> str:
        """Formatuje sekcjƒô ze statystykami filtrowania predykcji."""
        total = self.threshold_stats['total_predictions']
        accepted = self.threshold_stats['accepted']
        rejected = self.threshold_stats['rejected']
        
        report = [
            "\nTHRESHOLDING - FILTROWANIE PREDYKCJI:",
            f"   Pr√≥g pewno≈õci: SHORT={cfg.CONFIDENCE_THRESHOLDS[0]}, HOLD={cfg.CONFIDENCE_THRESHOLDS[1]}, LONG={cfg.CONFIDENCE_THRESHOLDS[2]}",
            f"   Zaakceptowano: {accepted:,} / {total:,} ({accepted/total:.2%})",
            f"   Odrzucono: {rejected:,} / {total:,} ({rejected/total:.2%})"
        ]
        return "\n".join(report) 