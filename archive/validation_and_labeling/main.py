"""
GÅ‚Ã³wny moduÅ‚ przetwarzania danych
Orchestrator dla caÅ‚ego pipeline'u validation_and_labeling
âœ… TRAINING COMPATIBILITY: ObsÅ‚uguje generowanie training-ready output
"""
import logging
import json
import sys
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional

import pandas as pd
import numpy as np

# ObsÅ‚uga importÃ³w - zarÃ³wno jako moduÅ‚ jak i standalone script
try:
    from . import config
    from .utils import (
        setup_logging, find_input_files, load_data_file, save_data_file,
        cleanup_partial_file, get_memory_usage_mb, PerformanceTimer
    )
    from .data_validator import DataValidator
    from .feature_calculator import FeatureCalculator, FeatureQualityValidator, FeatureDistributionAnalyzer
    from .competitive_labeler import CompetitiveLabeler, LabelingStatistics
except ImportError:
    # Standalone script - dodaj bieÅ¼Ä…cy katalog do sys.path
    sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
    
    import config
    from utils import (
        setup_logging, find_input_files, load_data_file, save_data_file,
        cleanup_partial_file, get_memory_usage_mb, PerformanceTimer
    )
    from data_validator import DataValidator
    from feature_calculator import FeatureCalculator, FeatureQualityValidator, FeatureDistributionAnalyzer
    from competitive_labeler import CompetitiveLabeler, LabelingStatistics

class NumpyEncoder(json.JSONEncoder):
    """JSON Encoder ktÃ³ry obsÅ‚uguje numpy types"""
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (bool, np.bool)):
            return bool(obj)
        return super().default(obj)

class ValidationAndLabelingPipeline:
    """
    GÅ‚Ã³wna klasa Pipeline dla przetwarzania danych
    âœ… TRAINING COMPATIBILITY: Generuje training-ready output
    """
    
    def __init__(self):
        self.logger = setup_logging(f"{__name__}.ValidationAndLabelingPipeline")
        
        # Inicjalizuj komponenty
        self.data_validator = DataValidator()
        self.feature_calculator = FeatureCalculator()
        self.feature_quality_validator = FeatureQualityValidator()
        self.feature_distribution_analyzer = FeatureDistributionAnalyzer()
        self.competitive_labeler = CompetitiveLabeler()
        self.labeling_statistics = LabelingStatistics()
        
        self.logger.info("Pipeline validation_and_labeling initialized")
        
        # âœ… TRAINING COMPATIBILITY: Zaloguj konfiguracjÄ™
        if config.TRAINING_COMPATIBILITY_MODE:
            self.logger.info(f"ðŸŽ¯ TRAINING COMPATIBILITY MODE: ENABLED")
            self.logger.info(f"Label format: {config.LABEL_OUTPUT_FORMAT}, dtype: {config.LABEL_DTYPE}")
            self.logger.info(f"Output files: *_training_ready.feather")
        else:
            self.logger.info("Training compatibility mode: DISABLED")
        
        self.logger.info(f"Konfiguracja: TP={config.LONG_TP_PCT}%, SL={config.LONG_SL_PCT}%, FW={config.FUTURE_WINDOW}min")
    
    def process_single_pair(self, pair_name: str, input_file_path: Path) -> Dict[str, Any]:
        """
        Przetwarza pojedynczÄ… parÄ™ walutowÄ…
        âœ… TRAINING COMPATIBILITY: Generuje training-ready output z metadata
        
        Args:
            pair_name: Nazwa pary (np. BTCUSDT)
            input_file_path: ÅšcieÅ¼ka do pliku wejÅ›ciowego
            
        Returns:
            dict: Raport przetwarzania z training compatibility metadata
        """
        self.logger.info(f"Rozpoczynam przetwarzanie pary {pair_name}")
        
        timer = PerformanceTimer()
        timer.start()
        
        # Przygotuj raport
        processing_report = {
            "pair": pair_name,
            "input_file": str(input_file_path),
            "config_snapshot": self._get_config_snapshot(),
            "success": False
        }
        
        # âœ… TRAINING COMPATIBILITY: Dodaj metadata do raportu
        if config.TRAINING_COMPATIBILITY_MODE:
            processing_report["training_compatibility"] = {
                "enabled": True,
                "metadata": config.get_training_metadata(),
                "output_filename_suffix": "training_ready"
            }
        
        output_file_path = None
        
        try:
            # KROK 1: ZaÅ‚aduj surowe dane
            self.logger.info(f"ÅadujÄ™ dane z {input_file_path}")
            raw_data = load_data_file(input_file_path)
            self.logger.info(f"ZaÅ‚adowano {len(raw_data):,} wierszy surowych danych")
            processing_report["input_rows"] = len(raw_data)
            
            # KROK 2: Walidacja i czyszczenie danych
            self.logger.info("Rozpoczynam walidacjÄ™ i czyszczenie danych")
            validated_data, validation_report = self.data_validator.validate_and_clean(raw_data, pair_name)
            processing_report["validation_report"] = validation_report
            
            # ðŸ†• KROK 2.5: Zapisz raw validated data dla strategii (jeÅ›li wÅ‚Ä…czone)
            if config.SAVE_RAW_VALIDATED_DATA:
                self.logger.info("ZapisujÄ™ raw validated data w formacie FreqTrade")
                
                # Przygotuj nazwÄ™ pliku w formacie FreqTrade
                # BTCUSDT -> BTC_USDT_USDT-1m-futures.feather
                freqtrade_pair_name = pair_name.replace('/', '_').replace(':', '_')
                raw_validated_filename = f"{freqtrade_pair_name}-1m-futures.feather"
                raw_validated_path = config.RAW_VALIDATED_OUTPUT_PATH / raw_validated_filename
                
                # Przygotuj dane w formacie FreqTrade
                freqtrade_data = validated_data.reset_index()
                
                # ZmieÅ„ nazwÄ™ kolumny datetime -> date (wymagane przez FreqTrade)
                freqtrade_data.rename(columns={'datetime': 'date'}, inplace=True)
                
                # ðŸŽ¯ KLUCZOWA POPRAWKA: Poprawnie przypisz strefÄ™ czasowÄ… UTC, nie konwertuj!
                # Dane sÄ… juÅ¼ w UTC, wiÄ™c tylko to deklarujemy.
                freqtrade_data['date'] = pd.to_datetime(freqtrade_data['date']).dt.tz_localize('UTC')
                
                # Zapisz w formacie kompatybilnym z FreqTrade
                freqtrade_data.to_feather(raw_validated_path)
                
                self.logger.info(f"ðŸ’¾ Zapisano raw validated data (FreqTrade format): {raw_validated_path}")
                self.logger.info(f"ðŸ“Š Format: date={freqtrade_data['date'].dtype}, shape={freqtrade_data.shape}")
                processing_report["raw_validated_file"] = str(raw_validated_path)

            # == EARLY EXIT: JeÅ›li LABELING jest wyÅ‚Ä…czony w konfiguracji ==
            if not getattr(config, "ENABLE_LABELING", True):
                self.logger.info("âš ï¸  Labeling disabled via config.ENABLE_LABELING = False. ZakoÅ„czono po walidacji danych.")

                # Finalize timings
                timer.stop()
                processing_report["processing_time_seconds"] = timer.elapsed_seconds()
                processing_report["memory_usage_mb"] = get_memory_usage_mb()
                processing_report["success"] = True

                # Zapisz raport i zakoÅ„cz
                report_path = config.REPORTS_PATH / f"{pair_name}_validation_only_report.json"
                with open(report_path, "w", encoding="utf-8") as f:
                    json.dump(processing_report, f, indent=4, cls=NumpyEncoder)
                self.logger.info(f"âœ… Validation-only processing finished. Report saved to {report_path}")
                return processing_report

            # === CONTINUE NORMAL PIPELINE WHEN LABELING ENABLED ===
            # KROK 3: Obliczanie features technicznych
            self.logger.info("Rozpoczynam obliczanie features technicznych")
            features_data, features_report = self.feature_calculator.calculate_features(validated_data, pair_name)
            processing_report["features_report"] = features_report
            
            # KROK 4: Walidacja jakoÅ›ci features
            self.logger.info("Rozpoczynam walidacjÄ™ jakoÅ›ci features")
            features_quality_report = self.feature_quality_validator.validate_features_quality(
                features_data, pair_name
            )
            processing_report["features_report"]["quality_validation"] = features_quality_report
            
            # KROK 4.5: Analiza rozkÅ‚adu wartoÅ›ci features
            self.logger.info("Rozpoczynam analizÄ™ rozkÅ‚adu wartoÅ›ci features")
            distribution_report = self.feature_distribution_analyzer.analyze_feature_distributions(
                features_data, pair_name
            )
            processing_report["features_report"]["distribution_analysis"] = distribution_report
            
            # KROK 5: Competitive labeling (z dostÄ™pem do OHLC)
            self.logger.info("Rozpoczynam competitive labeling")
            labeled_data, labeling_report = self.competitive_labeler.generate_labels_with_ohlc(
                features_data, validated_data, pair_name
            )
            processing_report["labeling_report"] = labeling_report
            
            # KROK 6: Statystyki koÅ„cowe
            # âœ… TRAINING COMPATIBILITY: Handle different label formats for statistics
            if config.LABEL_OUTPUT_FORMAT == "onehot":
                # For onehot format, pass the DataFrame to handle properly
                label_stats = self.labeling_statistics.analyze_label_distribution(
                    labeled_data[['label_0', 'label_1', 'label_2']]
                )
            else:
                # For int8/sparse_categorical, pass the label column
                label_stats = self.labeling_statistics.analyze_label_distribution(labeled_data['label'])
            
            processing_report["labeling_report"]["detailed_statistics"] = label_stats
            
            # KROK 7: Zapisz finalne dane
            output_filename = config.get_output_filename(pair_name, config.TIMEFRAME)
            output_file_path = config.OUTPUT_DATA_PATH / output_filename
            
            self.logger.info(f"ZapisujÄ™ finalne dane do {output_file_path}")
            
            # âœ… TRAINING COMPATIBILITY: Log format info
            if config.TRAINING_COMPATIBILITY_MODE:
                if config.LABEL_OUTPUT_FORMAT == "onehot":
                    label_cols = ['label_0', 'label_1', 'label_2']
                    self.logger.info(f"ðŸŽ¯ Training-ready format: Features + One-hot labels ({label_cols})")
                else:
                    self.logger.info(f"ðŸŽ¯ Training-ready format: Features + {config.LABEL_OUTPUT_FORMAT} labels")
            
            # SprawdÅº czy plik istnieje i loguj nadpisanie
            if output_file_path.exists():
                self.logger.warning(f"NadpisujÄ™ istniejÄ…cy plik: {output_file_path}")
            
            # âœ… ZACHOWAJ TIMESTAMP - konwertuj datetime index na kolumnÄ™ przed zapisem
            labeled_data_with_timestamp = labeled_data.reset_index()
            if 'datetime' not in labeled_data_with_timestamp.columns and labeled_data_with_timestamp.columns[0] != 'datetime':
                # JeÅ›li pierwsza kolumna to datetime index, zmieÅ„ nazwÄ™
                labeled_data_with_timestamp.rename(columns={labeled_data_with_timestamp.columns[0]: 'datetime'}, inplace=True)
            
            # ðŸŽ¯ KLUCZOWA POPRAWKA: Upewnij siÄ™, Å¼e dane treningowe majÄ… strefÄ™ czasowÄ… UTC
            if 'datetime' in labeled_data_with_timestamp.columns:
                self.logger.info("KonwertujÄ™ kolumnÄ™ 'datetime' na Å›wiadomÄ… strefy czasowej UTC.")
                labeled_data_with_timestamp['datetime'] = pd.to_datetime(labeled_data_with_timestamp['datetime'], utc=True)

            # âœ… KOMPATYBILNOÅšÄ† Z MODUÅEM TRENUJÄ„CYM - zmieÅ„ 'datetime' na 'timestamp'
            if 'datetime' in labeled_data_with_timestamp.columns:
                labeled_data_with_timestamp.rename(columns={'datetime': 'timestamp'}, inplace=True)
                self.logger.info("ðŸ”„ Renamed 'datetime' column to 'timestamp' for training module compatibility")
            
            self.logger.info(f"ðŸ’¾ ZapisujÄ™ dane z timestamp: {labeled_data_with_timestamp.shape} (kolumny: {list(labeled_data_with_timestamp.columns)})")
            
            save_data_file(labeled_data_with_timestamp, output_file_path, overwrite=config.OVERWRITE_FILES)
            processing_report["output_file"] = str(output_file_path)
            
            # KROK 8: Finalne statystyki
            timer.stop()
            processing_report["processing_time_seconds"] = timer.elapsed_seconds()
            processing_report["memory_usage_mb"] = get_memory_usage_mb()
            processing_report["success"] = True
            
            # KROK 9: Zapisz prÃ³bkÄ™ etykiet do CSV dla weryfikacji wizualnej
            self._save_label_sample_to_csv(labeled_data, pair_name)
            
            # Przygotuj statystyki do raportu
            processing_report["statistics"] = {
                "total_rows": len(labeled_data),
                "input_rows": len(raw_data),
                "gaps_filled": validation_report.get("gaps_filled", 0),
                "duplicates_removed": validation_report.get("duplicates_removed", 0),
                "features_anomalies": features_quality_report.get("extreme_values", {}),
                "label_distribution": labeling_report["label_distribution"],
                "processing_time_seconds": processing_report["processing_time_seconds"],
                "rows_per_second": timer.rows_per_second(len(labeled_data)),
                "memory_usage_mb": processing_report["memory_usage_mb"]
            }
            
            # âœ… TRAINING COMPATIBILITY: Add training-specific statistics
            if config.TRAINING_COMPATIBILITY_MODE:
                processing_report["statistics"]["training_compatibility"] = {
                    "label_format": config.LABEL_OUTPUT_FORMAT,
                    "features_count": len(config.get_training_metadata()["features_list"]),
                    "data_shape": list(labeled_data.shape),
                    "memory_efficient": config.LABEL_OUTPUT_FORMAT != "onehot",
                    "ready_for_keras": True
                }
            
            success_msg = f"Przetwarzanie {pair_name} zakoÅ„czone pomyÅ›lnie: {len(labeled_data):,} wierszy w {timer.elapsed_seconds():.2f}s (Speed: {timer.rows_per_second(len(labeled_data)):.0f} rows/s)"
            
            if config.TRAINING_COMPATIBILITY_MODE:
                success_msg += f" â†’ Training-ready output: {output_filename}"
            
            self.logger.info(success_msg)
            
            return processing_report
            
        except Exception as e:
            timer.stop()
            error_msg = f"BÅ‚Ä…d podczas przetwarzania {pair_name}: {str(e)}"
            self.logger.error(error_msg)
            
            # Cleanup w przypadku bÅ‚Ä™du (strategia all-or-nothing)
            if output_file_path and output_file_path.exists():
                cleanup_partial_file(output_file_path, self.logger)
            
            processing_report["success"] = False
            processing_report["error_message"] = str(e)
            processing_report["processing_time_seconds"] = timer.elapsed_seconds()
            processing_report["memory_usage_mb"] = get_memory_usage_mb()
            
            return processing_report
    
    def _save_label_sample_to_csv(self, final_df: pd.DataFrame, pair_name: str):
        """Zapisuje prÃ³bkÄ™ danych (OHLC + cechy + etykieta) do pliku CSV dla weryfikacji wizualnej."""
        
        # Konfiguracja "na twardo", aby uniknÄ…Ä‡ problemÃ³w z plikiem config.py
        SAVE_SAMPLE = True
        SAMPLE_HOURS = 24
        DIAGNOSTICS_DIR = "diagnostics"

        if not SAVE_SAMPLE:
            return

        try:
            self.logger.info(f"Zapisywanie {SAMPLE_HOURS}h prÃ³bki etykiet do pliku CSV dla {pair_name}...")
            
            # StwÃ³rz kopiÄ™, aby uniknÄ…Ä‡ modyfikacji oryginalnego DataFrame (SettingWithCopyWarning)
            df_copy = final_df.copy()

            # Dynamicznie znajdÅº kolumnÄ™ z etykietÄ… i przygotuj jÄ… do mapowania
            label_col_name = None
            if 'label' in df_copy.columns:
                label_col_name = 'label'
            elif 'label_1' in df_copy.columns: # ObsÅ‚uga formatu one-hot
                # SHORT=0, LONG=1, HOLD=2 (zgodnie z competitive_labeler)
                df_copy['temp_label'] = 2  # DomyÅ›lnie HOLD
                df_copy.loc[df_copy['label_0'] == 1, 'temp_label'] = 0  # SHORT
                df_copy.loc[df_copy['label_1'] == 1, 'temp_label'] = 1  # LONG
                label_col_name = 'temp_label'

            # JeÅ›li znaleziono kolumnÄ™ z etykietami, zmapuj jÄ… na wartoÅ›ci sÅ‚owne
            if label_col_name:
                signal_map = {0: 'SHORT', 1: 'LONG', 2: 'HOLD'}
                df_copy['signal'] = df_copy[label_col_name].map(signal_map)
                # Opcjonalnie usuÅ„ tymczasowÄ… kolumnÄ™, jeÅ›li byÅ‚a tworzona
                if label_col_name == 'temp_label':
                    df_copy.drop(columns=['temp_label'], inplace=True)
            else:
                self.logger.warning("Nie znaleziono kolumny z etykietÄ… ('label' lub 'label_x'). PrÃ³bka CSV nie bÄ™dzie zawieraÄ‡ sygnaÅ‚u.")

            # Upewnijmy siÄ™, Å¼e DataFrame ma indeks typu Datetime
            if not isinstance(df_copy.index, pd.DatetimeIndex):
                 self.logger.warning("DataFrame nie ma indeksu typu Datetime. PrÃ³bujÄ™ przekonwertowaÄ‡...")
                 # SprÃ³buj przekonwertowaÄ‡, jeÅ›li to moÅ¼liwe, lub zrezygnuj
                 if 'timestamp' in df_copy.columns:
                     df_copy = df_copy.set_index('timestamp')
                 elif 'datetime' in df_copy.columns:
                     df_copy = df_copy.set_index('datetime')
                 else:
                     self.logger.error("Brak kolumny czasowej do ustawienia jako indeks. Nie moÅ¼na zapisaÄ‡ prÃ³bki.")
                     return

            last_timestamp = df_copy.index.max()
            start_timestamp = last_timestamp - pd.Timedelta(hours=SAMPLE_HOURS)
            sample_df = df_copy[df_copy.index >= start_timestamp].copy()

            columns_to_save = [
                'open', 'high', 'low', 'close', 'volume',
                'high_change', 'low_change', 'close_change', 'volume_change',
                'price_to_ma1440', 'price_to_ma43200',
                'volume_to_ma1440', 'volume_to_ma43200',
                'signal'
            ]
            
            # SprawdÅº, ktÃ³re z Å¼Ä…danych kolumn faktycznie istniejÄ…
            existing_columns = [col for col in columns_to_save if col in sample_df.columns]
            if len(existing_columns) != len(columns_to_save):
                missing = set(columns_to_save) - set(existing_columns)
                self.logger.warning(f"BrakujÄ…ce kolumny w ramce danych: {missing}. ZapisujÄ™ tylko dostÄ™pne.")

            output_path = config.OUTPUT_DATA_PATH / DIAGNOSTICS_DIR
            os.makedirs(output_path, exist_ok=True)
            filename = f"label_sample_{pair_name}_{last_timestamp.strftime('%Y%m%d')}.csv"
            full_path = os.path.join(output_path, filename)

            # Definicja precyzji zaokrÄ…glenia dla poszczegÃ³lnych kolumn
            rounding_rules = {
                'open': 4,
                'high': 4,
                'low': 4,
                'close': 4,
                'volume': 2,
                'high_change': 6,
                'low_change': 6,
                'close_change': 6,
                'volume_change': 2,
                'price_to_ma1440': 8,
                'price_to_ma43200': 8,
                'volume_to_ma1440': 8,
                'volume_to_ma43200': 8
            }
            
            # ZaokrÄ…glij wartoÅ›ci w kolumnach, ktÃ³re istniejÄ… w DataFrame
            for col, precision in rounding_rules.items():
                if col in sample_df.columns:
                    sample_df[col] = sample_df[col].round(precision)

            sample_df[existing_columns].to_csv(full_path, index=True)

            self.logger.info(f"âœ… PrÃ³bka etykiet zapisana pomyÅ›lnie do: {full_path}")

        except Exception as e:
            self.logger.error(f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d podczas zapisywania prÃ³bki etykiet: {e}")
            import traceback
            traceback.print_exc()

    def process_all_pairs(self, input_dir: Optional[Path] = None, 
                         output_dir: Optional[Path] = None) -> List[Dict[str, Any]]:
        """
        Przetwarza wszystkie pary z katalogu input
        
        Args:
            input_dir: Katalog z surowymi danymi (domyÅ›lnie z config)
            output_dir: Katalog docelowy (domyÅ›lnie z config)
            
        Returns:
            list: Lista raportÃ³w dla kaÅ¼dej pary
        """
        if input_dir is None:
            input_dir = config.INPUT_DATA_PATH
        if output_dir is None:
            output_dir = config.OUTPUT_DATA_PATH
        
        self.logger.info(f"Rozpoczynam przetwarzanie wszystkich par z {input_dir}")
        
        # ZnajdÅº wszystkie pliki wejÅ›ciowe
        input_files = find_input_files(input_dir)
        
        if not input_files:
            self.logger.warning(f"Nie znaleziono plikÃ³w wejÅ›ciowych w {input_dir}")
            return []
        
        self.logger.info(f"Znaleziono {len(input_files)} plikÃ³w do przetworzenia")
        
        all_reports = []
        successful_pairs = 0
        failed_pairs = 0
        
        for pair_name, file_path in input_files:
            try:
                self.logger.info(f"Przetwarzam parÄ™ {pair_name} ({successful_pairs + failed_pairs + 1}/{len(input_files)})")
                
                report = self.process_single_pair(pair_name, file_path)
                all_reports.append(report)
                
                if report["success"]:
                    successful_pairs += 1
                    # Zapisz raport dla tej pary
                    self._save_pair_report(report)
                else:
                    failed_pairs += 1
                    self.logger.error(f"Przetwarzanie {pair_name} nieudane: {report.get('error_message', 'Unknown error')}")
                
            except Exception as e:
                failed_pairs += 1
                error_report = {
                    "pair": pair_name,
                    "success": False,
                    "error_message": str(e),
                    "input_file": str(file_path)
                }
                all_reports.append(error_report)
                self.logger.error(f"Krytyczny bÅ‚Ä…d dla {pair_name}: {str(e)}")
        
        # Statystyki koÅ„cowe
        final_msg = f"Przetwarzanie zakoÅ„czone: {successful_pairs} sukces, {failed_pairs} bÅ‚Ä™dÃ³w, Å‚Ä…cznie {len(input_files)} par"
        
        if config.TRAINING_COMPATIBILITY_MODE and successful_pairs > 0:
            final_msg += f"\nðŸŽ¯ Training-ready files generated in: {config.OUTPUT_DATA_PATH}"
        
        self.logger.info(final_msg)
        
        return all_reports
    
    def _get_config_snapshot(self) -> Dict[str, Any]:
        """
        Zwraca snapshot aktualnej konfiguracji
        âœ… TRAINING COMPATIBILITY: Dodaje training-specific config
        """
        base_config = {
            "LONG_TP_PCT": config.LONG_TP_PCT,
            "LONG_SL_PCT": config.LONG_SL_PCT,
            "SHORT_TP_PCT": config.SHORT_TP_PCT,
            "SHORT_SL_PCT": config.SHORT_SL_PCT,
            "FUTURE_WINDOW": config.FUTURE_WINDOW,
            "MA_SHORT_WINDOW": config.MA_SHORT_WINDOW,
            "MA_LONG_WINDOW": config.MA_LONG_WINDOW,
            "MAX_CHANGE_THRESHOLD": config.MAX_CHANGE_THRESHOLD,
            "MAX_VOLUME_CHANGE": config.MAX_VOLUME_CHANGE,
            "TIMEFRAME": config.TIMEFRAME
        }
        
        # âœ… TRAINING COMPATIBILITY: Add training-specific config
        if config.TRAINING_COMPATIBILITY_MODE:
            base_config.update({
                "TRAINING_COMPATIBILITY_MODE": config.TRAINING_COMPATIBILITY_MODE,
                "LABEL_OUTPUT_FORMAT": config.LABEL_OUTPUT_FORMAT,
                "LABEL_DTYPE": config.LABEL_DTYPE,
                "STANDARDIZE_TIMESTAMP_FORMAT": config.STANDARDIZE_TIMESTAMP_FORMAT,
                "INCLUDE_TRAINING_METADATA": config.INCLUDE_TRAINING_METADATA
            })
        
        return base_config
    
    def _save_pair_report(self, report: Dict[str, Any]) -> None:
        """
        Zapisuje raport JSON dla pary
        âœ… TRAINING COMPATIBILITY: Enhanced report with training metadata
        """
        try:
            pair_name = report["pair"]
            report_filename = config.get_report_filename(pair_name, config.TIMEFRAME)
            report_path = config.REPORTS_PATH / report_filename
            
            # Upewnij siÄ™ Å¼e katalog reports istnieje
            config.REPORTS_PATH.mkdir(parents=True, exist_ok=True)
            
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
            
            self.logger.debug(f"Raport zapisany: {report_path}")
            
        except Exception as e:
            self.logger.error(f"BÅ‚Ä…d podczas zapisywania raportu dla {report.get('pair', 'unknown')}: {e}")

def main():
    """
    GÅ‚Ã³wna funkcja uruchamiajÄ…ca przetwarzanie
    Uruchomienie: python main.py
    """
    # Ustawienie logowania na poziomie gÅ‚Ã³wnym
    logger = setup_logging("main")
    
    logger.info("=" * 80)
    logger.info("URUCHOMIENIE MODUÅU VALIDATION_AND_LABELING")
    # âœ… TRAINING COMPATIBILITY: Enhanced startup message
    if config.TRAINING_COMPATIBILITY_MODE:
        logger.info(f"ðŸŽ¯ TRAINING COMPATIBILITY MODE: {config.LABEL_OUTPUT_FORMAT} labels")
    logger.info("=" * 80)
    
    try:
        # SprawdÅº czy katalogi istniejÄ…
        if not config.INPUT_DATA_PATH.exists():
            logger.error(f"Katalog input nie istnieje: {config.INPUT_DATA_PATH}")
            logger.info("UtwÃ³rz katalog i umieÅ›Ä‡ w nim pliki .feather lub .csv z danymi OHLCV")
            return
        
        # Inicjalizuj pipeline
        pipeline = ValidationAndLabelingPipeline()
        
        # PrzetwÃ³rz wszystkie pary
        reports = pipeline.process_all_pairs()
        
        # Podsumowanie
        if reports:
            successful = sum(1 for r in reports if r.get("success", False))
            total = len(reports)
            
            logger.info("=" * 80)
            logger.info(f"PODSUMOWANIE: {successful}/{total} par przetworzonych pomyÅ›lnie")
            
            # âœ… TRAINING COMPATIBILITY: Enhanced summary
            if config.TRAINING_COMPATIBILITY_MODE and successful > 0:
                logger.info(f"ðŸŽ¯ TRAINING-READY OUTPUT:")
                logger.info(f"   Format: {config.LABEL_OUTPUT_FORMAT} labels, {config.LABEL_DTYPE} dtype")
                logger.info(f"   Location: {config.OUTPUT_DATA_PATH}")
                logger.info(f"   Files: *_training_ready.feather")
                logger.info(f"   Ready for: Keras/TensorFlow training pipeline")
            
            logger.info("=" * 80)
            
            if successful < total:
                failed_pairs = [r["pair"] for r in reports if not r.get("success", False)]
                logger.warning(f"Nieudane pary: {failed_pairs}")
        else:
            logger.warning("Brak raportÃ³w - prawdopodobnie nie znaleziono plikÃ³w do przetworzenia")
    
    except Exception as e:
        logger.error(f"Krytyczny bÅ‚Ä…d w main(): {str(e)}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 