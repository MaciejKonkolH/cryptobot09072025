import os
import requests
import zipfile
import pandas as pd
import json
from datetime import datetime, timedelta
import sys
from pathlib import Path
import io
import argparse
import logging

ORDERBOOK_DIR = "orderbook_raw"
OHLC_DIR = "ohlc_raw"
MERGED_FILE = "orderbook_ohlc_merged.feather"  # Tylko feather, bez JSON
METADATA_FILE = "download_metadata.json"


def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days) + 1):
        yield start_date + timedelta(n)

def download_and_extract_orderbook(symbol, date_str, market="futures", out_dir=ORDERBOOK_DIR):
    """Pobiera plik ZIP order book, rozpakowuje go i zwraca Å›cieÅ¼kÄ™ do CSV"""
    url = f"https://data.binance.vision/data/{market}/um/daily/bookDepth/{symbol}/{symbol}-bookDepth-{date_str}.zip"
    zip_path = os.path.join(out_dir, f"{symbol}-bookDepth-{date_str}.zip")
    csv_path = os.path.join(out_dir, f"{symbol}-bookDepth-{date_str}.csv")
    
    try:
        # SprawdÅº czy CSV juÅ¼ istnieje i jest kompletny
        if os.path.exists(csv_path):
            file_size = os.path.getsize(csv_path)
            if file_size > 1000:  # SprawdÅº czy plik ma sensowny rozmiar
                print(f"âœ… {date_str}: Order book CSV juÅ¼ istnieje ({file_size:,} bajtÃ³w)")
                return csv_path
            else:
                print(f"âš ï¸ {date_str}: Order book CSV istnieje ale jest za maÅ‚y ({file_size} bajtÃ³w) - usuwam")
                os.remove(csv_path)
        
        # Plik nie istnieje lokalnie - sprawdÅº serwer
        print(f"ğŸ” {date_str}: Order book nie istnieje lokalnie - sprawdzam serwer...")
        head_resp = requests.head(url, timeout=10)
        if head_resp.status_code == 404:
            print(f"âŒ {date_str}: Order book niedostÄ™pny na serwerze (404)")
            return None
        
        # Pobierz ZIP
        resp = requests.get(url, stream=True, timeout=30)
        if resp.status_code == 200:
            # Zapisz ZIP
            with open(zip_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # Rozpakuj ZIP
            with zipfile.ZipFile(zip_path) as zip_file:
                csv_filename = zip_file.namelist()[0]
                zip_file.extract(csv_filename, out_dir)
                # ZmieÅ„ nazwÄ™ na standardowÄ…
                old_path = os.path.join(out_dir, csv_filename)
                os.rename(old_path, csv_path)
            
            # UsuÅ„ ZIP
            os.remove(zip_path)
            
            file_size = os.path.getsize(csv_path)
            print(f"âœ… {date_str}: Order book pobrano i rozpakowano -> {csv_path} ({file_size:,} bajtÃ³w)")
            return csv_path
        else:
            print(f"âŒ {date_str}: bÅ‚Ä…d pobierania order book ({resp.status_code})")
            return None
    except Exception as e:
        print(f"âŒ {date_str}: bÅ‚Ä…d pobierania order book: {e}")
        return None

def download_and_extract_ohlc(symbol, date_str, interval="1m", market="futures", out_dir=OHLC_DIR):
    """Pobiera plik ZIP OHLC, rozpakowuje go i zwraca Å›cieÅ¼kÄ™ do CSV"""
    url = f"https://data.binance.vision/data/{market}/um/daily/klines/{symbol}/{interval}/{symbol}-{interval}-{date_str}.zip"
    zip_path = os.path.join(out_dir, f"{symbol}-{interval}-{date_str}.zip")
    csv_path = os.path.join(out_dir, f"{symbol}-{interval}-{date_str}.csv")
    
    try:
        # SprawdÅº czy CSV juÅ¼ istnieje i jest kompletny
        if os.path.exists(csv_path):
            file_size = os.path.getsize(csv_path)
            if file_size > 1000:  # SprawdÅº czy plik ma sensowny rozmiar
                print(f"âœ… {date_str}: OHLC CSV juÅ¼ istnieje ({file_size:,} bajtÃ³w)")
                return csv_path
            else:
                print(f"âš ï¸ {date_str}: OHLC CSV istnieje ale jest za maÅ‚y ({file_size} bajtÃ³w) - usuwam")
                os.remove(csv_path)
        
        # TYLKO jeÅ›li plik nie istnieje lokalnie - sprawdÅº serwer
        print(f"ğŸ” {date_str}: OHLC nie istnieje lokalnie - sprawdzam serwer...")
        head_resp = requests.head(url, timeout=10)
        if head_resp.status_code == 404:
            print(f"âŒ {date_str}: OHLC niedostÄ™pny na serwerze (404)")
            return None
        
        # Pobierz ZIP
        resp = requests.get(url, stream=True, timeout=30)
        if resp.status_code == 200:
            # Zapisz ZIP
            with open(zip_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # Rozpakuj ZIP
            with zipfile.ZipFile(zip_path) as zip_file:
                csv_filename = zip_file.namelist()[0]
                zip_file.extract(csv_filename, out_dir)
                # ZmieÅ„ nazwÄ™ na standardowÄ…
                old_path = os.path.join(out_dir, csv_filename)
                os.rename(old_path, csv_path)
            
            # UsuÅ„ ZIP
            os.remove(zip_path)
            
            file_size = os.path.getsize(csv_path)
            print(f"âœ… {date_str}: OHLC pobrano i rozpakowano -> {csv_path} ({file_size:,} bajtÃ³w)")
            return csv_path
        else:
            print(f"âŒ {date_str}: bÅ‚Ä…d pobierania OHLC ({resp.status_code})")
            return None
    except Exception as e:
        print(f"âŒ {date_str}: bÅ‚Ä…d pobierania OHLC: {e}")
        return None

def load_and_process_orderbook(csv_files):
    """Wczytuje i przetwarza pliki order book"""
    print(f"\nğŸ“Š WczytujÄ™ {len(csv_files)} plikÃ³w order book...")
    
    all_orderbook_data = []
    
    for i, csv_file in enumerate(csv_files, 1):
        if csv_file and os.path.exists(csv_file):
            try:
                df = pd.read_csv(csv_file)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                all_orderbook_data.append(df)
                print(f"  ğŸ“Š {i}/{len(csv_files)}: {os.path.basename(csv_file)} - {len(df)} wierszy")
            except Exception as e:
                print(f"  âŒ BÅ‚Ä…d wczytania {csv_file}: {e}")
    
    if all_orderbook_data:
        # PoÅ‚Ä…cz wszystkie dane
        combined_df = pd.concat(all_orderbook_data, ignore_index=True)
        combined_df = combined_df.sort_values('timestamp')
        
        print(f"âœ… Wczytano {len(combined_df)} wierszy order book")
        print(f"â° Zakres czasowy: {combined_df['timestamp'].min()} do {combined_df['timestamp'].max()}")
        
        return combined_df
    else:
        print("âŒ Brak danych order book do wczytania")
        return None

def load_and_process_ohlc(csv_files):
    """Wczytuje i przetwarza pliki OHLC"""
    print(f"\nğŸ“ˆ WczytujÄ™ {len(csv_files)} plikÃ³w OHLC...")
    
    all_ohlc_data = []
    
    for i, csv_file in enumerate(csv_files, 1):
        if csv_file and os.path.exists(csv_file):
            try:
                # Wczytaj surowe dane OHLC (pomiÅ„ pierwszy wiersz z nagÅ‚Ã³wkami)
                df = pd.read_csv(csv_file, header=0, names=[
                    'open_time', 'open', 'high', 'low', 'close', 'volume',
                    'close_time', 'quote_asset_volume', 'number_of_trades',
                    'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
                ])
                
                # Konwertuj timestamp na datetime (sprawdÅº czy to string czy numeric)
                if df['open_time'].dtype == 'object':
                    # JeÅ›li string, sprÃ³buj konwersji na numeric
                    df['open_time'] = pd.to_numeric(df['open_time'], errors='coerce')
                
                # Konwertuj z milliseconds na datetime
                df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms', errors='coerce')
                
                # UsuÅ„ wiersze z nieprawidÅ‚owymi timestampami
                df = df.dropna(subset=['timestamp'])
                
                # Konwertuj kolumny numeryczne
                numeric_columns = ['open', 'high', 'low', 'close', 'volume']
                for col in numeric_columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # Wybierz tylko potrzebne kolumny
                df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
                
                all_ohlc_data.append(df)
                print(f"  ğŸ“ˆ {i}/{len(csv_files)}: {os.path.basename(csv_file)} - {len(df)} wierszy")
            except Exception as e:
                print(f"  âŒ BÅ‚Ä…d wczytania {csv_file}: {e}")
    
    if all_ohlc_data:
        # PoÅ‚Ä…cz wszystkie dane
        combined_df = pd.concat(all_ohlc_data, ignore_index=True)
        combined_df = combined_df.sort_values('timestamp')
        
        print(f"âœ… Wczytano {len(combined_df)} wierszy OHLC")
        print(f"â° Zakres czasowy: {combined_df['timestamp'].min()} do {combined_df['timestamp'].max()}")
        
        return combined_df
    else:
        print("âŒ Brak danych OHLC do wczytania")
        return None

def process_snapshots_for_candle(ohlc_timestamp, wide_orderbook_df):
    """
    Przetwarza snapshoty order book dla jednej Å›wieczki OHLC
    Zwraca 2 snapshoty (lub None jeÅ›li nie moÅ¼na utworzyÄ‡)
    """
    # OHLC timestamp to poczÄ…tek Å›wieczki (np. 00:04:00)
    # Szukamy snapshotÃ³w w trakcie Å›wieczki (00:04:00 - 00:05:00)
    window_start = ohlc_timestamp
    window_end = ohlc_timestamp + pd.Timedelta(minutes=1)
    
    # DEBUG: SprawdÅº pierwsze kilka wywoÅ‚aÅ„
    if ohlc_timestamp.hour == 0 and ohlc_timestamp.minute <= 5:
        print(f"DEBUG: process_snapshots_for_candle dla {ohlc_timestamp}")
        print(f"DEBUG: window_start = {window_start}, window_end = {window_end}")
        print(f"DEBUG: wide_orderbook_df.index.min() = {wide_orderbook_df.index.min()}")
        print(f"DEBUG: wide_orderbook_df.index.max() = {wide_orderbook_df.index.max()}")
        print(f"DEBUG: Snapshoty w wide_orderbook_df:")
        for i, idx in enumerate(wide_orderbook_df.index[:10]):
            print(f"  {i}: {idx}")
    
    # ZnajdÅº snapshoty w oknie Å›wieczki (00:04:00 - 00:05:00)
    relevant_orderbook = wide_orderbook_df[
        (wide_orderbook_df.index >= window_start) &
        (wide_orderbook_df.index < window_end)
    ].sort_index()
    
    # DEBUG: SprawdÅº pierwsze kilka wywoÅ‚aÅ„
    if ohlc_timestamp.hour == 0 and ohlc_timestamp.minute <= 5:
        print(f"DEBUG: Znaleziono {len(relevant_orderbook)} snapshotÃ³w w oknie")
        for i, idx in enumerate(relevant_orderbook.index):
            print(f"  {i}: {idx}")
    
    snapshot_count = len(relevant_orderbook)
    
    if snapshot_count == 0:
        # ZnajdÅº najbliÅ¼sze snapshoty przed i po oknie
        before_window = wide_orderbook_df[wide_orderbook_df.index < window_start].sort_index()
        after_window = wide_orderbook_df[wide_orderbook_df.index >= window_end].sort_index()
        
        if len(before_window) > 0 and len(after_window) > 0:
            # Interpoluj miÄ™dzy snapshotami przed i po oknie
            before_snapshot = before_window.iloc[-1]
            after_snapshot = after_window.iloc[0]
            
            # StwÃ³rz 2 snapshoty przez interpolacjÄ™
            snapshot1 = interpolate_snapshots(before_snapshot, after_snapshot, 0.25)
            snapshot2 = interpolate_snapshots(before_snapshot, after_snapshot, 0.75)
            
            return [snapshot1, snapshot2]
        else:
            return None
    
    elif snapshot_count == 1:
        # SprawdÅº odlegÅ‚oÅ›Ä‡ od koÅ„ca okna
        single_snapshot = relevant_orderbook.iloc[0]
        distance_to_end = (window_end - single_snapshot.name).total_seconds()
        
        if distance_to_end < 30:
            # Snapshot jest bliÅ¼ej niÅ¼ 30 sekund od koÅ„ca okna
            # ZnajdÅº snapshot przed oknem
            before_window = wide_orderbook_df[wide_orderbook_df.index < window_start].sort_index()
            
            if len(before_window) > 0:
                before_snapshot = before_window.iloc[-1]
                # Interpoluj miÄ™dzy snapshotem przed oknem a bieÅ¼Ä…cym
                interpolated_snapshot = interpolate_snapshots(before_snapshot, single_snapshot, 0.5)
                return [interpolated_snapshot, single_snapshot]
            else:
                return [single_snapshot, single_snapshot]  # Duplikuj
        else:
            # Snapshot jest dalej niÅ¼ 30 sekund od koÅ„ca okna
            # ZnajdÅº snapshot po oknie
            after_window = wide_orderbook_df[wide_orderbook_df.index >= window_end].sort_index()
            
            if len(after_window) > 0:
                after_snapshot = after_window.iloc[0]
                # Interpoluj miÄ™dzy bieÅ¼Ä…cym a snapshotem po oknie
                interpolated_snapshot = interpolate_snapshots(single_snapshot, after_snapshot, 0.5)
                return [single_snapshot, interpolated_snapshot]
            else:
                return [single_snapshot, single_snapshot]  # Duplikuj
    
    elif snapshot_count == 2:
        # Idealny przypadek - zwrÃ³Ä‡ oba snapshoty
        return [relevant_orderbook.iloc[0], relevant_orderbook.iloc[1]]
    
    elif snapshot_count >= 3:
        # UsuÅ„ Å›rodkowy snapshot, zostaw skrajne
        return [relevant_orderbook.iloc[0], relevant_orderbook.iloc[-1]]
    
    return None

def interpolate_snapshots(snapshot1, snapshot2, ratio):
    """
    Interpoluje miÄ™dzy dwoma snapshotami order book
    ratio: 0.0 = snapshot1, 1.0 = snapshot2, 0.5 = Å›rednia
    """
    interpolated = {'timestamp': snapshot1['timestamp']}  # UÅ¼yj timestamp z pierwszego
    
    # Interpoluj wszystkie poziomy order book
    for i in range(-5, 6):
        if i == 0:
            continue  # Pomijamy poziom 0
        
        depth_key = f'depth_{i}'
        notional_key = f'notional_{i}'
        
        if depth_key in snapshot1 and depth_key in snapshot2:
            # Interpoluj depth i notional
            depth1 = snapshot1[depth_key]
            depth2 = snapshot2[depth_key]
            notional1 = snapshot1[notional_key]
            notional2 = snapshot2[notional_key]
            
            interpolated[depth_key] = depth1 + (depth2 - depth1) * ratio
            interpolated[notional_key] = notional1 + (notional2 - notional1) * ratio
    
    return interpolated

def check_existing_data_range(start_date, end_date, symbol="BTCUSDT"):
    """Sprawdza czy dane z zakresu juÅ¼ istniejÄ… i zwraca brakujÄ…ce fragmenty"""
    print(f"ğŸ” Sprawdzam istniejÄ…ce dane dla zakresu: {start_date} - {end_date}")
    
    # SprawdÅº czy plik merged juÅ¼ istnieje
    if os.path.exists(MERGED_FILE):
        try:
            # SprawdÅº metadata
            if os.path.exists(METADATA_FILE):
                with open(METADATA_FILE, 'r') as f:
                    metadata = json.load(f)
                
                existing_start = datetime.fromisoformat(metadata['start_date'])
                existing_end = datetime.fromisoformat(metadata['end_date'])
                
                # SprawdÅº czy zakres jest kompletny
                if existing_start <= start_date and existing_end >= end_date:
                    print(f"âœ… Dane z zakresu {start_date} - {end_date} juÅ¼ istniejÄ…!")
                    print(f"   IstniejÄ…cy zakres: {existing_start} - {existing_end}")
                    return True, []
                else:
                    print(f"âš ï¸ IstniejÄ… dane, ale zakres nie jest kompletny")
                    print(f"   IstniejÄ…cy: {existing_start} - {existing_end}")
                    print(f"   Wymagany: {start_date} - {end_date}")
                    
                    # ZnajdÅº brakujÄ…ce fragmenty
                    missing_ranges = []
                    if start_date < existing_start:
                        missing_ranges.append((start_date, existing_start - timedelta(days=1)))
                    if end_date > existing_end:
                        missing_ranges.append((existing_end + timedelta(days=1), end_date))
                    
                    return False, missing_ranges
            else:
                print(f"âš ï¸ Plik merged istnieje, ale brak metadata")
                return False, [(start_date, end_date)]
        except Exception as e:
            print(f"âš ï¸ BÅ‚Ä…d sprawdzania metadata: {e}")
            return False, [(start_date, end_date)]
    
    print(f"âŒ Brak istniejÄ…cych danych - pobieram caÅ‚y zakres")
    return False, [(start_date, end_date)]

def save_metadata(start_date, end_date, symbol="BTCUSDT"):
    """Zapisuje metadata o pobranych danych"""
    metadata = {
        'symbol': symbol,
        'start_date': start_date.isoformat(),
        'end_date': end_date.isoformat(),
        'created_at': datetime.now().isoformat(),
        'file_format': 'feather'
    }
    
    with open(METADATA_FILE, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"ğŸ’¾ Metadata zapisana: {METADATA_FILE}")

def merge_orderbook_with_ohlc(orderbook_df, ohlc_df, save_json=False, extra_month_for_ma=False):
    """ZOPTYMALIZOWANA wersja Å‚Ä…czenia order book z OHLC - TYLKO WSPÃ“LNE DANE"""
    print(f"\nğŸ”— ÅÄ…czÄ™ order book z OHLC (ZOPTYMALIZOWANE)...")
    print(f"ğŸ“Š Dane OHLC: {len(ohlc_df)} wierszy")
    print(f"ğŸ“Š Dane Order Book: {len(orderbook_df)} wierszy")
    
    # KROK 1: Indeksowanie i groupby (ZAMIENIA O(nÂ²) na O(n log n))
    print("ğŸ“Š IndeksujÄ™ dane order book...")
    orderbook_df.set_index('timestamp', inplace=True)
    
    # KROK 2: Vectorizowane tworzenie wide format
    print("ğŸ“Š TworzÄ™ wide format order book (VECTORIZED)...")
    
    # Dodaj logi progress
    total_snapshots = orderbook_df.groupby(level=0).ngroups
    print(f"ğŸ“Š Przetwarzam {total_snapshots:,} snapshots order book...")
    
    # ZastÄ…p groupby().apply() pÄ™tlÄ… z logami postÄ™pu
    wide_orderbook_data = []
    processed_count = 0
    
    for timestamp, group in orderbook_df.groupby(level=0):
        # StwÃ³rz wiersz dla tego timestampu
        row_data = {'timestamp': timestamp}
        
        # Dodaj dane depth i notional dla kaÅ¼dego poziomu
        for _, snapshot in group.iterrows():
            row_data[f'depth_{snapshot["percentage"]}'] = snapshot["depth"]
            row_data[f'notional_{snapshot["percentage"]}'] = snapshot["notional"]
        
        wide_orderbook_data.append(row_data)
        processed_count += 1
        
        # Logi postÄ™pu co 10,000 snapshots
        if processed_count % 10000 == 0:
            progress = (processed_count / total_snapshots) * 100
            print(f"ğŸ“Š PostÄ™p: {processed_count:,}/{total_snapshots:,} snapshots ({progress:.1f}%)")
    
    print(f"âœ… Wide format utworzony: {len(wide_orderbook_data):,} snapshots")
    
    # Konwertuj na DataFrame
    wide_orderbook_df = pd.DataFrame(wide_orderbook_data)
    wide_orderbook_df.set_index('timestamp', inplace=True)
    
    # KROK 4: ZnajdÅº zakres wspÃ³lnych danych
    orderbook_start = wide_orderbook_df.index.min()
    orderbook_end = wide_orderbook_df.index.max()
    ohlc_start = ohlc_df.index.min()
    ohlc_end = ohlc_df.index.max()
    
    # Diagnostyka typÃ³w
    print(f"ğŸ” Diagnostyka typÃ³w indeksÃ³w:")
    print(f"   orderbook_start: {type(orderbook_start)} = {orderbook_start}")
    print(f"   ohlc_start: {type(ohlc_start)} = {ohlc_start}")
    print(f"   orderbook_end: {type(orderbook_end)} = {orderbook_end}")
    print(f"   ohlc_end: {type(ohlc_end)} = {ohlc_end}")
    
    # Konwertuj na pd.Timestamp jeÅ›li potrzebne
    if not isinstance(orderbook_start, pd.Timestamp):
        orderbook_start = pd.Timestamp(orderbook_start)
    if not isinstance(orderbook_end, pd.Timestamp):
        orderbook_end = pd.Timestamp(orderbook_end)
    
    # OHLC ma RangeIndex - uÅ¼yj timestamp z kolumny
    if isinstance(ohlc_start, int):
        ohlc_start = ohlc_df['timestamp'].min()
    if isinstance(ohlc_end, int):
        ohlc_end = ohlc_df['timestamp'].max()
    
    if not isinstance(ohlc_start, pd.Timestamp):
        ohlc_start = pd.Timestamp(ohlc_start)
    if not isinstance(ohlc_end, pd.Timestamp):
        ohlc_end = pd.Timestamp(ohlc_end)

    # Oblicz wspÃ³lny zakres
    common_start = max(orderbook_start, ohlc_start)
    common_end = min(orderbook_end, ohlc_end)
    
    print(f"ğŸ“… Zakresy danych:")
    print(f"   OHLC: {ohlc_start} - {ohlc_end}")
    print(f"   Order Book: {orderbook_start} - {orderbook_end}")
    print(f"   WSPÃ“LNY: {common_start} - {common_end}")
    
    # KROK 5: Filtruj OHLC do wspÃ³lnego zakresu
    if extra_month_for_ma:
        # Dodaj 30 dni wczeÅ›niej dla MA 43200
        ma_start = common_start - pd.Timedelta(days=30)
        print(f"   +30 dni dla MA: {ma_start} - {common_end}")
        filtered_ohlc = ohlc_df[(ohlc_df['timestamp'] >= ma_start) & (ohlc_df['timestamp'] <= common_end)]
    else:
        filtered_ohlc = ohlc_df[(ohlc_df['timestamp'] >= common_start) & (ohlc_df['timestamp'] <= common_end)]
    
    print(f"ğŸ“Š Filtrowane OHLC: {len(filtered_ohlc)} wierszy")
    
    # KROK 6: Vectorizowane przetwarzanie OHLC
    print("ğŸ“Š Przetwarzam snapshoty dla Å›wieczek OHLC (VECTORIZED)...")
    
    def process_candle_vectorized(ohlc_row):
        """Vectorizowana funkcja przetwarzania jednej Å›wieczki"""
        ohlc_timestamp = ohlc_row['timestamp']
        
        # DEBUG: SprawdÅº pierwsze kilka Å›wieczek
        if processed_count < 5:
            print(f"DEBUG: Przetwarzam Å›wieczkÄ™ {ohlc_timestamp}")
            print(f"DEBUG: wide_orderbook_df.index.min() = {wide_orderbook_df.index.min()}")
            print(f"DEBUG: wide_orderbook_df.index.max() = {wide_orderbook_df.index.max()}")
        
        # Szybki lookup w indeksowanym DataFrame
        try:
            # UÅ¼yj exclusive end slicing (jak w process_snapshots_for_candle)
            snapshots = wide_orderbook_df[
                (wide_orderbook_df.index >= ohlc_timestamp) & 
                (wide_orderbook_df.index < ohlc_timestamp + pd.Timedelta(minutes=1))
            ]
            
            # DEBUG: SprawdÅº pierwsze kilka Å›wieczek
            if processed_count < 5:
                print(f"DEBUG: Znaleziono {len(snapshots)} snapshotÃ³w dla {ohlc_timestamp}")
                
        except KeyError:
            if processed_count < 5:
                print(f"DEBUG: KeyError dla {ohlc_timestamp}")
            return None  # Brak order book - pomijamy ten wiersz
        
        if len(snapshots) == 0:
            if processed_count < 5:
                print(f"DEBUG: Brak snapshotÃ³w dla {ohlc_timestamp}")
            return None  # Brak order book - pomijamy ten wiersz
        
        # PrzetwÃ³rz snapshoty
        processed_snapshots = process_snapshots_for_candle(ohlc_timestamp, snapshots)
        
        # DEBUG: SprawdÅº pierwsze kilka Å›wieczek
        if processed_count < 5:
            print(f"DEBUG: processed_snapshots = {processed_snapshots}")
        
        # SprawdÅº czy mamy kompletne dane
        if not processed_snapshots or len(processed_snapshots) != 2:
            if processed_count < 5:
                print(f"DEBUG: Niekompletne dane dla {ohlc_timestamp}")
            return None  # Niekompletne dane - pomijamy ten wiersz
        
        # StwÃ³rz wiersz wynikowy
        merged_row = {
            'timestamp': ohlc_timestamp,
            'open': ohlc_row['open'],
            'high': ohlc_row['high'],
            'low': ohlc_row['low'],
            'close': ohlc_row['close'],
            'volume': ohlc_row['volume']
        }
        
        # Dodaj dane z pierwszego snapshotu
        for key, value in processed_snapshots[0].items():
            if key != 'timestamp':
                merged_row[f'snapshot1_{key}'] = value
        
        # Dodaj dane z drugiego snapshotu
        for key, value in processed_snapshots[1].items():
            if key != 'timestamp':
                merged_row[f'snapshot2_{key}'] = value
        
        # Dodaj informacje o jakoÅ›ci danych
        merged_row['snapshot1_timestamp'] = processed_snapshots[0]['timestamp']
        merged_row['snapshot2_timestamp'] = processed_snapshots[1]['timestamp']
        merged_row['data_quality'] = 'complete'
        
        return merged_row
    
    # KROK 7: Vectorizowane przetwarzanie wszystkich Å›wieczek
    print("ğŸ”„ Przetwarzam wszystkie Å›wieczki OHLC...")
    
    # Dodaj logi progress
    total_candles = len(filtered_ohlc)
    print(f"ğŸ“Š Przetwarzam {total_candles:,} Å›wieczek OHLC...")
    
    # ZastÄ…p apply() pÄ™tlÄ… z logami postÄ™pu
    processed_data = []
    processed_count = 0
    
    for index, ohlc_row in filtered_ohlc.iterrows():
        result = process_candle_vectorized(ohlc_row)
        if result is not None:
            processed_data.append(result)
        
        processed_count += 1
        
        # Logi postÄ™pu co 50,000 Å›wieczek
        if processed_count % 50000 == 0:
            progress = (processed_count / total_candles) * 100
            complete_count = len(processed_data)
            print(f"ğŸ“Š PostÄ™p: {processed_count:,}/{total_candles:,} Å›wieczek ({progress:.1f}%) - Kompletne: {complete_count:,}")
    
    print(f"âœ… Przetworzono {len(processed_data):,} Å›wieczek z kompletnymi danymi order book")
    
    # KROK 8: Tworzenie finalnego DataFrame
    print("ğŸ“Š TworzÄ™ finalny DataFrame...")
    full_merged_df = pd.DataFrame(processed_data)
    
    # Statystyki
    complete_count = len(full_merged_df)
    total_ohlc = len(filtered_ohlc)
    coverage = (complete_count / total_ohlc * 100) if total_ohlc > 0 else 0
    
    print(f"\nğŸ“ˆ Statystyki przetwarzania (TYLKO WSPÃ“LNE DANE):")
    print(f"  Åšwieczek OHLC w zakresie: {total_ohlc}")
    print(f"  Åšwieczek z kompletnymi danymi: {complete_count}")
    print(f"  Pokrycie: {coverage:.1f}%")
    print(f"  Zakres wynikowy: {full_merged_df['timestamp'].min()} - {full_merged_df['timestamp'].max()}")
    
    # Zapisz tylko feather (szybszy)
    print(f"\nğŸ’¾ ZapisujÄ™ dane do {MERGED_FILE}...")
    full_merged_df.to_feather(MERGED_FILE)
    print(f"âœ… Dane zapisane pomyÅ›lnie do {MERGED_FILE}")
    
    return full_merged_df

def create_missing_orderbook_data(missing_dates, orderbook_dir):
    """
    Tworzy brakujÄ…ce dane order book na podstawie dni sÄ…siadujÄ…cych
    """
    print(f"ğŸ”§ TworzÄ™ brakujÄ…ce dane order book dla {len(missing_dates)} dni...")
    
    for missing_date in missing_dates:
        print(f"   ğŸ“… Przetwarzam: {missing_date}")
        
        # ZnajdÅº sÄ…siadujÄ…ce dni
        missing_dt = datetime.strptime(missing_date, '%Y-%m-%d')
        
        # SprawdÅº dzieÅ„ przed
        day_before = (missing_dt - timedelta(days=1)).strftime('%Y-%m-%d')
        file_before = f"orderbook_raw/BTCUSDT-bookDepth-{day_before}.csv"
        
        # SprawdÅº dzieÅ„ po
        day_after = (missing_dt + timedelta(days=1)).strftime('%Y-%m-%d')
        file_after = f"orderbook_raw/BTCUSDT-bookDepth-{day_after}.csv"
        
        # Wybierz ÅºrÃ³dÅ‚o danych (preferuj dzieÅ„ przed)
        source_file = None
        source_date = None
        
        if os.path.exists(file_before):
            source_file = file_before
            source_date = day_before
            print(f"      ğŸ“‹ UÅ¼ywam danych z: {day_before}")
        elif os.path.exists(file_after):
            source_file = file_after
            source_date = day_after
            print(f"      ğŸ“‹ UÅ¼ywam danych z: {day_after}")
        else:
            print(f"      âŒ Brak danych sÄ…siadujÄ…cych dla {missing_date}")
            continue
        
        # Wczytaj dane ÅºrÃ³dÅ‚owe
        try:
            source_df = pd.read_csv(source_file)
            print(f"      ğŸ“Š Wczytano {len(source_df):,} wierszy z {source_date}")
            
            # Skopiuj dane i dostosuj timestampy
            new_df = source_df.copy()
            
            # ZamieÅ„ timestampy na brakujÄ…cy dzieÅ„
            source_start = pd.to_datetime(source_date)
            target_start = pd.to_datetime(missing_date)
            
            # Oblicz przesuniÄ™cie czasowe
            time_shift = target_start - source_start
            
            # Dostosuj timestampy
            new_df['timestamp'] = pd.to_datetime(new_df['timestamp']) + time_shift
            new_df['timestamp'] = new_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')
            
            # Zapisz nowy plik
            output_file = f"orderbook_raw/BTCUSDT-bookDepth-{missing_date}.csv"
            new_df.to_csv(output_file, index=False)
            
            print(f"      âœ… Utworzono: {output_file} ({len(new_df):,} wierszy)")
            
        except Exception as e:
            print(f"      âŒ BÅ‚Ä…d podczas tworzenia danych: {e}")

def check_and_fill_missing_orderbook():
    """
    Sprawdza brakujÄ…ce dni order book i wypeÅ‚nia je danymi z dni sÄ…siadujÄ…cych
    """
    orderbook_dir = "orderbook_raw"
    ohlc_dir = "ohlc_raw"
    
    # Pobierz listÄ™ plikÃ³w
    orderbook_files = [f for f in os.listdir(orderbook_dir) if f.endswith('.csv')]
    ohlc_files = [f for f in os.listdir(ohlc_dir) if f.endswith('.csv')]
    
    # WyciÄ…gnij daty z nazw plikÃ³w
    orderbook_dates = set()
    for file in orderbook_files:
        if 'BTCUSDT-bookDepth-' in file:
            date_str = file.replace('BTCUSDT-bookDepth-', '').replace('.csv', '')
            orderbook_dates.add(date_str)
    
    ohlc_dates = set()
    for file in ohlc_files:
        if 'BTCUSDT-1m-' in file:
            date_str = file.replace('BTCUSDT-1m-', '').replace('.csv', '')
            ohlc_dates.add(date_str)
    
    # ZnajdÅº luki
    missing_orderbook = ohlc_dates - orderbook_dates
    
    if missing_orderbook:
        print(f"ğŸ” Znaleziono {len(missing_orderbook)} brakujÄ…cych dni order book:")
        for date in sorted(missing_orderbook):
            print(f"   - {date}")
        
        # WypeÅ‚nij brakujÄ…ce dane
        create_missing_orderbook_data(sorted(missing_orderbook), orderbook_dir)
        
        print(f"âœ… WypeÅ‚niono wszystkie brakujÄ…ce dni order book!")
        return True
    else:
        print(f"âœ… Brak luk w danych order book!")
        return False

def main():
    """GÅ‚Ã³wna funkcja pobierania i Å‚Ä…czenia danych"""
    import argparse
    
    # Parsuj argumenty z linii komend
    parser = argparse.ArgumentParser(description='Pobierz i poÅ‚Ä…cz dane OHLC z Order Book')
    parser.add_argument('symbol', help='Symbol kryptowaluty (np. BTCUSDT)')
    parser.add_argument('start_date', help='Data poczÄ…tkowa (YYYY-MM-DD)')
    parser.add_argument('end_date', help='Data koÅ„cowa (YYYY-MM-DD)')
    parser.add_argument('--extra-month', action='store_true', help='Dodaj 30 dni dla MA 43200')
    
    args = parser.parse_args()
    
    # Konwertuj stringi dat na datetime
    try:
        start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d')
    except ValueError as e:
        print(f"âŒ BÅ‚Ä…d formatu daty: {e}")
        print("UÅ¼yj formatu: YYYY-MM-DD (np. 2023-01-01)")
        return
    
    symbol = args.symbol
    extra_month_for_ma = args.extra_month
    
    print(f"ğŸš€ Rozpoczynanie pobierania danych dla {symbol}")
    print(f"ğŸ“… Zakres: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
    if extra_month_for_ma:
        print(f"ğŸ“Š DodajÄ™ 30 dni dla obliczania MA 43200")
    
    # SprawdÅº i wypeÅ‚nij brakujÄ…ce dane order book
    print(f"\nğŸ” Sprawdzam luki w danych order book...")
    check_and_fill_missing_orderbook()
    
    # KROK 1: SprawdÅº istniejÄ…ce dane
    data_exists, missing_ranges = check_existing_data_range(start_date, end_date, symbol)
    
    if data_exists:
        print(f"âœ… Dane juÅ¼ istniejÄ…! KoÅ„czÄ™ pracÄ™.")
        return
    
    if missing_ranges:
        print(f"ğŸ“‹ BrakujÄ…ce zakresy do pobrania:")
        for start, end in missing_ranges:
            print(f"   {start.strftime('%Y-%m-%d')} - {end.strftime('%Y-%m-%d')}")
    
    # KROK 2: Pobierz brakujÄ…ce dane
    all_csv_files_orderbook = []
    all_csv_files_ohlc = []
    
    for start, end in missing_ranges:
        print(f"\nğŸ“¥ Pobieram dane dla zakresu: {start.strftime('%Y-%m-%d')} - {end.strftime('%Y-%m-%d')}")
        
        for date in daterange(start, end):
            date_str = date.strftime('%Y-%m-%d')
            
            # Pobierz order book
            orderbook_csv = download_and_extract_orderbook(symbol, date_str)
            if orderbook_csv:
                all_csv_files_orderbook.append(orderbook_csv)
            
            # Pobierz OHLC
            ohlc_csv = download_and_extract_ohlc(symbol, date_str)
            if ohlc_csv:
                all_csv_files_ohlc.append(ohlc_csv)
    
    if not all_csv_files_orderbook or not all_csv_files_ohlc:
        print("âŒ Brak plikÃ³w do przetworzenia!")
        return
    
    # KROK 3: PrzetwÃ³rz dane
    print(f"\nğŸ“Š Przetwarzam {len(all_csv_files_orderbook)} plikÃ³w order book...")
    orderbook_df = load_and_process_orderbook(all_csv_files_orderbook)
    
    print(f"\nğŸ“Š Przetwarzam {len(all_csv_files_ohlc)} plikÃ³w OHLC...")
    ohlc_df = load_and_process_ohlc(all_csv_files_ohlc)
    
    if orderbook_df is None or ohlc_df is None:
        print("âŒ BÅ‚Ä…d przetwarzania danych!")
        return
    
    # KROK 4: PoÅ‚Ä…cz dane (ZOPTYMALIZOWANE - TYLKO WSPÃ“LNE)
    merged_df = merge_orderbook_with_ohlc(orderbook_df, ohlc_df, extra_month_for_ma=extra_month_for_ma)
    
    # KROK 5: Zapisz metadata
    save_metadata(start_date, end_date, symbol)
    
    print(f"\nğŸ‰ Proces zakoÅ„czony pomyÅ›lnie!")
    print(f"ğŸ“ Plik wynikowy: {MERGED_FILE}")
    print(f"ğŸ“Š Liczba wierszy: {len(merged_df):,}")
    print(f"ğŸ“… Zakres wynikowy: {merged_df['timestamp'].min()} - {merged_df['timestamp'].max()}")

if __name__ == "__main__":
    main() 