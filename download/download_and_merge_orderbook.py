import os
import requests
import zipfile
import pandas as pd
import json
from datetime import datetime, timedelta
import sys
from pathlib import Path
import io
import argparse
import logging

ORDERBOOK_DIR = "orderbook_raw"
OHLC_DIR = "ohlc_raw"
MERGED_FILE = "orderbook_ohlc_merged.feather"  # Tylko feather, bez JSON
METADATA_FILE = "download_metadata.json"


def daterange(start_date, end_date):
    for n in range(int((end_date - start_date).days) + 1):
        yield start_date + timedelta(n)

def download_and_extract_orderbook(symbol, date_str, market="futures", out_dir=ORDERBOOK_DIR):
    """Pobiera plik ZIP order book, rozpakowuje go i zwraca ≈õcie≈ºkƒô do CSV"""
    url = f"https://data.binance.vision/data/{market}/um/daily/bookDepth/{symbol}/{symbol}-bookDepth-{date_str}.zip"
    zip_path = os.path.join(out_dir, f"{symbol}-bookDepth-{date_str}.zip")
    csv_path = os.path.join(out_dir, f"{symbol}-bookDepth-{date_str}.csv")
    
    try:
        # Sprawd≈∫ czy CSV ju≈º istnieje i jest kompletny
        if os.path.exists(csv_path):
            file_size = os.path.getsize(csv_path)
            if file_size > 1000:  # Sprawd≈∫ czy plik ma sensowny rozmiar
                print(f"‚úÖ {date_str}: Order book CSV ju≈º istnieje ({file_size:,} bajt√≥w)")
                return csv_path
            else:
                print(f"‚ö†Ô∏è {date_str}: Order book CSV istnieje ale jest za ma≈Çy ({file_size} bajt√≥w) - usuwam")
                os.remove(csv_path)
        
        # Plik nie istnieje lokalnie - sprawd≈∫ serwer
        print(f"üîç {date_str}: Order book nie istnieje lokalnie - sprawdzam serwer...")
        head_resp = requests.head(url, timeout=10)
        if head_resp.status_code == 404:
            print(f"‚ùå {date_str}: Order book niedostƒôpny na serwerze (404)")
            return None
        
        # Pobierz ZIP
        resp = requests.get(url, stream=True, timeout=30)
        if resp.status_code == 200:
            # Zapisz ZIP
            with open(zip_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # Rozpakuj ZIP
            with zipfile.ZipFile(zip_path) as zip_file:
                csv_filename = zip_file.namelist()[0]
                zip_file.extract(csv_filename, out_dir)
                # Zmie≈Ñ nazwƒô na standardowƒÖ
                old_path = os.path.join(out_dir, csv_filename)
                os.rename(old_path, csv_path)
            
            # Usu≈Ñ ZIP
            os.remove(zip_path)
            
            file_size = os.path.getsize(csv_path)
            print(f"‚úÖ {date_str}: Order book pobrano i rozpakowano -> {csv_path} ({file_size:,} bajt√≥w)")
            return csv_path
        else:
            print(f"‚ùå {date_str}: b≈ÇƒÖd pobierania order book ({resp.status_code})")
            return None
    except Exception as e:
        print(f"‚ùå {date_str}: b≈ÇƒÖd pobierania order book: {e}")
        return None

def download_and_extract_ohlc(symbol, date_str, interval="1m", market="futures", out_dir=OHLC_DIR):
    """Pobiera plik ZIP OHLC, rozpakowuje go i zwraca ≈õcie≈ºkƒô do CSV"""
    url = f"https://data.binance.vision/data/{market}/um/daily/klines/{symbol}/{interval}/{symbol}-{interval}-{date_str}.zip"
    zip_path = os.path.join(out_dir, f"{symbol}-{interval}-{date_str}.zip")
    csv_path = os.path.join(out_dir, f"{symbol}-{interval}-{date_str}.csv")
    
    try:
        # Sprawd≈∫ czy CSV ju≈º istnieje i jest kompletny
        if os.path.exists(csv_path):
            file_size = os.path.getsize(csv_path)
            if file_size > 1000:  # Sprawd≈∫ czy plik ma sensowny rozmiar
                print(f"‚úÖ {date_str}: OHLC CSV ju≈º istnieje ({file_size:,} bajt√≥w)")
                return csv_path
            else:
                print(f"‚ö†Ô∏è {date_str}: OHLC CSV istnieje ale jest za ma≈Çy ({file_size} bajt√≥w) - usuwam")
                os.remove(csv_path)
        
        # TYLKO je≈õli plik nie istnieje lokalnie - sprawd≈∫ serwer
        print(f"üîç {date_str}: OHLC nie istnieje lokalnie - sprawdzam serwer...")
        head_resp = requests.head(url, timeout=10)
        if head_resp.status_code == 404:
            print(f"‚ùå {date_str}: OHLC niedostƒôpny na serwerze (404)")
            return None
        
        # Pobierz ZIP
        resp = requests.get(url, stream=True, timeout=30)
        if resp.status_code == 200:
            # Zapisz ZIP
            with open(zip_path, "wb") as f:
                for chunk in resp.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            # Rozpakuj ZIP
            with zipfile.ZipFile(zip_path) as zip_file:
                csv_filename = zip_file.namelist()[0]
                zip_file.extract(csv_filename, out_dir)
                # Zmie≈Ñ nazwƒô na standardowƒÖ
                old_path = os.path.join(out_dir, csv_filename)
                os.rename(old_path, csv_path)
            
            # Usu≈Ñ ZIP
            os.remove(zip_path)
            
            file_size = os.path.getsize(csv_path)
            print(f"‚úÖ {date_str}: OHLC pobrano i rozpakowano -> {csv_path} ({file_size:,} bajt√≥w)")
            return csv_path
        else:
            print(f"‚ùå {date_str}: b≈ÇƒÖd pobierania OHLC ({resp.status_code})")
            return None
    except Exception as e:
        print(f"‚ùå {date_str}: b≈ÇƒÖd pobierania OHLC: {e}")
        return None

def load_and_process_orderbook(csv_files):
    """Wczytuje i przetwarza pliki order book"""
    print(f"\nüìä Wczytujƒô {len(csv_files)} plik√≥w order book...")
    
    all_orderbook_data = []
    
    for i, csv_file in enumerate(csv_files, 1):
        if csv_file and os.path.exists(csv_file):
            try:
                df = pd.read_csv(csv_file)
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                all_orderbook_data.append(df)
                print(f"  üìä {i}/{len(csv_files)}: {os.path.basename(csv_file)} - {len(df)} wierszy")
            except Exception as e:
                print(f"  ‚ùå B≈ÇƒÖd wczytania {csv_file}: {e}")
    
    if all_orderbook_data:
        # Po≈ÇƒÖcz wszystkie dane
        combined_df = pd.concat(all_orderbook_data, ignore_index=True)
        combined_df = combined_df.sort_values('timestamp')
        
        print(f"‚úÖ Wczytano {len(combined_df)} wierszy order book")
        print(f"‚è∞ Zakres czasowy: {combined_df['timestamp'].min()} do {combined_df['timestamp'].max()}")
        
        return combined_df
    else:
        print("‚ùå Brak danych order book do wczytania")
        return None

def load_and_process_ohlc(csv_files):
    """Wczytuje i przetwarza pliki OHLC"""
    print(f"\nüìà Wczytujƒô {len(csv_files)} plik√≥w OHLC...")
    
    all_ohlc_data = []
    
    for i, csv_file in enumerate(csv_files, 1):
        if csv_file and os.path.exists(csv_file):
            try:
                # Wczytaj surowe dane OHLC (pomi≈Ñ pierwszy wiersz z nag≈Ç√≥wkami)
                df = pd.read_csv(csv_file, header=0, names=[
                    'open_time', 'open', 'high', 'low', 'close', 'volume',
                    'close_time', 'quote_asset_volume', 'number_of_trades',
                    'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
                ])
                
                # Konwertuj timestamp na datetime (sprawd≈∫ czy to string czy numeric)
                if df['open_time'].dtype == 'object':
                    # Je≈õli string, spr√≥buj konwersji na numeric
                    df['open_time'] = pd.to_numeric(df['open_time'], errors='coerce')
                
                # Konwertuj z milliseconds na datetime
                df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms', errors='coerce')
                
                # Usu≈Ñ wiersze z nieprawid≈Çowymi timestampami
                df = df.dropna(subset=['timestamp'])
                
                # Konwertuj kolumny numeryczne
                numeric_columns = ['open', 'high', 'low', 'close', 'volume']
                for col in numeric_columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
                
                # Wybierz tylko potrzebne kolumny
                df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]
                
                all_ohlc_data.append(df)
                print(f"  üìà {i}/{len(csv_files)}: {os.path.basename(csv_file)} - {len(df)} wierszy")
            except Exception as e:
                print(f"  ‚ùå B≈ÇƒÖd wczytania {csv_file}: {e}")
    
    if all_ohlc_data:
        # Po≈ÇƒÖcz wszystkie dane
        combined_df = pd.concat(all_ohlc_data, ignore_index=True)
        combined_df = combined_df.sort_values('timestamp')
        
        print(f"‚úÖ Wczytano {len(combined_df)} wierszy OHLC")
        print(f"‚è∞ Zakres czasowy: {combined_df['timestamp'].min()} do {combined_df['timestamp'].max()}")
        
        return combined_df
    else:
        print("‚ùå Brak danych OHLC do wczytania")
        return None

def process_snapshots_for_candle(ohlc_timestamp, wide_orderbook_df):
    """
    Przetwarza snapshoty order book dla jednej ≈õwieczki OHLC
    Zwraca 2 snapshoty (lub None je≈õli nie mo≈ºna utworzyƒá)
    """
    # OHLC timestamp to poczƒÖtek ≈õwieczki (np. 00:04:00)
    # Szukamy snapshot√≥w w trakcie ≈õwieczki (00:04:00 - 00:05:00)
    window_start = ohlc_timestamp
    window_end = ohlc_timestamp + pd.Timedelta(minutes=1)
    
    # DEBUG: Sprawd≈∫ pierwsze kilka wywo≈Ça≈Ñ
    if ohlc_timestamp.hour == 0 and ohlc_timestamp.minute <= 5:
        print(f"DEBUG: process_snapshots_for_candle dla {ohlc_timestamp}")
        print(f"DEBUG: window_start = {window_start}, window_end = {window_end}")
        print(f"DEBUG: wide_orderbook_df.index.min() = {wide_orderbook_df.index.min()}")
        print(f"DEBUG: wide_orderbook_df.index.max() = {wide_orderbook_df.index.max()}")
        print(f"DEBUG: Snapshoty w wide_orderbook_df:")
        for i, idx in enumerate(wide_orderbook_df.index[:10]):
            print(f"  {i}: {idx}")
    
    # Znajd≈∫ snapshoty w oknie ≈õwieczki (00:04:00 - 00:05:00)
    relevant_orderbook = wide_orderbook_df[
        (wide_orderbook_df.index >= window_start) &
        (wide_orderbook_df.index < window_end)
    ].sort_index()
    
    # DEBUG: Sprawd≈∫ pierwsze kilka wywo≈Ça≈Ñ
    if ohlc_timestamp.hour == 0 and ohlc_timestamp.minute <= 5:
        print(f"DEBUG: Znaleziono {len(relevant_orderbook)} snapshot√≥w w oknie")
        for i, idx in enumerate(relevant_orderbook.index):
            print(f"  {i}: {idx}")
    
    snapshot_count = len(relevant_orderbook)
    
    if snapshot_count == 0:
        # Znajd≈∫ najbli≈ºsze snapshoty przed i po oknie
        before_window = wide_orderbook_df[wide_orderbook_df.index < window_start].sort_index()
        after_window = wide_orderbook_df[wide_orderbook_df.index >= window_end].sort_index()
        
        if len(before_window) > 0 and len(after_window) > 0:
            # Interpoluj miƒôdzy snapshotami przed i po oknie
            before_snapshot = before_window.iloc[-1]
            after_snapshot = after_window.iloc[0]
            
            # Stw√≥rz 2 snapshoty przez interpolacjƒô
            snapshot1 = interpolate_snapshots(before_snapshot, after_snapshot, 0.25)
            snapshot2 = interpolate_snapshots(before_snapshot, after_snapshot, 0.75)
            
            return [snapshot1, snapshot2]
        else:
            return None
    
    elif snapshot_count == 1:
        # Sprawd≈∫ odleg≈Ço≈õƒá od ko≈Ñca okna
        single_snapshot = relevant_orderbook.iloc[0]
        distance_to_end = (window_end - single_snapshot.name).total_seconds()
        
        if distance_to_end < 30:
            # Snapshot jest bli≈ºej ni≈º 30 sekund od ko≈Ñca okna
            # Znajd≈∫ snapshot przed oknem
            before_window = wide_orderbook_df[wide_orderbook_df.index < window_start].sort_index()
            
            if len(before_window) > 0:
                before_snapshot = before_window.iloc[-1]
                # Interpoluj miƒôdzy snapshotem przed oknem a bie≈ºƒÖcym
                interpolated_snapshot = interpolate_snapshots(before_snapshot, single_snapshot, 0.5)
                return [interpolated_snapshot, single_snapshot]
            else:
                return [single_snapshot, single_snapshot]  # Duplikuj
        else:
            # Snapshot jest dalej ni≈º 30 sekund od ko≈Ñca okna
            # Znajd≈∫ snapshot po oknie
            after_window = wide_orderbook_df[wide_orderbook_df.index >= window_end].sort_index()
            
            if len(after_window) > 0:
                after_snapshot = after_window.iloc[0]
                # Interpoluj miƒôdzy bie≈ºƒÖcym a snapshotem po oknie
                interpolated_snapshot = interpolate_snapshots(single_snapshot, after_snapshot, 0.5)
                return [single_snapshot, interpolated_snapshot]
            else:
                return [single_snapshot, single_snapshot]  # Duplikuj
    
    elif snapshot_count == 2:
        # Idealny przypadek - zwr√≥ƒá oba snapshoty
        return [relevant_orderbook.iloc[0], relevant_orderbook.iloc[1]]
    
    elif snapshot_count >= 3:
        # Usu≈Ñ ≈õrodkowy snapshot, zostaw skrajne
        return [relevant_orderbook.iloc[0], relevant_orderbook.iloc[-1]]
    
    return None

def interpolate_snapshots(snapshot1, snapshot2, ratio):
    """
    Interpoluje miƒôdzy dwoma snapshotami order book
    ratio: 0.0 = snapshot1, 1.0 = snapshot2, 0.5 = ≈õrednia
    """
    interpolated = {'timestamp': snapshot1['timestamp']}  # U≈ºyj timestamp z pierwszego
    
    # Interpoluj wszystkie poziomy order book
    for i in range(-5, 6):
        if i == 0:
            continue  # Pomijamy poziom 0
        
        depth_key = f'depth_{i}'
        notional_key = f'notional_{i}'
        
        if depth_key in snapshot1 and depth_key in snapshot2:
            # Interpoluj depth i notional
            depth1 = snapshot1[depth_key]
            depth2 = snapshot2[depth_key]
            notional1 = snapshot1[notional_key]
            notional2 = snapshot2[notional_key]
            
            interpolated[depth_key] = depth1 + (depth2 - depth1) * ratio
            interpolated[notional_key] = notional1 + (notional2 - notional1) * ratio
    
    return interpolated

def check_existing_data_range(start_date, end_date, symbol="BTCUSDT"):
    """Sprawdza czy dane z zakresu ju≈º istniejƒÖ i zwraca brakujƒÖce fragmenty"""
    print(f"üîç Sprawdzam istniejƒÖce dane dla zakresu: {start_date} - {end_date}")
    
    # Sprawd≈∫ czy plik merged ju≈º istnieje
    if os.path.exists(MERGED_FILE):
        try:
            # Sprawd≈∫ metadata
            if os.path.exists(METADATA_FILE):
                with open(METADATA_FILE, 'r') as f:
                    metadata = json.load(f)
                
                existing_start = datetime.fromisoformat(metadata['start_date'])
                existing_end = datetime.fromisoformat(metadata['end_date'])
                
                # Sprawd≈∫ czy zakres jest kompletny
                if existing_start <= start_date and existing_end >= end_date:
                    print(f"‚úÖ Dane z zakresu {start_date} - {end_date} ju≈º istniejƒÖ!")
                    print(f"   IstniejƒÖcy zakres: {existing_start} - {existing_end}")
                    return True, []
                else:
                    print(f"‚ö†Ô∏è IstniejƒÖ dane, ale zakres nie jest kompletny")
                    print(f"   IstniejƒÖcy: {existing_start} - {existing_end}")
                    print(f"   Wymagany: {start_date} - {end_date}")
                    
                    # Znajd≈∫ brakujƒÖce fragmenty
                    missing_ranges = []
                    if start_date < existing_start:
                        missing_ranges.append((start_date, existing_start - timedelta(days=1)))
                    if end_date > existing_end:
                        missing_ranges.append((existing_end + timedelta(days=1), end_date))
                    
                    return False, missing_ranges
            else:
                print(f"‚ö†Ô∏è Plik merged istnieje, ale brak metadata")
                return False, [(start_date, end_date)]
        except Exception as e:
            print(f"‚ö†Ô∏è B≈ÇƒÖd sprawdzania metadata: {e}")
            return False, [(start_date, end_date)]
    
    print(f"‚ùå Brak istniejƒÖcych danych - pobieram ca≈Çy zakres")
    return False, [(start_date, end_date)]

def save_metadata(start_date, end_date, symbol="BTCUSDT"):
    """Zapisuje metadata o pobranych danych"""
    metadata = {
        'symbol': symbol,
        'start_date': start_date.isoformat(),
        'end_date': end_date.isoformat(),
        'created_at': datetime.now().isoformat(),
        'file_format': 'feather'
    }
    
    with open(METADATA_FILE, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"üíæ Metadata zapisana: {METADATA_FILE}")

def merge_orderbook_with_ohlc(orderbook_df, ohlc_df, save_json=False, extra_month_for_ma=False):
    """ZOPTYMALIZOWANA wersja ≈ÇƒÖczenia order book z OHLC - TYLKO WSP√ìLNE DANE"""
    print(f"\nüîó ≈ÅƒÖczƒô order book z OHLC (ZOPTYMALIZOWANE)...")
    print(f"üìä Dane OHLC: {len(ohlc_df)} wierszy")
    print(f"üìä Dane Order Book: {len(orderbook_df)} wierszy")
    
    # KROK 1: Indeksowanie i groupby (ZAMIENIA O(n¬≤) na O(n log n))
    print("üìä Indeksujƒô dane order book...")
    orderbook_df.set_index('timestamp', inplace=True)
    
    # KROK 2: Vectorizowane tworzenie wide format
    print("üìä Tworzƒô wide format order book (VECTORIZED)...")
    
    # Dodaj logi progress
    total_snapshots = orderbook_df.groupby(level=0).ngroups
    print(f"üìä Przetwarzam {total_snapshots:,} snapshots order book...")
    
    # ZastƒÖp groupby().apply() pƒôtlƒÖ z logami postƒôpu
    wide_orderbook_data = []
    processed_count = 0
    
    for timestamp, group in orderbook_df.groupby(level=0):
        # Stw√≥rz wiersz dla tego timestampu
        row_data = {'timestamp': timestamp}
        
        # Dodaj dane depth i notional dla ka≈ºdego poziomu
        for _, snapshot in group.iterrows():
            row_data[f'depth_{snapshot["percentage"]}'] = snapshot["depth"]
            row_data[f'notional_{snapshot["percentage"]}'] = snapshot["notional"]
        
        wide_orderbook_data.append(row_data)
        processed_count += 1
        
        # Logi postƒôpu co 10,000 snapshots
        if processed_count % 10000 == 0:
            progress = (processed_count / total_snapshots) * 100
            print(f"üìä Postƒôp: {processed_count:,}/{total_snapshots:,} snapshots ({progress:.1f}%)")
    
    print(f"‚úÖ Wide format utworzony: {len(wide_orderbook_data):,} snapshots")
    
    # Konwertuj na DataFrame
    wide_orderbook_df = pd.DataFrame(wide_orderbook_data)
    wide_orderbook_df.set_index('timestamp', inplace=True)
    
    # KROK 4: Znajd≈∫ zakres wsp√≥lnych danych
    orderbook_start = wide_orderbook_df.index.min()
    orderbook_end = wide_orderbook_df.index.max()
    ohlc_start = ohlc_df.index.min()
    ohlc_end = ohlc_df.index.max()
    
    # Diagnostyka typ√≥w
    print(f"üîç Diagnostyka typ√≥w indeks√≥w:")
    print(f"   orderbook_start: {type(orderbook_start)} = {orderbook_start}")
    print(f"   ohlc_start: {type(ohlc_start)} = {ohlc_start}")
    print(f"   orderbook_end: {type(orderbook_end)} = {orderbook_end}")
    print(f"   ohlc_end: {type(ohlc_end)} = {ohlc_end}")
    
    # Konwertuj na pd.Timestamp je≈õli potrzebne
    if not isinstance(orderbook_start, pd.Timestamp):
        orderbook_start = pd.Timestamp(orderbook_start)
    if not isinstance(orderbook_end, pd.Timestamp):
        orderbook_end = pd.Timestamp(orderbook_end)
    
    # OHLC ma RangeIndex - u≈ºyj timestamp z kolumny
    if isinstance(ohlc_start, int):
        ohlc_start = ohlc_df['timestamp'].min()
    if isinstance(ohlc_end, int):
        ohlc_end = ohlc_df['timestamp'].max()
    
    if not isinstance(ohlc_start, pd.Timestamp):
        ohlc_start = pd.Timestamp(ohlc_start)
    if not isinstance(ohlc_end, pd.Timestamp):
        ohlc_end = pd.Timestamp(ohlc_end)

    # Oblicz wsp√≥lny zakres
    common_start = max(orderbook_start, ohlc_start)
    common_end = min(orderbook_end, ohlc_end)
    
    print(f"üìÖ Zakresy danych:")
    print(f"   OHLC: {ohlc_start} - {ohlc_end}")
    print(f"   Order Book: {orderbook_start} - {orderbook_end}")
    print(f"   WSP√ìLNY: {common_start} - {common_end}")
    
    # KROK 5: Filtruj OHLC do wsp√≥lnego zakresu
    if extra_month_for_ma:
        # Dodaj 30 dni wcze≈õniej dla MA 43200
        ma_start = common_start - pd.Timedelta(days=30)
        print(f"   +30 dni dla MA: {ma_start} - {common_end}")
        filtered_ohlc = ohlc_df[(ohlc_df['timestamp'] >= ma_start) & (ohlc_df['timestamp'] <= common_end)]
    else:
        filtered_ohlc = ohlc_df[(ohlc_df['timestamp'] >= common_start) & (ohlc_df['timestamp'] <= common_end)]
    
    print(f"üìä Filtrowane OHLC: {len(filtered_ohlc)} wierszy")
    
    # KROK 6: Vectorizowane przetwarzanie OHLC
    print("üìä Przetwarzam snapshoty dla ≈õwieczek OHLC (VECTORIZED)...")
    
    def process_candle_vectorized(ohlc_row):
        """Vectorizowana funkcja przetwarzania jednej ≈õwieczki"""
        ohlc_timestamp = ohlc_row['timestamp']
        
        # DEBUG: Sprawd≈∫ pierwsze kilka ≈õwieczek
        if processed_count < 5:
            print(f"DEBUG: Przetwarzam ≈õwieczkƒô {ohlc_timestamp}")
            print(f"DEBUG: wide_orderbook_df.index.min() = {wide_orderbook_df.index.min()}")
            print(f"DEBUG: wide_orderbook_df.index.max() = {wide_orderbook_df.index.max()}")
        
        # Szybki lookup w indeksowanym DataFrame
        try:
            # U≈ºyj exclusive end slicing (jak w process_snapshots_for_candle)
            snapshots = wide_orderbook_df[
                (wide_orderbook_df.index >= ohlc_timestamp) & 
                (wide_orderbook_df.index < ohlc_timestamp + pd.Timedelta(minutes=1))
            ]
            
            # DEBUG: Sprawd≈∫ pierwsze kilka ≈õwieczek
            if processed_count < 5:
                print(f"DEBUG: Znaleziono {len(snapshots)} snapshot√≥w dla {ohlc_timestamp}")
                
        except KeyError:
            if processed_count < 5:
                print(f"DEBUG: KeyError dla {ohlc_timestamp}")
            return None  # Brak order book - pomijamy ten wiersz
        
        if len(snapshots) == 0:
            if processed_count < 5:
                print(f"DEBUG: Brak snapshot√≥w dla {ohlc_timestamp}")
            return None  # Brak order book - pomijamy ten wiersz
        
        # Przetw√≥rz snapshoty
        processed_snapshots = process_snapshots_for_candle(ohlc_timestamp, snapshots)
        
        # DEBUG: Sprawd≈∫ pierwsze kilka ≈õwieczek
        if processed_count < 5:
            print(f"DEBUG: processed_snapshots = {processed_snapshots}")
        
        # Sprawd≈∫ czy mamy kompletne dane
        if not processed_snapshots or len(processed_snapshots) != 2:
            if processed_count < 5:
                print(f"DEBUG: Niekompletne dane dla {ohlc_timestamp}")
            return None  # Niekompletne dane - pomijamy ten wiersz
        
        # Stw√≥rz wiersz wynikowy
        merged_row = {
            'timestamp': ohlc_timestamp,
            'open': ohlc_row['open'],
            'high': ohlc_row['high'],
            'low': ohlc_row['low'],
            'close': ohlc_row['close'],
            'volume': ohlc_row['volume']
        }
        
        # Dodaj dane z pierwszego snapshotu
        for key, value in processed_snapshots[0].items():
            if key != 'timestamp':
                merged_row[f'snapshot1_{key}'] = value
        
        # Dodaj dane z drugiego snapshotu
        for key, value in processed_snapshots[1].items():
            if key != 'timestamp':
                merged_row[f'snapshot2_{key}'] = value
        
        # Dodaj informacje o jako≈õci danych
        merged_row['snapshot1_timestamp'] = processed_snapshots[0]['timestamp']
        merged_row['snapshot2_timestamp'] = processed_snapshots[1]['timestamp']
        merged_row['data_quality'] = 'complete'
        
        return merged_row
    
    # KROK 7: Vectorizowane przetwarzanie wszystkich ≈õwieczek
    print("üîÑ Przetwarzam wszystkie ≈õwieczki OHLC...")
    
    # Dodaj logi progress
    total_candles = len(filtered_ohlc)
    print(f"üìä Przetwarzam {total_candles:,} ≈õwieczek OHLC...")
    
    # ZastƒÖp apply() pƒôtlƒÖ z logami postƒôpu
    processed_data = []
    processed_count = 0
    
    for index, ohlc_row in filtered_ohlc.iterrows():
        result = process_candle_vectorized(ohlc_row)
        if result is not None:
            processed_data.append(result)
        
        processed_count += 1
        
        # Logi postƒôpu co 50,000 ≈õwieczek
        if processed_count % 50000 == 0:
            progress = (processed_count / total_candles) * 100
            complete_count = len(processed_data)
            print(f"üìä Postƒôp: {processed_count:,}/{total_candles:,} ≈õwieczek ({progress:.1f}%) - Kompletne: {complete_count:,}")
    
    print(f"‚úÖ Przetworzono {len(processed_data):,} ≈õwieczek z kompletnymi danymi order book")
    
    # KROK 8: Tworzenie finalnego DataFrame
    print("üìä Tworzƒô finalny DataFrame...")
    full_merged_df = pd.DataFrame(processed_data)
    
    # Statystyki
    complete_count = len(full_merged_df)
    total_ohlc = len(filtered_ohlc)
    coverage = (complete_count / total_ohlc * 100) if total_ohlc > 0 else 0
    
    print(f"\nüìà Statystyki przetwarzania (TYLKO WSP√ìLNE DANE):")
    print(f"  ≈öwieczek OHLC w zakresie: {total_ohlc}")
    print(f"  ≈öwieczek z kompletnymi danymi: {complete_count}")
    print(f"  Pokrycie: {coverage:.1f}%")
    print(f"  Zakres wynikowy: {full_merged_df['timestamp'].min()} - {full_merged_df['timestamp'].max()}")
    
    # Zapisz tylko feather (szybszy)
    print(f"\nüíæ Zapisujƒô dane do {MERGED_FILE}...")
    full_merged_df.to_feather(MERGED_FILE)
    print(f"‚úÖ Dane zapisane pomy≈õlnie do {MERGED_FILE}")
    
    return full_merged_df

def create_missing_orderbook_data(missing_dates, orderbook_dir):
    """
    Tworzy brakujƒÖce dane order book na podstawie dni sƒÖsiadujƒÖcych
    """
    print(f"üîß Tworzƒô brakujƒÖce dane order book dla {len(missing_dates)} dni...")
    
    for missing_date in missing_dates:
        print(f"   üìÖ Przetwarzam: {missing_date}")
        
        # Znajd≈∫ sƒÖsiadujƒÖce dni
        missing_dt = datetime.strptime(missing_date, '%Y-%m-%d')
        
        # Sprawd≈∫ dzie≈Ñ przed
        day_before = (missing_dt - timedelta(days=1)).strftime('%Y-%m-%d')
        file_before = f"orderbook_raw/BTCUSDT-bookDepth-{day_before}.csv"
        
        # Sprawd≈∫ dzie≈Ñ po
        day_after = (missing_dt + timedelta(days=1)).strftime('%Y-%m-%d')
        file_after = f"orderbook_raw/BTCUSDT-bookDepth-{day_after}.csv"
        
        # Wybierz ≈∫r√≥d≈Ço danych (preferuj dzie≈Ñ przed)
        source_file = None
        source_date = None
        
        if os.path.exists(file_before):
            source_file = file_before
            source_date = day_before
            print(f"      üìã U≈ºywam danych z: {day_before}")
        elif os.path.exists(file_after):
            source_file = file_after
            source_date = day_after
            print(f"      üìã U≈ºywam danych z: {day_after}")
        else:
            print(f"      ‚ùå Brak danych sƒÖsiadujƒÖcych dla {missing_date}")
            continue
        
        # Wczytaj dane ≈∫r√≥d≈Çowe
        try:
            source_df = pd.read_csv(source_file)
            print(f"      üìä Wczytano {len(source_df):,} wierszy z {source_date}")
            
            # Skopiuj dane i dostosuj timestampy
            new_df = source_df.copy()
            
            # Zamie≈Ñ timestampy na brakujƒÖcy dzie≈Ñ
            source_start = pd.to_datetime(source_date)
            target_start = pd.to_datetime(missing_date)
            
            # Oblicz przesuniƒôcie czasowe
            time_shift = target_start - source_start
            
            # Dostosuj timestampy
            new_df['timestamp'] = pd.to_datetime(new_df['timestamp']) + time_shift
            new_df['timestamp'] = new_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M:%S')
            
            # Zapisz nowy plik
            output_file = f"orderbook_raw/BTCUSDT-bookDepth-{missing_date}.csv"
            new_df.to_csv(output_file, index=False)
            
            print(f"      ‚úÖ Utworzono: {output_file} ({len(new_df):,} wierszy)")
            
        except Exception as e:
            print(f"      ‚ùå B≈ÇƒÖd podczas tworzenia danych: {e}")

def check_and_fill_missing_orderbook():
    """
    Sprawdza brakujƒÖce dni order book i wype≈Çnia je danymi z dni sƒÖsiadujƒÖcych
    """
    orderbook_dir = "orderbook_raw"
    ohlc_dir = "ohlc_raw"
    
    # Pobierz listƒô plik√≥w
    orderbook_files = [f for f in os.listdir(orderbook_dir) if f.endswith('.csv')]
    ohlc_files = [f for f in os.listdir(ohlc_dir) if f.endswith('.csv')]
    
    # WyciƒÖgnij daty z nazw plik√≥w
    orderbook_dates = set()
    for file in orderbook_files:
        if 'BTCUSDT-bookDepth-' in file:
            date_str = file.replace('BTCUSDT-bookDepth-', '').replace('.csv', '')
            orderbook_dates.add(date_str)
    
    ohlc_dates = set()
    for file in ohlc_files:
        if 'BTCUSDT-1m-' in file:
            date_str = file.replace('BTCUSDT-1m-', '').replace('.csv', '')
            ohlc_dates.add(date_str)
    
    # Znajd≈∫ luki
    missing_orderbook = ohlc_dates - orderbook_dates
    
    if missing_orderbook:
        print(f"üîç Znaleziono {len(missing_orderbook)} brakujƒÖcych dni order book:")
        for date in sorted(missing_orderbook):
            print(f"   - {date}")
        
        # Wype≈Çnij brakujƒÖce dane
        create_missing_orderbook_data(sorted(missing_orderbook), orderbook_dir)
        
        print(f"‚úÖ Wype≈Çniono wszystkie brakujƒÖce dni order book!")
        return True
    else:
        print(f"‚úÖ Brak luk w danych order book!")
        return False

def main():
    """G≈Ç√≥wna funkcja pobierania i ≈ÇƒÖczenia danych"""
    import argparse
    
    # Parsuj argumenty z linii komend
    parser = argparse.ArgumentParser(description='Pobierz i po≈ÇƒÖcz dane OHLC z Order Book')
    parser.add_argument('symbol', help='Symbol kryptowaluty (np. BTCUSDT)')
    parser.add_argument('start_date', help='Data poczƒÖtkowa (YYYY-MM-DD)')
    parser.add_argument('end_date', help='Data ko≈Ñcowa (YYYY-MM-DD)')
    parser.add_argument('--extra-month', action='store_true', help='Dodaj 30 dni dla MA 43200')
    
    args = parser.parse_args()
    
    # Konwertuj stringi dat na datetime
    try:
        start_date = datetime.strptime(args.start_date, '%Y-%m-%d')
        end_date = datetime.strptime(args.end_date, '%Y-%m-%d')
    except ValueError as e:
        print(f"‚ùå B≈ÇƒÖd formatu daty: {e}")
        print("U≈ºyj formatu: YYYY-MM-DD (np. 2023-01-01)")
        return
    
    symbol = args.symbol
    extra_month_for_ma = args.extra_month
    
    print(f"üöÄ Rozpoczynanie pobierania danych dla {symbol}")
    print(f"üìÖ Zakres: {start_date.strftime('%Y-%m-%d')} - {end_date.strftime('%Y-%m-%d')}")
    if extra_month_for_ma:
        print(f"üìä Dodajƒô 30 dni dla obliczania MA 43200")
    
    # Sprawd≈∫ i wype≈Çnij brakujƒÖce dane order book
    print(f"\nüîç Sprawdzam luki w danych order book...")
    check_and_fill_missing_orderbook()
    
    # KROK 1: Sprawd≈∫ istniejƒÖce dane
    data_exists, missing_ranges = check_existing_data_range(start_date, end_date, symbol)
    
    if data_exists:
        print(f"‚úÖ Dane ju≈º istniejƒÖ! Ko≈Ñczƒô pracƒô.")
        return
    
    if missing_ranges:
        print(f"üìã BrakujƒÖce zakresy do pobrania:")
        for start, end in missing_ranges:
            print(f"   {start.strftime('%Y-%m-%d')} - {end.strftime('%Y-%m-%d')}")
    
    # KROK 2: Pobierz brakujƒÖce dane
    all_csv_files_orderbook = []
    all_csv_files_ohlc = []
    
    for start, end in missing_ranges:
        print(f"\nüì• Pobieram dane dla zakresu: {start.strftime('%Y-%m-%d')} - {end.strftime('%Y-%m-%d')}")
        
        for date in daterange(start, end):
            date_str = date.strftime('%Y-%m-%d')
            
            # Pobierz order book
            orderbook_csv = download_and_extract_orderbook(symbol, date_str)
            if orderbook_csv:
                all_csv_files_orderbook.append(orderbook_csv)
            
            # Pobierz OHLC
            ohlc_csv = download_and_extract_ohlc(symbol, date_str)
            if ohlc_csv:
                all_csv_files_ohlc.append(ohlc_csv)
    
    if not all_csv_files_orderbook or not all_csv_files_ohlc:
        print("‚ùå Brak plik√≥w do przetworzenia!")
        return
    
    # KROK 3: Przetw√≥rz dane
    print(f"\nüìä Przetwarzam {len(all_csv_files_orderbook)} plik√≥w order book...")
    orderbook_df = load_and_process_orderbook(all_csv_files_orderbook)
    
    print(f"\nüìä Przetwarzam {len(all_csv_files_ohlc)} plik√≥w OHLC...")
    ohlc_df = load_and_process_ohlc(all_csv_files_ohlc)
    
    if orderbook_df is None or ohlc_df is None:
        print("‚ùå B≈ÇƒÖd przetwarzania danych!")
        return
    
    # KROK 4: Po≈ÇƒÖcz dane (ZOPTYMALIZOWANE - TYLKO WSP√ìLNE)
    merged_df = merge_orderbook_with_ohlc(orderbook_df, ohlc_df, extra_month_for_ma=extra_month_for_ma)
    
    # KROK 5: Zapisz metadata
    save_metadata(start_date, end_date, symbol)
    
    print(f"\nüéâ Proces zako≈Ñczony pomy≈õlnie!")
    print(f"üìÅ Plik wynikowy: {MERGED_FILE}")
    print(f"üìä Liczba wierszy: {len(merged_df):,}")
    print(f"üìÖ Zakres wynikowy: {merged_df['timestamp'].min()} - {merged_df['timestamp'].max()}")

if __name__ == "__main__":
    main() 